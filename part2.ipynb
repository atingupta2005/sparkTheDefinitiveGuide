{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: The Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Structured APIs - DataFrames, SQL, and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(master='spark://164.52.193.152:7077', appName='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightDataJson2015 = 'data/flight-data/json/2015-summary.json'\n",
    "flightDataJson = 'data/flight-data/json/*-summary.json'\n",
    "retailData20101201 = 'data/retail-data/by-day/2010-12-01.csv'\n",
    "retailDataAll = 'data/retail-data/all/*.csv'\n",
    "flightDataCSV2010 = 'data/flight-data/csv/2010-summary.csv'\n",
    "flightDataJson2010 = 'data/flight-data/json/2010-summary.json'\n",
    "flightDataParquet2010 = 'data/flight-data/parquet/2010-summary.parquet'\n",
    "flightDataORC2010 = 'data/flight-data/orc/2010-summary.orc'\n",
    "sqliteJDBC = 'data/flight-data/jdbc/my-sqlite.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #4 - Structured API Overview_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Columns\n",
    "-  Rows\n",
    "-  Spark Types\n",
    "\n",
    "### _How code is executed across the cluster_:\n",
    "1. write DF/DS/SQL\n",
    "2. if valid code, Spark converts this to a **Logical Plan**\n",
    "3. Spark transforms this **Logical Plan** to a **Physical Plan**, checking for optimizations along the way\n",
    "4. Spark executes this **Physical Plan** (RDD manipulations) across the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Catalyst Optimizer analyzes and decides how the code should be executed via a plan_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Plan Execution_:\n",
    "1.  Convert user's code into an Unresolved Logical Plan\n",
    "2.  Unresolved Logical Plan uses the Catalog to that check object (DF/DS/SQL) information is valid\n",
    "3.  If valid, Catalyst Optimizer collects information that attempts to optimize the Logical Plan\n",
    "4.  After Logical Plan is created, Spark plans out the Physical Plan\n",
    "5.  Physical Plan maps process of how logic will be executed on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #5 - Basic Structured Operations_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions:\n",
    "-  **Schema** (defines the column names and types of a DF):\n",
    "    - schema on read is fine for ad hoc use cases however inferring the schema can be expensive and or incorrect\n",
    "    - it is recommended to define schema for production ETL use cases   \n",
    "    <br>\n",
    "-  **Structure**:\n",
    "    - StructType [schema made up of a number of fields]\n",
    "    - StructField [contain name, type, boolean flag specifying whether a column can contain missing or null values]\n",
    "    - Metadata [way of storing information about this column]   \n",
    "    <br>\n",
    "-  **Expressions**:\n",
    "    - operations that select, manipulate, and remove columns from DFs\n",
    "    - set of transformations on one or more values in a record in a DF\n",
    "    - same performance is achieved via DF code vs SQL expression   \n",
    "    <br>\n",
    "-  **Columns**:\n",
    "    - label that represents a value computed on a per record basis by means of an expression\n",
    "    - columns are resolved when compared to column names maintained in the **catalog** of the **analyzer phase**   \n",
    "    <br>\n",
    "-  **Rows**:\n",
    "    -  each row in a DF is a single record of type Row\n",
    "    -  Row objects internally arrays of bytes   \n",
    "    <br>\n",
    "-  **Repartition**:\n",
    "    -  repartition forces a full shuffle of the data\n",
    "    -  typically only use repartition when the future # of partitions is > current # of partitions\n",
    "    -  can be used when paritioning by a set of columns (ex: frequent filter on same column)   \n",
    "    <br>\n",
    "-  **Coalesce**:\n",
    "    -  does not cause a shuffle instead tries to combine partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Transformations:\n",
    "-  add rows or columns\n",
    "-  remove rows or columns\n",
    "-  transform a row into a column\n",
    "-  transform a column into a row\n",
    "-  change order of rows based on the values in columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #5 Exercises (DataFrames)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Schema Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(flightDataJson2015).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Create Schema Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myManualSchema = StructType([\\\n",
    "                            StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\\\n",
    "                            StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\\\n",
    "                            StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\\\n",
    "                            ])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    ".load(flightDataJson2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Columns Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'someColumnName'>\n",
      "Column<b'someColumnName'>\n"
     ]
    }
   ],
   "source": [
    "print(col(\"someColumnName\"))\n",
    "print(column(\"someColumnName\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(flightDataJson2015).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Expressions Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df code\n",
    "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sql code\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Row Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# accessing row elements\n",
    "print(myRow[0])\n",
    "print(myRow[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Creating DF Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(flightDataJson2015)\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Convert Rows to DF Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(some='Hello', col=None, names=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myManualSchema = StructType([\\\n",
    "                            StructField(\"some\", StringType(), True),\\\n",
    "                            StructField(\"col\", StringType(), True),\\\n",
    "                            StructField(\"names\", LongType(), False)\\\n",
    "                            ])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDF = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _select & expr Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\\\n",
    "         expr(\"DEST_COUNTRY_NAME\"),\\\n",
    "         col(\"DEST_COUNTRY_NAME\"),\\\n",
    "         column(\"DEST_COUNTRY_NAME\"))\\\n",
    ".show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\").alias(\"DEST_COUNTRY_NAME\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _selectExpr Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shorthand to select(expr(...))\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _literal [lit] & withColumn Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "|    United States|            Ireland|  344|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "|    United States|            Ireland|  344|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _withColumnRenamed Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Columns w/ Escape Characters Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfWithLongColName = df.withColumn(\"This Long Column-Name\", expr(\"ORIGIN_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "|              Ireland|Ireland|\n",
      "+---------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# backticks (`) required when referencing a column with whitespaces/escape characters in an expression [expr]\n",
    "dfWithLongColName.selectExpr(\"`This Long Column-Name`\",\\\n",
    "                            \"`This Long Column-Name` as `new col`\")\\\n",
    ".show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Removing Columns [drop] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count: bigint, This Long Column-Name: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Changing Column Types [cast] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: bigint]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"count2\", col(\"count\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Filter Rows [filter/where] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"count\") < 2).show(3)\n",
    "df.where(\"count < 2\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use WHERE for multiple \"AND\" filters since Spark performs all FILTER operations at the same time\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Unique Rows [distinct] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Random Samples [sample] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Random Split [randomSplit] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "60\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "print(dataFrames[0].count() > dataFrames[1].count())\n",
    "print(dataFrames[0].count())\n",
    "print(dataFrames[1].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Union Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = df.schema\n",
    "newRows = [\\\n",
    "          Row(\"New Country\", \"Other Country\", 0),\\\n",
    "          Row(\"New Country 2\", \"Other Country 3\", 999999999)\\\n",
    "          ]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(newDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.union(newDF).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.union(newDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Sorting Rows [sort & orderBy] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New Country|      Other Country|    0|\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New Country|      Other Country|    0|\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# default sort is ascending order\n",
    "df.sort(\"count\").show(3)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|    count|\n",
      "+-----------------+-------------------+---------+\n",
      "|    New Country 2|    Other Country 3|999999999|\n",
      "|    United States|      United States|   370002|\n",
      "|    United States|             Canada|     8483|\n",
      "|           Canada|      United States|     8399|\n",
      "|    United States|             Mexico|     7187|\n",
      "+-----------------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Partition Sorts [sortWithinPartitions] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(flightDataJson).sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Limit [limit] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|    count|\n",
      "+-----------------+-------------------+---------+\n",
      "|    New Country 2|    Other Country 3|999999999|\n",
      "|    United States|      United States|   370002|\n",
      "|    United States|             Canada|     8483|\n",
      "+-----------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\").desc()).limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Repartitioning [repartition & coalesce] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ex: shuffle data into 5 partitions based on column then coalesce partitions into 2\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Collect Rows to Driver Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)\n",
      "Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)\n",
      "\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)\n",
      "Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62)\n",
      "Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588)\n",
      "Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40)\n",
      "Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)\n"
     ]
    }
   ],
   "source": [
    "collectDF = df.limit(10)\n",
    "for i in collectDF.take(5): print(i)\n",
    "print(\"\\n\")\n",
    "collectDF.show()\n",
    "for i in collectDF.collect(): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x1111cf630>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #6 - Working with Different Types of Data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Documentation:\n",
    "-  http://spark.apache.org/docs/latest/api/python/#package\n",
    "-  http://spark.apache.org/docs/latest/api/scala/#package   \n",
    "\n",
    "### **Data**:   \n",
    "-  **Booleans**:\n",
    "   -  filters\n",
    "   -  conditionals\n",
    "   -  elements (and, or, true, false)\n",
    "   -  always chain together \"and\" filters as sequential filter   \n",
    "   <br>\n",
    "-  **Numbers**:\n",
    "   -  statFunctions package is useful for statistical inference and functions   \n",
    "<br>\n",
    "-  **Strings**:\n",
    "    -  regular expressions   \n",
    "    <br>\n",
    "-  **Dates & Timestamps**:\n",
    "    -  Spark follows Java SimpleDateFormat standard - https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html\n",
    "    -  correct format recommended is 'yyyy-MM-dd'\n",
    "    -  to_date function [convert string to a date]\n",
    "    -  Spark will return unparsed \"error\" to_date conversions as **null** due to different date/timestamp formats      \n",
    "    <br>\n",
    "-  **Handling Nulls**:\n",
    "    -  Spark recommends to use 'null' for missing/empty data instead of empty strings/other values for optimization purposes\n",
    "    -  options (1. drop nulls; 2. fill nulls with a new value)\n",
    "    -  if column(s) in schema are declared to not have nulls (nullable = false) it does not mean nulls are forbidden to be in that column ... this feature helps spark sql optimize handling for that column ... having null values in columns with (nullable = false) can lead to strange results and performance\n",
    "    -  nulls can be ordered with the following functions (**asc_nulls_first, desc_nulls_first, asc_nulls_last_desc_nulls_last**)   \n",
    "<br>\n",
    "-  **Complex Types**:\n",
    "    -  _structs_ [\"DFs within DFs\" w/ dot syntax]\n",
    "    -  _arrays_ [list of elements]\n",
    "    -  _maps_ [key-value pairs]\n",
    "    - _JSON_ [semi-structured nested dictionary data]   \n",
    "<br>\n",
    "-  **UDFs**:\n",
    "    -  ability to write own custom transformations\n",
    "    -  Spark serializes the UDF on the driver and transfers it over the network to all executor processes hence there can be performance penalties (Python) because:\n",
    "        1. Spark starts a Python process on the worker\n",
    "        2. then Spark serializes all of the data to a format Python can understand\n",
    "        3. then executes the UDF row by row on the data in the Python process\n",
    "        4. then returns the results of the row operations back to the JVM/Spark\n",
    "    -  Python data serialization is expensive bc Spark cannot manage the memory of the worker when the UDF is sent to the workers to be serialized\n",
    "    -  ***Scala/Java UDFs do not have a separate process like Python hence the logic is computed in the JVM***\n",
    "    -  it is a best practice to define return type of UDF since Spark and Python Data Types can be different (_see UDF example_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #6 Exercises (DataFrames)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(retailData20101201)\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Convert to Spark Types Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Boolean Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    ".select(\"InvoiceNo\", \"Description\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|\n",
      "|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "|536367   |22748    |POPPY'S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"InvoiceNo = 536365\").show(5, False)\n",
    "df.where(\"InvoiceNo != 536365\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Boolean Substring [instr] Expression [isin] Filter Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show() # isin() means boolean expression / pipe means \"OR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|   Description|unitPrice|isExpensive|\n",
      "+--------------+---------+-----------+\n",
      "|DOTCOM POSTAGE|   569.77|       true|\n",
      "|DOTCOM POSTAGE|   607.49|       true|\n",
      "+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DOTCodeFilter = col(\"StockCode\") == 'DOT'\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"Description\", \"unitPrice\", \"isExpensive\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|   Description|UnitPrice|isExpensive|\n",
      "+--------------+---------+-----------+\n",
      "|DOTCOM POSTAGE|   569.77|       true|\n",
      "|DOTCOM POSTAGE|   607.49|       true|\n",
      "+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"Description\", \"UnitPrice\", \"isExpensive\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Boolean Expression [eqNullSafe] for NULLs Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|       null|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n",
      "|   536545|    21134|       null|       1|2010-12-01 14:32:00|      0.0|      null|United Kingdom|\n",
      "|   536546|    22145|       null|       1|2010-12-01 14:33:00|      0.0|      null|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"Description\").eqNullSafe(None)).show(3)\n",
    "df.where(col(\"Description\").eqNullSafe(\"null\")).show(3)\n",
    "df.where(col(\"Description\").eqNullSafe(\"NULL\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|(Description <=> NULL)|\n",
      "+----------------------+\n",
      "|                 false|\n",
      "|                  true|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"Description\"].eqNullSafe(None)).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Power Expression [pow] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "+----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "+----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# via select expression\n",
    "df.selectExpr(\"CustomerId\", \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Round Up [round] & Round Down [bround] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, round, bround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Column Correlation [corr] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Summary Stats [describe] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(summary='count', InvoiceNo='3108', StockCode='3108', Description='3098', Quantity='3108', UnitPrice='3108', CustomerID='1968', Country='3108')\n",
      "Row(summary='mean', InvoiceNo='536516.684944841', StockCode='27834.304044117645', Description=None, Quantity='8.627413127413128', UnitPrice='4.151946589446603', CustomerID='15661.388719512195', Country=None)\n",
      "Row(summary='stddev', InvoiceNo='72.89447869788873', StockCode='17407.897548583845', Description=None, Quantity='26.371821677029203', UnitPrice='15.638659854603892', CustomerID='1854.4496996893627', Country=None)\n",
      "Row(summary='min', InvoiceNo='536365', StockCode='10002', Description=' 4 PURPLE FLOCK DINNER CANDLES', Quantity='-24', UnitPrice='0.0', CustomerID='12431.0', Country='Australia')\n",
      "Row(summary='max', InvoiceNo='C536548', StockCode='POST', Description='ZINC WILLIE WINKIE  CANDLE STICK', Quantity='600', UnitPrice='607.49', CustomerID='18229.0', Country='United Kingdom')\n"
     ]
    }
   ],
   "source": [
    "for i in df.describe().collect(): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manual import\n",
    "from pyspark.sql.functions import count, mean, stddev_pop, min, max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Median Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colName = \"UnitPrice\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _View Frequent Column Value Occurences [stat.freqItems] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(StockCode_freqItems=['90214E', '20728', '20755', '21703', '22113', '22524', '22041', '72803A', '72798C', '90181B', '21756', '22694', '90206C', '20970', '21624', '90209C', '84744', '82494L', '22952', '20682', '22583', '21705', '20679', '22220', '90177E', '90214A', '22448', '90214S', '22121', '22802', '84970L', '72818', '90192', '90200C', '22910', '21380', '90211A', '21137', '35271S', '84926A', '20765', '22384', '21524', '22165', '22366', '21221', '21704', '22519', '85035C', '21967', '22114', '22909', '22900', '22447', '21577', '21877', '20726', '85034A', 'DOT', '84658', '21472', '22804', '22222', '72802C', '21739', '22467', '90214H', '22785', '22446', '22197', '20665', '21733', '22731', '21709', '22086', '40001', '85123A'], Quantity_freqItems=[200, 128, 23, 32, 50, 600, 8, 17, 80, -1, -10, 11, 56, 47, 20, -7, 2, 5, 480, -4, 14, 432, 100, 64, 40, 13, 4, -5, 22, 16, -2, 7, 70, 384, 25, 34, 10, 1, 288, 216, 28, 252, 19, 120, 192, 60, 96, 72, 144, 36, 27, 9, 18, 48, 21, 12, 3, -6, -24, 30, 15, 33, 6, 24, -12, -3])\n"
     ]
    }
   ],
   "source": [
    "# be careful; output may be large\n",
    "for i in df.stat.freqItems([\"StockCode\", \"Quantity\"]).collect(): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Unique ID [monotonically...] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "|                            2|\n",
      "|                            3|\n",
      "|                            4|\n",
      "+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(monotonically_increasing_id()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Capitalize per Whitespace [initcap] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|Description                       |initcap(Description)              |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|White Hanging Heart T-light Holder|\n",
      "|WHITE METAL LANTERN               |White Metal Lantern               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |Cream Cupid Hearts Coat Hanger    |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Description\", initcap(col(\"Description\"))).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Uppercase & Lowercase [upper / lower] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|Description                       |lower(Description)                |upper(lower(Description))         |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|white hanging heart t-light holder|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |white metal lantern               |WHITE METAL LANTERN               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |cream cupid hearts coat hanger    |CREAM CUPID HEARTS COAT HANGER    |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Description\"),\\\n",
    "         lower(col(\"Description\")),\\\n",
    "         upper(lower(col(\"Description\")))).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Removing Spaces [lpad, ltrim, rpad, rtrim, trim] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+---+----------+\n",
      "|   ltrim|   rtrim| trim| lp|        rp|\n",
      "+--------+--------+-----+---+----------+\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLOxxxxx|\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLOxxxxx|\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLOxxxxx|\n",
      "+--------+--------+-----+---+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\\\n",
    "         ltrim(lit(\"   HELLO   \")).alias(\"ltrim\"),\\\n",
    "         rtrim(lit(\"   HELLO   \")).alias(\"rtrim\"),\\\n",
    "         trim(lit(\"   HELLO   \")).alias(\"trim\"),\\\n",
    "         lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\\\n",
    "         rpad(lit(\"HELLO\"), 10, \"x\").alias(\"rp\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RegEx Example_:\n",
    "-  regexp_extract\n",
    "-  regexp_replace\n",
    "-  translate\n",
    "-  instr\n",
    "-  locate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+\n",
      "|color_clean                        |Description                        |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|COLOR METAL LANTERN                |WHITE METAL LANTERN                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|COLOR WOOLLY HOTTIE COLOR HEART.   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\" # replace any of these colors with the word \"COLOR\"\n",
    "df.select(\\\n",
    "         regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\\\n",
    "         col(\"Description\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|translate(Description, LET, 137)  |Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHI73 M37A1 1AN73RN               |WHITE METAL LANTERN               |\n",
      "|CR3AM CUPID H3AR7S COA7 HANG3R    |CREAM CUPID HEARTS COAT HANGER    |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(translate(col(\"Description\"), \"LET\", \"137\"), col(\"Description\")).show(3, False) # replace characters w/ intergers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------+\n",
      "|color_clean|Description                        |\n",
      "+-----------+-----------------------------------+\n",
      "|WHITE      |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHITE      |WHITE METAL LANTERN                |\n",
      "|           |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|           |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|RED        |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\\\n",
    "         regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\\\n",
    "         col(\"Description\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    ".where(\"hasSimpleColor\")\\\n",
    ".select(\"Description\")\\\n",
    ".show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, locate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|Description                      |\n",
      "+---------------------------------+\n",
      "|BLUE COAT RACK PARIS FASHION     |\n",
      "|JUMBO  BAG BAROQUE BLACK WHITE   |\n",
      "|BLUE 3 PIECE POLKADOT CUTLERY SET|\n",
      "+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "\n",
    "def color_locator(column, color_string):\n",
    "  return locate(color_string.upper(), column)\\\n",
    "          .cast(\"boolean\")\\\n",
    "          .alias(\"is_\" + color_string)\n",
    "\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(expr(\"*\")) # has to a be Column type\n",
    "\n",
    "df.select(*selectedColumns).where(expr(\"is_black or is_blue\"))\\\n",
    "  .select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS `is_black`'>,\n",
       " Column<b'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS `is_white`'>,\n",
       " Column<b'CAST(locate(RED, Description, 1) AS BOOLEAN) AS `is_red`'>,\n",
       " Column<b'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS `is_green`'>,\n",
       " Column<b'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS `is_blue`'>,\n",
       " Column<b'unresolvedstar()'>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Dates & Timestamps Example_:\n",
    "-  Dates [calendar dates]\n",
    "-  Timestamps [date and time information]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2018-10-28|2018-10-28 22:05:08.165|\n",
      "+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.limit(1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|     today|date_sub(today, 5)|date_add(today, 5)|\n",
      "+----------+------------------+------------------+\n",
      "|2018-10-28|        2018-10-23|        2018-11-02|\n",
      "+----------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(col(\"today\"), date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Difference Between 2 Dates [datediff] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of days between 2 dates\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    ".select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|months_between(start, end)|\n",
      "+--------------------------+\n",
      "|              -84.19354839|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of months between 2 dates\n",
    "dateDF.select(\\\n",
    "             to_date(lit(\"2000-01-01\")).alias(\"start\"),\\\n",
    "             to_date(lit(\"2007-01-07\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\")))\\\n",
    ".show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _DateFormat Debugging Part 1 Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\")).select(to_date(col(\"date\"))).show(1)\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1)\n",
    "# month slot does not have '20' hence null error\n",
    "# unable to know if '12' is 12th day or Dec as well as if '11' is 11th day or Nov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _DateFormat Debugging Part 2 Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\\\n",
    "                                   to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\\\n",
    "                                   to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dataTable2\")\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|to_timestamp(`date`, 'yyyy-dd-MM')|\n",
      "+----------------------------------+\n",
      "|               2017-11-12 00:00:00|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n",
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()\n",
    "cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Selecting non-null values [coalesce] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|coalesce(Description, Customerid) |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(coalesce(col(\"Description\"), col(\"Customerid\"))).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Deleting Rows w/ Null [drop] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3108\n",
      "1968\n",
      "1968\n",
      "3108\n",
      "3108\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(df.na.drop().count()) # drops rows if any of the values are NULL\n",
    "print(df.na.drop(\"any\").count()) # drops rows if any of the values are NULL\n",
    "print(df.na.drop(\"all\").count()) # drops rows only if all values are NULLs or NaNs\n",
    "print(df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"]).count()) # drops rows for certain subset of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Filling Rows w/ Null [fill] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type string column\n",
    "df.na.fill(\"All Null values become this string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(9999, subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map key (column) - value (what to fill nulls with) fill\n",
    "fill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Replace Values w/ Values [replace] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|     UK|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|     UK|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|     UK|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)\n",
    "df.na.replace([\"United Kingdom\"], [\"UK\"], \"Country\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Ordering Nulls Example_:\n",
    "-  asc_nulls_first\n",
    "-  desc_nulls_first\n",
    "-  asc_nulls_last\n",
    "-  desc_nulls_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Struct Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[complex: struct<Description:string,InvoiceNo:string>, InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- complex: struct (nullable = false)\n",
      " |    |-- Description: string (nullable = true)\n",
      " |    |-- InvoiceNo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "complexDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|complex                                     |\n",
      "+--------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER, 536365]|\n",
      "+--------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------------------+\n",
      "|complex.Description               |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------------------+---------+\n",
      "|Description                       |InvoiceNo|\n",
      "+----------------------------------+---------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |\n",
      "+----------------------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\").show(1, False)\n",
    "complexDF.select(col(\"complex\").getField(\"Description\")).show(1, False)\n",
    "complexDF.select(\"complex.*\").show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Array (Convert Every Description Token into Row) [explode] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|split(Description,  )                   |\n",
      "+----------------------------------------+\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "|[WHITE, METAL, LANTERN]                 |\n",
      "|[CREAM, CUPID, HEARTS, COAT, HANGER]    |\n",
      "+----------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "|       CREAM|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"Description\"), \" \")).show(3, False)\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\")).selectExpr(\"array_col[0]\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrayType: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayDF = df.select(split(col(\"Description\"), \" \").alias(\"arrayType\"))\n",
    "arrayDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|size(split(Description,  ))|\n",
      "+---------------------------+\n",
      "|                          5|\n",
      "|                          3|\n",
      "|                          5|\n",
      "+---------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(size(split(col(\"Description\"), \" \"))).show(3) # shows each row's array length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|array_contains(split(Description,  ), WHITE)|\n",
      "+--------------------------------------------+\n",
      "|                                        true|\n",
      "|                                        true|\n",
      "|                                       false|\n",
      "+--------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(3) # search within array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+--------+\n",
      "|Description                       |InvoiceNo|exploded|\n",
      "+----------------------------------+---------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HEART   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |T-LIGHT |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HOLDER  |\n",
      "+----------------------------------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Description\", \"InvoiceNo\", \"exploded\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Map Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|complex_map                                   |\n",
      "+----------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER -> 536365]|\n",
      "|[WHITE METAL LANTERN -> 536365]               |\n",
      "|[CREAM CUPID HEARTS COAT HANGER -> 536365]    |\n",
      "+----------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- complex_map: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF = df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n",
    "mapDF.show(3, False)\n",
    "mapDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|null                            |\n",
      "|536365                          |\n",
      "|null                            |\n",
      "+--------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF.selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------+\n",
      "|key                               |value |\n",
      "+----------------------------------+------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365|\n",
      "|WHITE METAL LANTERN               |536365|\n",
      "|CREAM CUPID HEARTS COAT HANGER    |536365|\n",
      "+----------------------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF.selectExpr(\"explode(complex_map)\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _JSON Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|jsonString                                 |\n",
      "+-------------------------------------------+\n",
      "|{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}|\n",
      "+-------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- jsonString: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "  '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\n",
    "jsonDF.show(1, False)\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|col|c0                     |\n",
      "+---+-----------------------+\n",
      "|1  |{\"myJSONValue\":[1,2,3]}|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.select(\\\n",
    "              get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[0]\").alias(\"col\"),\\\n",
    "              json_tuple(col(\"jsonString\"), \"myJSONKey\"))\\\n",
    ".show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|structstojson(myStruct)                                                  |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}    |\n",
      "+-------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\").select(to_json(col(\"myStruct\"))).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "|jsontostructs(newJSON)                      |newJSON                                                                  |\n",
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "|[536365, WHITE HANGING HEART T-LIGHT HOLDER]|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
      "|[536365, WHITE METAL LANTERN]               |{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
      "|[536365, CREAM CUPID HEARTS COAT HANGER]    |{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}    |\n",
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parseSchema = StructType((\n",
    "  StructField(\"InvoiceNo\",StringType(),True),\n",
    "  StructField(\"Description\",StringType(),True)))\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    ".select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _UDF Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "  return double_value ** 3\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.select(power3udf(col(\"num\"))).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|power3py(num)|\n",
      "+-------------+\n",
      "|         null|\n",
      "|         null|\n",
      "|         null|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register UDF type\n",
    "spark.udf.register(\"power3py\", power3, DoubleType())\n",
    "# via expression\n",
    "udfExampleDF.selectExpr(\"power3py(num)\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|power3py(num)|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            1|\n",
      "|            8|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register UDF type\n",
    "spark.udf.register(\"power3py\", power3, IntegerType())\n",
    "# via expression\n",
    "udfExampleDF.selectExpr(\"power3py(num)\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #7 - Aggregations_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  specify _key_ or _grouping_\n",
    "-  specify _aggregation function_ for column(s) transformation\n",
    "-  a _\"group by\"_ takes data where every row can only go in one grouping\n",
    "-  majority of Spark aggregation functions are in the **org.apache.spark.sql.functions (pyspark.sql.functions)** package\n",
    "-  **pyspark.sql.functions** documentation: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions   \n",
    "<br>\n",
    "-  **Grouping Types**:\n",
    "    -  aggregation via select statement ex: (df.count()):\n",
    "        - covers DF level aggregations   \n",
    "        <br>\n",
    "    - \"group by\" on 1 or more keys / 1 or more aggregation functions to transform column(s) ex: (df.groupBy(...)):\n",
    "        - covers grouping data on columns(s) and perform calculations (aggs) on other column(s) assigned to specific group by clause   \n",
    "        <br>\n",
    "    -  \"window\" ex: (rolling averages with each row representing 1 day):\n",
    "        -  computes aggregation of specific window of data\n",
    "        -  returns value for every input row of a table based on a group of rows (frame)\n",
    "        -  typically contains a \"partitions\" (how data is broken up in group)\n",
    "        -  functions:\n",
    "             - ranking (rank vs dense_rank)\n",
    "             - analytic\n",
    "             - aggregate   \n",
    "         <br>\n",
    "    -  \"grouping set\":\n",
    "        -  covers aggregation across multiple groups ex: (total quantity by stock codes and customers)   \n",
    "    <br>\n",
    "    -  \"rollup\":\n",
    "        -  multidimensional aggregation that performs a variety of group-by calculations ex: (aggregation across groups by time)\n",
    "        -  \"null\" values indicate grand totals across columns (null in all columns specifies total aggregation across those cols)   \n",
    "    <br>\n",
    "    -  \"cube\":\n",
    "        -  rollup at a deeper level   \n",
    "        -  must filter out NULL values for aggregation levels on cubes, rollups, and grouping sets or else code will compute incorrect results   \n",
    "<br>\n",
    "    -  \"pivot\":\n",
    "        -  convert a row into a column   \n",
    "        <br>\n",
    "    -  \"UDAFs\":\n",
    "        -  user-defined aggregation functions are designed to define custom aggregation functions over groups of input data (as opposed to single rows)\n",
    "        -  currently only available in Scala and Java\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #7 Exercises (DataFrames)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repartition data to have less partition because of small data volume stored in many small files\n",
    "# cache DF for in memory access\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(retailDataAll)\\\n",
    ".coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Count Example_:\n",
    "- count\n",
    "- countDistinct\n",
    "- approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n",
      "None\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n",
      "None\n",
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n",
      "None\n",
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.select(count(\"StockCode\")).show()) # Spark will not count NULLs in individual col\n",
    "print(df.select(count(\"*\")).show()) # Spark will count NULLs\n",
    "\n",
    "print(df.select(countDistinct(\"StockCode\")).show()) # count unique values in col\n",
    "\n",
    "print(df.select(approx_count_distinct(\"StockCode\", 0.1)).show()) # approx count w/ degree of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Boundry Example_:\n",
    "- first and last\n",
    "- min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "from pyspark.sql.functions import min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n",
      "None\n",
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.select(first(\"StockCode\"), last(\"StockCode\")).show()) # return first and last col value in row in DF\n",
    "\n",
    "print(df.select(min(\"Quantity\"), max(\"Quantity\")).show()) # return min and max value in DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Sum Example_:\n",
    "-  sum\n",
    "-  sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n",
      "None\n",
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.select(sum(\"Quantity\")).show()) # sum values in col\n",
    "\n",
    "print(df.select(sumDistinct(\"Quantity\")).show()) # sum distinct set of values in col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Average Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(total_transactions_COUNT=541909, total_purchases_SUM=5176450, (total_purchases_SUM / total_transactions_COUNT)=9.55224954743324, avg_purchases_AVG=9.55224954743324, mean_purchases_MEAN=9.55224954743324)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions_COUNT\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases_SUM\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases_AVG\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases_MEAN\"))\\\n",
    "  .selectExpr(\n",
    "    \"total_transactions_COUNT\",\n",
    "    \"total_purchases_SUM\",\n",
    "    \"total_purchases_SUM/total_transactions_COUNT\",\n",
    "    \"avg_purchases_AVG\",\n",
    "    \"mean_purchases_MEAN\")\\\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Stats Example_:\n",
    "##### Var and Stddev measure the spread of the data around the mean\n",
    "##### Skewness and Kurtosis measure extreme points in data and are used in probability distribution of a random varaible\n",
    "##### Corr and Covar compares the interactions of the values in 2 different variables\n",
    "\n",
    "-  variance (sample and population) __average of squared differences from the mean__\n",
    "-  standard deviation (sample and population) __square root of the variance__\n",
    "-  skewness __measure of asymmetry of values in data around the mean__\n",
    "-  kurtosis __measure of the tail of data__\n",
    "-  covariance (sample and population) __measures variability between 2 different varaibles__\n",
    "-  correlation __measures the Pearson correlation coefficient between -1 and +1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609056|47559.391409298754|  218.08095663447796|   218.08115785023418|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052562|119768.05495536952|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085635685E-4|             1052.7280543902734|            1052.7260778741693|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"), covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Complex Types Example_:\n",
    "-  collect_list [collects list of present values in col]\n",
    "-  collect_set [collects list of unique present values in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Group By Example_:\n",
    "-  groupBy\n",
    "-  agg\n",
    "-  grouping with maps [key is the column; value is the aggregation function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536366|     17850|    2|\n",
      "|   536367|     13047|   12|\n",
      "|   536369|     13047|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536370|  20|             20|\n",
      "|   536380|   1|              1|\n",
      "|   536384|  13|             13|\n",
      "+---------+----+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536370|             22.45|   8.935742834258381|\n",
      "|   536380|              24.0|                 0.0|\n",
      "|   536384|14.615384615384615|  15.750645708563392|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Window Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window\\\n",
    ".partitionBy(\"CustomerId\", \"date\")\\\n",
    ".orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "purchaseDenseRank = dense_rank().over(windowSpec) # dense rank avoids ranking ties\n",
    "purchaseRank = rank().over(windowSpec) # rank does tied values (duplicate rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----+-----+-----------+\n",
      "|CustomerId|      date|Quantity|rank|dRank|maxPurchase|\n",
      "+----------+----------+--------+----+-----+-----------+\n",
      "|     12346|2011-01-18|   74215|   1|    1|      74215|\n",
      "|     12346|2011-01-18|  -74215|   2|    2|      74215|\n",
      "|     12347|2010-12-07|      36|   1|    1|         36|\n",
      "|     12347|2010-12-07|      30|   2|    2|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      24|   3|    3|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "+----------+----------+--------+----+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, date, Quantity,\n",
    "  rank(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                       ORDER BY Quantity DESC NULLS LAST\n",
    "                       ROWS BETWEEN\n",
    "                         UNBOUNDED PRECEDING AND\n",
    "                         CURRENT ROW) as rank,\n",
    "\n",
    "  dense_rank(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                             ORDER BY Quantity DESC NULLS LAST\n",
    "                             ROWS BETWEEN\n",
    "                               UNBOUNDED PRECEDING AND\n",
    "                               CURRENT ROW) as dRank,\n",
    "\n",
    "  max(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                      ORDER BY Quantity DESC NULLS LAST\n",
    "                      ROWS BETWEEN\n",
    "                        UNBOUNDED PRECEDING AND\n",
    "                        CURRENT ROW) as maxPurchase\n",
    "FROM dfWithDate WHERE CustomerId IS NOT NULL ORDER BY CustomerId\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Grouping Set (via Spark SQL) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "+----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "+----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "\"\"\").show(3)\n",
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode))\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Rollup Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|       5176450|\n",
      "|2010-12-01|          null|         26814|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|United Kingdom|         23949|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      null|   null|       5176450|\n",
      "|2010-12-01|   null|         26814|\n",
      "|2010-12-02|   null|         21023|\n",
      "+----------+-------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+-------+--------------+\n",
      "|Date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|null|   null|       5176450|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    ".selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n",
    ".orderBy(\"Date\")\n",
    "rolledUpDF.show(5)\n",
    "rolledUpDF.where(\"Country IS NULL\").show(3)\n",
    "rolledUpDF.where(\"Date IS NULL\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Cube Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n",
      "|Date|             Country|sum(Quantity)|\n",
      "+----+--------------------+-------------+\n",
      "|null|         Switzerland|        30325|\n",
      "|null|                null|      5176450|\n",
      "|null|             Belgium|        23152|\n",
      "|null|     Channel Islands|         9479|\n",
      "|null|               Spain|        26824|\n",
      "|null|             Bahrain|          260|\n",
      "|null|           Singapore|         5234|\n",
      "|null|United Arab Emirates|          982|\n",
      "|null|        Saudi Arabia|           75|\n",
      "|null|             Finland|        10666|\n",
      "+----+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\"))).select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Pivot Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- Australia_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Australia_sum(UnitPrice): double (nullable = true)\n",
      " |-- Australia_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Austria_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Austria_sum(UnitPrice): double (nullable = true)\n",
      " |-- Austria_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Bahrain_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Bahrain_sum(UnitPrice): double (nullable = true)\n",
      " |-- Bahrain_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Belgium_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Belgium_sum(UnitPrice): double (nullable = true)\n",
      " |-- Belgium_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Brazil_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Brazil_sum(UnitPrice): double (nullable = true)\n",
      " |-- Brazil_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Canada_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Canada_sum(UnitPrice): double (nullable = true)\n",
      " |-- Canada_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Channel Islands_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Channel Islands_sum(UnitPrice): double (nullable = true)\n",
      " |-- Channel Islands_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Cyprus_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Cyprus_sum(UnitPrice): double (nullable = true)\n",
      " |-- Cyprus_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Czech Republic_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Czech Republic_sum(UnitPrice): double (nullable = true)\n",
      " |-- Czech Republic_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Denmark_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Denmark_sum(UnitPrice): double (nullable = true)\n",
      " |-- Denmark_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- EIRE_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- EIRE_sum(UnitPrice): double (nullable = true)\n",
      " |-- EIRE_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- European Community_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- European Community_sum(UnitPrice): double (nullable = true)\n",
      " |-- European Community_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Finland_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Finland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Finland_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- France_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- France_sum(UnitPrice): double (nullable = true)\n",
      " |-- France_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Germany_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Germany_sum(UnitPrice): double (nullable = true)\n",
      " |-- Germany_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Greece_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Greece_sum(UnitPrice): double (nullable = true)\n",
      " |-- Greece_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Hong Kong_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Hong Kong_sum(UnitPrice): double (nullable = true)\n",
      " |-- Hong Kong_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Iceland_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Iceland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Iceland_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Israel_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Israel_sum(UnitPrice): double (nullable = true)\n",
      " |-- Israel_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Italy_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Italy_sum(UnitPrice): double (nullable = true)\n",
      " |-- Italy_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Japan_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Japan_sum(UnitPrice): double (nullable = true)\n",
      " |-- Japan_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Lebanon_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Lebanon_sum(UnitPrice): double (nullable = true)\n",
      " |-- Lebanon_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Lithuania_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Lithuania_sum(UnitPrice): double (nullable = true)\n",
      " |-- Lithuania_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Malta_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Malta_sum(UnitPrice): double (nullable = true)\n",
      " |-- Malta_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Netherlands_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Netherlands_sum(UnitPrice): double (nullable = true)\n",
      " |-- Netherlands_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Norway_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Norway_sum(UnitPrice): double (nullable = true)\n",
      " |-- Norway_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Poland_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Poland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Poland_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Portugal_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Portugal_sum(UnitPrice): double (nullable = true)\n",
      " |-- Portugal_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- RSA_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- RSA_sum(UnitPrice): double (nullable = true)\n",
      " |-- RSA_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Saudi Arabia_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Saudi Arabia_sum(UnitPrice): double (nullable = true)\n",
      " |-- Saudi Arabia_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Singapore_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Singapore_sum(UnitPrice): double (nullable = true)\n",
      " |-- Singapore_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Spain_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Spain_sum(UnitPrice): double (nullable = true)\n",
      " |-- Spain_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Sweden_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Sweden_sum(UnitPrice): double (nullable = true)\n",
      " |-- Sweden_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Switzerland_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Switzerland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Switzerland_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- USA_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- USA_sum(UnitPrice): double (nullable = true)\n",
      " |-- USA_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- United Arab Emirates_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- United Arab Emirates_sum(UnitPrice): double (nullable = true)\n",
      " |-- United Arab Emirates_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- United Kingdom_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- United Kingdom_sum(UnitPrice): double (nullable = true)\n",
      " |-- United Kingdom_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Unspecified_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Unspecified_sum(UnitPrice): double (nullable = true)\n",
      " |-- Unspecified_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+\n",
      "|      date|USA_sum(CAST(Quantity AS BIGINT))|\n",
      "+----------+---------------------------------+\n",
      "|2011-12-07|                             null|\n",
      "|2011-12-06|                             null|\n",
      "|2011-12-08|                             -196|\n",
      "+----------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\", \"USA_sum(CAST(Quantity AS BIGINT))\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #8 - Joins_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  brings together 2 sets of data (left dataset and right dataset) by comparing the value of 1 or more keys of the left dataset and right dataset\n",
    "-  equi-join (compares whether the specified keys in your left and right datasets are equal and discards rows that do not have matching keys)   \n",
    "<br>\n",
    "- **Types**:\n",
    "    -  inner joins (keep rows with keys that exist in the left and right datasets)\n",
    "    -  full outer joins (keep rows with keys in either the left or right datasets)\n",
    "    -  left outer joins (keep rows with keys in the left dataset)\n",
    "    -  right outer joins (keep rows with keys in the right dataset)\n",
    "    -  left semi joins (keep rows in the left (only left) dataset where the key appears in the right dataset)\n",
    "    -  left anti joins (keep rows in the left (only left) dataset where they DO NOT appear in the right dataset)\n",
    "    -  natural joins (performs join that matches columns between the 2 datasets with the same names)\n",
    "    -  cross _cartesian_ joins (match every row in the left dataset with every row in the right dataset)   \n",
    "    <br>\n",
    "-  **How Spark Performs Joins**:\n",
    "    -  node-to-node communication strategy\n",
    "    -  per node computation strategy   \n",
    "    <br>\n",
    "-  **Communication Strategies**:\n",
    "    -  **shuffle join** [occurs on \"big table\" to \"big table\" joins where during shuffle every node talks to every other node and they share data according to which node has a certain key or set of keys being joined]\n",
    "    -  **broadcast join** [can be useful for \"small table\" to \"big table\" joins because table is small enough to fit into memory of single worker node]\n",
    "        -  plan is to replicate small DF onto every worker node in the cluster to avoid \"all to all (node to node) communication during the entire join process so worker can perform their own work thus not needing to communicate with other slaves\n",
    "        -  broadcasting tables too large can crash the driver\n",
    "        -  Spark tries to optimize broadcast automatically and can be confimed via EXPLAIN PLAN however can also specifiy broadcast join\n",
    "        -  it is recommended to let Spark optimize computation on \"small table\" to \"small table\" joins\n",
    "        -  Additional Optimization Technique: ***consider partitioning your data \"correctly\" prior to a join as the execution performance can significantly improve thus even if a shuffle is planned Spark can avoid a shuffle is data from the DFs are already located on the same machine***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #8 Exercises (DataFrames)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")])\\\n",
    "  .toDF(\"id\", \"status\")\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person table:\n",
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n",
      "graduateProgram table:\n",
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n",
      "sparkStatus table:\n",
      "+---+--------------+\n",
      "| id|        status|\n",
      "+---+--------------+\n",
      "|500|Vice President|\n",
      "|250|    PMC Member|\n",
      "|100|   Contributor|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"person table:\")\n",
    "person.show()\n",
    "print(\"graduateProgram table:\")\n",
    "graduateProgram.show()\n",
    "print(\"sparkStatus table:\")\n",
    "sparkStatus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _Inner Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              name  graduate_program     spark_status  id   degree  \\\n",
       "0   0     Bill Chambers                 0            [100]   0  Masters   \n",
       "1   1     Matei Zaharia                 1  [500, 250, 100]   1    Ph.D.   \n",
       "2   2  Michael Armbrust                 1       [250, 100]   1    Ph.D.   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  join only rows on keys in both datasets that evaluate to true\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram[\"id\"]\n",
    "person.join(graduateProgram, joinExpression).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, name, graduate_program, spark_status, id, degree, department, school]\n",
       "Index: []"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of key that does not exist in either dataset hence returns no rows\n",
    "wrongJoinExpression = person[\"name\"] == graduateProgram[\"school\"]\n",
    "person.join(graduateProgram, wrongJoinExpression).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              name  graduate_program     spark_status  id   degree  \\\n",
       "0   0     Bill Chambers                 0            [100]   0  Masters   \n",
       "1   1     Matei Zaharia                 1  [500, 250, 100]   1    Ph.D.   \n",
       "2   2  Michael Armbrust                 1       [250, 100]   1    Ph.D.   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# via specifying join type\n",
    "joinType = \"inner\"\n",
    "person.join(graduateProgram, joinExpression, joinType).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Outer Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id              name  graduate_program     spark_status  id   degree  \\\n",
       "0  0.0     Bill Chambers               0.0            [100]   0  Masters   \n",
       "1  NaN              None               NaN             None   2  Masters   \n",
       "2  1.0     Matei Zaharia               1.0  [500, 250, 100]   1    Ph.D.   \n",
       "3  2.0  Michael Armbrust               1.0       [250, 100]   1    Ph.D.   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  \n",
       "3                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  join rows on keys in both datasets and includes rows that evaluate to true or false (listed as nulls)\n",
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Left Outer Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[250, 100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   degree             department       school   id              name  \\\n",
       "0   0  Masters  School of Information  UC Berkeley  0.0     Bill Chambers   \n",
       "1   2  Masters                   EECS  UC Berkeley  NaN              None   \n",
       "2   1    Ph.D.                   EECS  UC Berkeley  1.0     Matei Zaharia   \n",
       "3   1    Ph.D.                   EECS  UC Berkeley  2.0  Michael Armbrust   \n",
       "\n",
       "   graduate_program     spark_status  \n",
       "0               0.0            [100]  \n",
       "1               NaN             None  \n",
       "2               1.0  [500, 250, 100]  \n",
       "3               1.0       [250, 100]  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join rows on keys in both datasets and includes ALL rows from left dataset\n",
    "# as well as any rows in the right dataset that have a match in left dataset\n",
    "# no equivalent row in the right dataset will be listed as null\n",
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Right Outer Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id              name  graduate_program     spark_status  id   degree  \\\n",
       "0  0.0     Bill Chambers               0.0            [100]   0  Masters   \n",
       "1  NaN              None               NaN             None   2  Masters   \n",
       "2  1.0     Matei Zaharia               1.0  [500, 250, 100]   1    Ph.D.   \n",
       "3  2.0  Michael Armbrust               1.0       [250, 100]   1    Ph.D.   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  \n",
       "3                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join rows on keys in both datasets and includes ALL rows from right dataset\n",
    "# as well as any rows in the left dataset that have a match in right dataset\n",
    "# no equivalent row in the left dataset will be listed as null\n",
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Left Semi Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# similar to filter on DF via key specified\n",
    "# compare values (key) to see if the key value exists in the right dataset\n",
    "# if values exists those rows will be kept in the result even if there is a duplicate key in left dataset\n",
    "joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  2|Masters|                EECS|      UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n",
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how duplicates are treated example\n",
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "    (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "gradProgram2.show()\n",
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _Left Anti Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# similar to NOT IN filter on DF via key specified\n",
    "# compare values (key) to see if the key value exists in the right dataset\n",
    "# opposite of semi joins ... only keep values that DO NOT have a corresponding key in the right dataset\n",
    "joinType = \"left_anti\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _Natural Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   degree             department       school              name  \\\n",
       "0   0  Masters  School of Information  UC Berkeley     Bill Chambers   \n",
       "1   2  Masters                   EECS  UC Berkeley  Michael Armbrust   \n",
       "2   1    Ph.D.                   EECS  UC Berkeley     Matei Zaharia   \n",
       "\n",
       "   graduate_program     spark_status  \n",
       "0                 0            [100]  \n",
       "1                 1       [250, 100]  \n",
       "2                 1  [500, 250, 100]  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finds matching columns and returns the results ... always use this join with CAUTION\n",
    "spark.sql(\"select * from graduateProgram NATURAL JOIN person\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Cross Cartesian Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>2</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   degree             department       school  id              name  \\\n",
       "0   0  Masters  School of Information  UC Berkeley   0     Bill Chambers   \n",
       "1   1    Ph.D.                   EECS  UC Berkeley   1     Matei Zaharia   \n",
       "2   1    Ph.D.                   EECS  UC Berkeley   2  Michael Armbrust   \n",
       "\n",
       "   graduate_program     spark_status  \n",
       "0                 0            [100]  \n",
       "1                 1  [500, 250, 100]  \n",
       "2                 1       [250, 100]  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join every row in the left dataset to every row in the right dataset\n",
    "# 1,000 left rows * 1,000 right rows = 100,000 rows returned so CAUTION this join\n",
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           name  graduate_program     spark_status  id   degree  \\\n",
       "0   0  Bill Chambers                 0            [100]   0  Masters   \n",
       "1   0  Bill Chambers                 0            [100]   2  Masters   \n",
       "2   0  Bill Chambers                 0            [100]   1    Ph.D.   \n",
       "3   1  Matei Zaharia                 1  [500, 250, 100]   0  Masters   \n",
       "4   1  Matei Zaharia                 1  [500, 250, 100]   2  Masters   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  \n",
       "3  School of Information  UC Berkeley  \n",
       "4                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.crossJoin(graduateProgram).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Joins On Complex Types Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------------+---------------+---+--------------+\n",
      "|personId|         name|graduate_program|   spark_status| id|        status|\n",
      "+--------+-------------+----------------+---------------+---+--------------+\n",
      "|       0|Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "+--------+-------------+----------------+---------------+---+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.withColumnRenamed(\"id\", \"personId\")\\\n",
    ".join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Handling Duplicate Column Names Example_:\n",
    "1.  different join expression\n",
    "2.  dropping column after join\n",
    "3.  renaming column before join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n",
    "joinExpr = gradProgramDupe[\"graduate_program\"] == person[\"graduate_program\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              name  graduate_program     spark_status  graduate_program  \\\n",
       "0   0     Bill Chambers                 0            [100]                 0   \n",
       "1   1     Matei Zaharia                 1  [500, 250, 100]                 1   \n",
       "2   2  Michael Armbrust                 1       [250, 100]                 1   \n",
       "\n",
       "    degree             department       school  \n",
       "0  Masters  School of Information  UC Berkeley  \n",
       "1    Ph.D.                   EECS  UC Berkeley  \n",
       "2    Ph.D.                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nperson.join(gradProgramDupe, joinExpr).select(\"graduate_program\").show() # triggers error\\n'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# challenge arrises when selecting duplicate column\n",
    "'''\n",
    "person.join(gradProgramDupe, joinExpr).select(\"graduate_program\").show() # triggers error\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "|graduate_program| id|            name|   spark_status| degree|          department|     school|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "|               0|  0|   Bill Chambers|          [100]|Masters|School of Informa...|UC Berkeley|\n",
      "|               1|  1|   Matei Zaharia|[500, 250, 100]|  Ph.D.|                EECS|UC Berkeley|\n",
      "|               1|  2|Michael Armbrust|     [250, 100]|  Ph.D.|                EECS|UC Berkeley|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "person.join(gradProgramDupe, \"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "| id|            name|   spark_status|graduate_program| degree|          department|     school|\n",
      "+---+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|          [100]|               0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "person.join(gradProgramDupe, joinExpr).drop(person[\"graduate_program\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>grad_id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              name  graduate_program     spark_status  grad_id   degree  \\\n",
       "0   0     Bill Chambers                 0            [100]        0  Masters   \n",
       "1   1     Matei Zaharia                 1  [500, 250, 100]        1    Ph.D.   \n",
       "2   2  Michael Armbrust                 1       [250, 100]        1    Ph.D.   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\n",
    "joinExpr = person[\"graduate_program\"] == gradProgram3[\"grad_id\"]\n",
    "person.join(gradProgram3, joinExpr).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Broadcast Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [graduate_program#7770L], [id#7784L], Inner, BuildRight\n",
      ":- *(2) Project [_1#7760L AS id#7768L, _2#7761 AS name#7769, _3#7762L AS graduate_program#7770L, _4#7763 AS spark_status#7771]\n",
      ":  +- *(2) Filter isnotnull(_3#7762L)\n",
      ":     +- Scan ExistingRDD[_1#7760L,_2#7761,_3#7762L,_4#7763]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))\n",
      "   +- *(1) Project [_1#7776L AS id#7784L, _2#7777 AS degree#7785, _3#7778 AS department#7786, _4#7779 AS school#7787]\n",
      "      +- *(1) Filter isnotnull(_1#7776L)\n",
      "         +- Scan ExistingRDD[_1#7776L,_2#7777,_3#7778,_4#7779]\n"
     ]
    }
   ],
   "source": [
    "joinExpr = person[\"graduate_program\"] == graduateProgram[\"id\"]\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #9 - Data Sources_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **6 \"Core\" Data Sources**:\n",
    "    -  CSV (parameters listed in table 9-3):\n",
    "        -  _does not support complex types (ex: array, nested data)_\n",
    "    -  JSON (parameters listed in table 9-4):\n",
    "        -  multiLine (allows for reading in non-line-delimited JSON files)\n",
    "    -  Parquet (parameters listed in table 9-4):\n",
    "        -  columnar compression for saving storage space and optimized select querying\n",
    "        -  default file format for Apache Spark\n",
    "        -  supports complex types (ex: array)\n",
    "        -  enforces own schema when storing data as schema is built into the file itself (so no inference needed)\n",
    "    -  ORC (no parameter options):\n",
    "        -  ORC is further optimized for Hive however works very well with Spark\n",
    "        -  optimized for large streaming reads and finds rows quickly\n",
    "        -  similar to parquet\n",
    "    -  JDBC/ODBC connections (parameters listed in table 9-6):\n",
    "        - JDBC read/write:\n",
    "            -  need JDBC driver for particular database on spark classpath\n",
    "            -  provide proper JAR for driver\n",
    "    -  TXT:\n",
    "        -  each line in file is a record in DF\n",
    "        -  writing to TXT requires source to have ONLY 1 string column or else write will fail   \n",
    "    <br>\n",
    "-  **Community-Created Data Sources**:\n",
    "    -  Cassandra / HBase / MongoDB / Redshift\n",
    "    -  XML / Avro\n",
    "    \n",
    "#### Structure for READING data: DataFrameReader.format(...).option(\"key\", \"value\").schema(...).load(...)\n",
    "\n",
    "-  **Reading Data** (spark.read):\n",
    "    -  format\n",
    "    -  schema\n",
    "    -  read mode:\n",
    "        -  _permissive_ (sets all fields to null when encountering a corrupted reocrd / places corrupted records in column (_corrupt_record)\n",
    "        -  _dropMalformed_ (drops rows containing malformed records)\n",
    "        -  _failFast_ (fails immediately when encountering malformed records)\n",
    "    -  options\n",
    "    \n",
    "#### Structure for WRITING data: DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save(...)\n",
    "\n",
    "-  **Writing Data** (dataFrame.write):\n",
    "    -  format\n",
    "    -  options\n",
    "    -  save mode:\n",
    "        -  _append_ (appends output files to the list of files that already exist at that location path)\n",
    "        -  _overwrite_ (completely overwrites files at location path)\n",
    "        -  _errorIfExists_ (shows an error and fails to write if files already exist at that specified location path)\n",
    "        -  _ignore_ (if files exist at the location path do nothing with current DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced I/O Concepts:\n",
    "-  Spark Developers have the ability to control the parallelism of files written by controlling the partitions prior to writing:\n",
    "    -  bucketing\n",
    "    -  partitioning   \n",
    "\n",
    "### Splittable File Types and Compression:\n",
    "-  parquet w/ GZIP compression is recommended\n",
    "\n",
    "### Reading Data in Parallel:\n",
    "-  multiple executors cannot read from the same file at the same time however can read different files at the same time\n",
    "-  each file in a folder becomes a partition in a DF and will be read by available executors in parallel\n",
    "\n",
    "### Writing Data in Parallel:\n",
    "-  _number of files written (output) is dependent on the number of partitions the DF object has when you write out the data_\n",
    "-  by default, 1 file is written / partition of the data\n",
    "\n",
    "#### File Organization Approach (methods for controlling the data that is specifically written to each output file):\n",
    "#### 1. Partitioning \"Partition By\":\n",
    "-  encodes a column as a folder\n",
    "-  output files contain data based on partitionBy predicate\n",
    "\n",
    "#### 2. Bucketing:\n",
    "-  groups data by bucket ID\n",
    "-  helps with avoiding shuffles when joining or aggregating\n",
    "-  supported only for Spark managed tables\n",
    "\n",
    "### Writing Complex Types:\n",
    "-  the best file format often depends on the type of data being read and processed\n",
    "\n",
    "### Managing File Size:\n",
    "-  try to avoid \"lots of small files\" aka \"small file problem\" because it requires a lot of metadata to manage\n",
    "-  Spark does not work well with many small files or many large files\n",
    "-  \"maxRecordsPerFile\" helps will controlling the file size / # of records written to each file (ex: df.write.option(\"maxRecordsPerFile\", 5000))\n",
    "-  always be aware of the # of partitions at write time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #9 Exercises (Data Sources)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _CSV Read Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myManualSchema = StructType([\\\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\\\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\\\n",
    "    StructField(\"count\", LongType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"mode\", \"FAILFAST\")\\\n",
    ".schema(myManualSchema)\\\n",
    ".load(flightDataCSV2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvFile.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _CSV Write Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\\\n",
    ".save(\"/Users/grp/sparkTheDefinitiveGuide/tmp/csvWrite.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reflects # of files outputed to target save directory\n",
    "csvFile.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States\tRomania\t1\r\n",
      "United States\tIreland\t264\r\n",
      "United States\tIndia\t69\r\n",
      "Egypt\tUnited States\t24\r\n",
      "Equatorial Guinea\tUnited States\t1\r\n",
      "United States\tSingapore\t25\r\n",
      "United States\tGrenada\t54\r\n",
      "Costa Rica\tUnited States\t477\r\n",
      "Senegal\tUnited States\t29\r\n",
      "United States\tMarshall Islands\t44\r\n"
     ]
    }
   ],
   "source": [
    "# 1 row per line tab delimited\n",
    "!head /Users/grp/sparkTheDefinitiveGuide/tmp/csvWrite.csv/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _JSON Read Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(flightDataJson2010).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _JSON Write Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"json\").mode(\"overwrite\")\\\n",
    ".save(\"/Users/grp/sparkTheDefinitiveGuide/tmp/jsonWrite.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"DEST_COUNTRY_NAME\":\"United States\",\"ORIGIN_COUNTRY_NAME\":\"Romania\",\"count\":1}\r\n",
      "{\"DEST_COUNTRY_NAME\":\"United States\",\"ORIGIN_COUNTRY_NAME\":\"Ireland\",\"count\":264}\r\n",
      "{\"DEST_COUNTRY_NAME\":\"United States\",\"ORIGIN_COUNTRY_NAME\":\"India\",\"count\":69}\r\n",
      "{\"DEST_COUNTRY_NAME\":\"Egypt\",\"ORIGIN_COUNTRY_NAME\":\"United States\",\"count\":24}\r\n",
      "{\"DEST_COUNTRY_NAME\":\"Equatorial Guinea\",\"ORIGIN_COUNTRY_NAME\":\"United States\",\"count\":1}\r\n",
      "{\"DEST_COUNTRY_NAME\":\"United States\",\"ORIGIN_COUNTRY_NAME\":\"Singapore\",\"count\":25}\r\n",
      "{\"DEST_COUNTRY_NAME\":\"United States\",\"ORIGIN_COUNTRY_NAME\":\"Grenada\",\"count\":54}\r\n",
      "{\"DEST_COUNTRY_NAME\":\"Costa Rica\",\"ORIGIN_COUNTRY_NAME\":\"United States\",\"count\":477}\r\n",
      "{\"DEST_COUNTRY_NAME\":\"Senegal\",\"ORIGIN_COUNTRY_NAME\":\"United States\",\"count\":29}\r\n",
      "{\"DEST_COUNTRY_NAME\":\"United States\",\"ORIGIN_COUNTRY_NAME\":\"Marshall Islands\",\"count\":44}\r\n"
     ]
    }
   ],
   "source": [
    "# 1 JSON object per line\n",
    "!head /Users/grp/sparkTheDefinitiveGuide/tmp/jsonWrite.json/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Parquet Read Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\")\\\n",
    ".load(flightDataParquet2010).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Parquet Write Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    ".save(\"/Users/grp/sparkTheDefinitiveGuide/tmp/parquetWrite.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 parquet file\n",
    "# !head /Users/grp/sparkTheDefinitiveGuide/tmp/parquetWrite.parquet/*.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _ORC Read Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"orc\")\\\n",
    ".load(flightDataORC2010).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _ORC Write Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"orc\").mode(\"overwrite\")\\\n",
    ".save(\"/Users/grp/sparkTheDefinitiveGuide/tmp/orcWrite.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 orc file\n",
    "# !head /Users/grp/sparkTheDefinitiveGuide/tmp/orcWrite.orc/*.orc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _SQLite SQL Database Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrun via shell: \\npyspark --master local[8] --driver-class-path /Users/grp/sparkTheDefinitiveGuide/sqlite-jdbc-3.8.6.jar --jars /Users/grp/sparkTheDefinitiveGuide/sqlite-jdbc-3.8.6.jar\\n'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "run via shell: \n",
    "pyspark \\\n",
    "--master local[8] \\\n",
    "--driver-class-path /Users/grp/sparkTheDefinitiveGuide/sqlite-jdbc-3.8.6.jar \\\n",
    "--jars /Users/grp/sparkTheDefinitiveGuide/sqlite-jdbc-3.8.6.jar\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = \"org.sqlite.JDBC\"\n",
    "path = sqliteJDBC\n",
    "url = \"jdbc:sqlite:\" + path\n",
    "tablename = \"flight_info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dbDataFrame = spark.read\\\n",
    ".format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\",  driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: decimal(20,0) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   DEST_COUNTRY_NAME|\n",
      "+--------------------+\n",
      "|   Equatorial Guinea|\n",
      "|             Bolivia|\n",
      "|Turks and Caicos ...|\n",
      "|            Pakistan|\n",
      "|    Marshall Islands|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.select(\"DEST_COUNTRY_NAME\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Additional Options Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npgDF = spark.read.format(\"jdbc\")  .option(\"driver\", \"org.postgresql.Driver\")  .option(\"url\", \"jdbc:postgresql://database_server\")  .option(\"dbtable\", \"schema.tablename\")  .option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()\\n'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pgDF = spark.read.format(\"jdbc\")\\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "  .option(\"url\", \"jdbc:postgresql://database_server\")\\\n",
    "  .option(\"dbtable\", \"schema.tablename\")\\\n",
    "  .option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Query Pushdown  Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#8455], functions=[])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#8455, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#8455], functions=[])\n",
      "      +- *(1) Scan JDBCRelation(flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#8455] PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.select(\"DEST_COUNTRY_NAME\").distinct().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Query Pushdown Filter To DB Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#8455,ORIGIN_COUNTRY_NAME#8456,count#8457] PushedFilters: [*In(DEST_COUNTRY_NAME, [Anguilla,Sweden])], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:decimal(20,0)>\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.filter(\"DEST_COUNTRY_NAME in ('Anguilla', 'Sweden')\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Specify SQL Query (Query Result of Query) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pushdownQuery = \"\"\"(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info\"\"\"\n",
    "dbDataFrame = spark.read\\\n",
    ".format(\"jdbc\").option(\"url\", url).option(\"dbtable\", pushdownQuery).option(\"driver\",  driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation((SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#8471] PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Parallel Reads From DB Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numPartitions specifies max num of partitions to how much spark is reading and writing in parallel\n",
    "# partitions = level of parallelism\n",
    "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
    ".option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\",  driver).option(\"numPartitions\", 10).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Parallel Predicate Pushdown To DB Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|           Sweden|      United States|   65|\n",
      "|    United States|             Sweden|   73|\n",
      "|         Anguilla|      United States|   21|\n",
      "|    United States|           Anguilla|   20|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
    "predicates = [\n",
    "  \"DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'\",\n",
    "  \"DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'\"]\n",
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).show()\n",
    "spark.read.jdbc(url,tablename,predicates=predicates,properties=props)\\\n",
    "  .rdd.getNumPartitions() # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Parallel Predicate Pushdown To DB w/ Duplicate Rows Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
    "predicates = [\n",
    "  \"DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'\",\n",
    "  \"DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'\"]\n",
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).rdd.getNumPartitions() #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Sliding Window Partition Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min and max partition\n",
    "colName = \"count\"\n",
    "lowerBound = 0\n",
    "upperBound = 348113\n",
    "numPartitions = 10\n",
    "\n",
    "spark.read.jdbc(url, tablename, column=colName, properties=props,\n",
    "                lowerBound=lowerBound, upperBound=upperBound,\n",
    "                numPartitions=numPartitions).count() # 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(url, tablename, column=colName, properties=props,\n",
    "                lowerBound=lowerBound, upperBound=upperBound,\n",
    "                numPartitions=numPartitions).rdd.getNumPartitions() #10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Writing To SQL DB Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newPath = \"jdbc:sqlite://Users/grp/sparkTheDefinitiveGuide/tmp/sqlWrite.db\"\n",
    "csvFile.write.jdbc(newPath, tablename, mode=\"overwrite\", properties=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1 db file\n",
    "# !head /Users/grp/sparkTheDefinitiveGuide/tmp/sqlWrite.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Reading Results Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(newPath, tablename, properties=props).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Append Write/Re-read Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvFile.write.jdbc(newPath, tablename, mode=\"append\", properties=props)\n",
    "spark.read.jdbc(newPath, tablename, properties=props).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _TXT Read Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|rows                                           |\n",
      "+-----------------------------------------------+\n",
      "|[DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count]|\n",
      "|[United States, Romania, 1]                    |\n",
      "|[United States, Ireland, 264]                  |\n",
      "+-----------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text(flightDataCSV2010).selectExpr(\"split(value, ',') as rows\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _TXT Write Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile.select(\"DEST_COUNTRY_NAME\").write.mode(\"overwrite\")\\\n",
    ".text(\"/Users/grp/sparkTheDefinitiveGuide/tmp/txtWrite.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States\r\n",
      "United States\r\n",
      "United States\r\n",
      "Egypt\r\n",
      "Equatorial Guinea\r\n",
      "United States\r\n",
      "United States\r\n",
      "Costa Rica\r\n",
      "Senegal\r\n",
      "United States\r\n"
     ]
    }
   ],
   "source": [
    "# 1 txt file\n",
    "!head /Users/grp/sparkTheDefinitiveGuide/tmp/txtWrite.txt/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _TXT Write Partition By Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\").write.mode(\"overwrite\").partitionBy(\"count\")\\\n",
    ".text(\"/Users/grp/sparkTheDefinitiveGuide/tmp/partitionWrite.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=69\n",
      "._SUCCESS.crc\n",
      "count=29\n",
      "count=264\n",
      "count=44\n",
      "count=54\n",
      "count=477\n",
      "_SUCCESS\n",
      "count=1\n",
      "count=24\n",
      "count=25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in os.listdir(\"/Users/grp/sparkTheDefinitiveGuide/tmp/partitionWrite.txt\"): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States\n",
      "Equatorial Guinea\n",
      "\n",
      "\n",
      "Egypt\n",
      "\n",
      "\n",
      "United States\n",
      "\n",
      "\n",
      "United States\n"
     ]
    }
   ],
   "source": [
    "# files split by count column\n",
    "!head /Users/grp/sparkTheDefinitiveGuide/tmp/partitionWrite.txt/count=1/*.txt\n",
    "print(\"\\n\")\n",
    "!head /Users/grp/sparkTheDefinitiveGuide/tmp/partitionWrite.txt/count=24/*.txt\n",
    "print(\"\\n\")\n",
    "!head /Users/grp/sparkTheDefinitiveGuide/tmp/partitionWrite.txt/count=25/*.txt\n",
    "print(\"\\n\")\n",
    "!head /Users/grp/sparkTheDefinitiveGuide/tmp/partitionWrite.txt/count=264/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Writing Data in Parallel Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile.repartition(5).write.mode(\"overwrite\").format(\"csv\")\\\n",
    ".save(\"/Users/grp/sparkTheDefinitiveGuide/tmp/repartitionWrite.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".part-00004-b24e2d82-a563-4d78-807c-6d3915c5d806-c000.csv.crc\n",
      "._SUCCESS.crc\n",
      "part-00004-b24e2d82-a563-4d78-807c-6d3915c5d806-c000.csv\n",
      ".part-00001-b24e2d82-a563-4d78-807c-6d3915c5d806-c000.csv.crc\n",
      "part-00003-b24e2d82-a563-4d78-807c-6d3915c5d806-c000.csv\n",
      ".part-00000-b24e2d82-a563-4d78-807c-6d3915c5d806-c000.csv.crc\n",
      ".part-00002-b24e2d82-a563-4d78-807c-6d3915c5d806-c000.csv.crc\n",
      "part-00002-b24e2d82-a563-4d78-807c-6d3915c5d806-c000.csv\n",
      ".part-00003-b24e2d82-a563-4d78-807c-6d3915c5d806-c000.csv.crc\n",
      "_SUCCESS\n",
      "part-00001-b24e2d82-a563-4d78-807c-6d3915c5d806-c000.csv\n",
      "part-00000-b24e2d82-a563-4d78-807c-6d3915c5d806-c000.csv\n"
     ]
    }
   ],
   "source": [
    "# files split into 5 partitions\n",
    "for i in os.listdir(\"/Users/grp/sparkTheDefinitiveGuide/tmp/repartitionWrite.csv/\"): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barbados,United States,130\r\n",
      "United States,Fiji,51\r\n",
      "United States,Senegal,46\r\n",
      "New Zealand,United States,86\r\n",
      "Kiribati,United States,17\r\n",
      "Afghanistan,United States,11\r\n",
      "Latvia,United States,12\r\n",
      "United States,Luxembourg,90\r\n",
      "United States,Angola,18\r\n",
      "United States,Cyprus,1\r\n"
     ]
    }
   ],
   "source": [
    "! head /Users/grp/sparkTheDefinitiveGuide/tmp/repartitionWrite.csv/part-00001*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Partitioning (partitionBy) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".save(\"/Users/grp/sparkTheDefinitiveGuide/tmp/partitionByWrite.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEST_COUNTRY_NAME=United States\n",
      "DEST_COUNTRY_NAME=Costa Rica\n",
      "._SUCCESS.crc\n",
      "DEST_COUNTRY_NAME=Senegal\n",
      "DEST_COUNTRY_NAME=Equatorial Guinea\n",
      "_SUCCESS\n",
      "DEST_COUNTRY_NAME=Egypt\n"
     ]
    }
   ],
   "source": [
    "# files split by predicate\n",
    "for i in os.listdir(\"/Users/grp/sparkTheDefinitiveGuide/tmp/partitionByWrite.parquet/\"): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Bucketing Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numberBuckets = 10\n",
    "columnToBucketBy = \"count\"\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    ".bucketBy(numberBuckets, columnToBucketBy).mode(\"overwrite\").saveAsTable(\"bucketedFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00007.c000.snappy.parquet.crc\n",
      ".part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00008.c000.snappy.parquet.crc\n",
      "part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00008.c000.snappy.parquet\n",
      "part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00009.c000.snappy.parquet\n",
      ".part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00003.c000.snappy.parquet.crc\n",
      "part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00006.c000.snappy.parquet\n",
      "._SUCCESS.crc\n",
      ".part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00000.c000.snappy.parquet.crc\n",
      "part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00007.c000.snappy.parquet\n",
      "part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00004.c000.snappy.parquet\n",
      "part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00005.c000.snappy.parquet\n",
      ".part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00004.c000.snappy.parquet.crc\n",
      "part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00003.c000.snappy.parquet\n",
      "part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00002.c000.snappy.parquet\n",
      "part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00001.c000.snappy.parquet\n",
      "part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00000.c000.snappy.parquet\n",
      ".part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00002.c000.snappy.parquet.crc\n",
      ".part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00009.c000.snappy.parquet.crc\n",
      ".part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00006.c000.snappy.parquet.crc\n",
      "_SUCCESS\n",
      ".part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00005.c000.snappy.parquet.crc\n",
      ".part-00000-417cbe3f-8759-4ed4-b863-80f242df250f_00001.c000.snappy.parquet.crc\n"
     ]
    }
   ],
   "source": [
    "# stored in spark-warehouse local dir or /user/hive/warehouse on a cluster\n",
    "for i in os.listdir(\"/Users/grp/sparkNotebooks/spark-warehouse/bucketedfiles/\"): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #10 - Spark SQL_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ability to run SQL queries against views or tables in databases\n",
    "-  SQL or \"Structured Query Language\" is a language expressing relational operations over data (manipulations, definitions, controls)\n",
    "-  supports both ANSI-SQL and HiveQL queries\n",
    "-  intended to operate as an OLAP DB not a OLTP DB for low-latency queries\n",
    "-  a Spark SQL CLI is available via ./bin/spark-sql\n",
    "-  ad hoc Spark SQL via SparkSession as spark.sql(...)\n",
    "-  includes an JDBC/ODBC server started via ./sbin/start-thriftserver.sh\n",
    "\n",
    "### Spark's Relationship to Hive:\n",
    "-  connects to Hive metastores (where Hive maintains table information)  \n",
    "-  configure Hive with Spark by placing hive-site.xml, core-site.xml, and hdfs-site.xml files in conf/\n",
    "\n",
    "### Catalog:\n",
    "-  stores metadata about data stored in tables, databases, functions, views\n",
    "-  available in the _org.apache.spark.sql.catalog.Catalog_ package\n",
    "\n",
    "### Tables:\n",
    "-  structure of data\n",
    "-  hold information - data and metadata (data about the tables)\n",
    "-  tables are defined in database and dataframes are defined within programming language code\n",
    "-  tables will be assigned a database (_default_ is default database) ... \"show tables in DATABASENAME\"\n",
    "-  Types:\n",
    "    -  Managed:\n",
    "        -  saveAsTable on DF\n",
    "        -  writes to default Hive Warehouse location (/user/hive/warehouse)\n",
    "    -  Unmanaged:\n",
    "        - define a table structure from files on disk\n",
    "\n",
    "### Dropping Tables:\n",
    "-  dropping a managed table the data and the table definition will be removed\n",
    "-  \"DROP TABLE IF EXISTS\" will delete the table and the data if the table exists\n",
    "-  dropping a unmanaged table the underlying table will be removed however the data will still exist in linked directory\n",
    "\n",
    "### Metadata:\n",
    "-  can describe the table's metadata\n",
    "-  REFRESH TABLE refreshes all cached files linked to table\n",
    "-  REPAIR TABLE refreshes partitions / collects new partitions maintained in the Catalog\n",
    "\n",
    "### Views:\n",
    "-  set of transformations (saved query) on top of an existing table\n",
    "-  equivalent to creating a new DF from an existing DF\n",
    "-  when dropping a VIEW the underlying data is not removed just the view definition itself\n",
    "-  Types:\n",
    "    -  global [viewable across entire Spark Application however removed at end of session] (GLOBAL TEMPORARY VIEW)\n",
    "    -  set to a database\n",
    "    -  per session [only available during current sesion and not registered to a database] (TEMPORARY VIEW)\n",
    "    \n",
    "### Databases:\n",
    "-  organize tables\n",
    "\n",
    "### Spark SQL Complex Types:\n",
    "-  structs [provide a way of creating or querying nested data in Spark]\n",
    "-  lists [array]\n",
    "-  maps\n",
    "\n",
    "### Functions:\n",
    "-  system functions as well as user defined functions\n",
    "\n",
    "### Subqueries:\n",
    "-  queries within other queries\n",
    "-  Kinds:\n",
    "    -  correlated subqueries [use logic from outer scope of query in inner query to suplement information in subquery]\n",
    "    -  uncorrelated subqueries [includes no information from the outer scope of the query]\n",
    "    -  predicate subqueries [filtering based on values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #10 Exercises (SQL)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop database if exists sparkSQL CASCADE\")\n",
    "spark.sql(\"create database sparkSQL\")\n",
    "spark.sql(\"use sparkSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists flights_from_select\")\n",
    "spark.sql(\"drop table if exists flights\")\n",
    "spark.sql(\"drop table if exists flights_csv\")\n",
    "spark.sql(\"drop table if exists hive_flights\")\n",
    "spark.sql(\"drop table if exists partitioned_flights\")\n",
    "spark.sql(\"drop table if exists nested_data\")\n",
    "spark.sql(\"drop table if exists just_usa_view\")\n",
    "spark.sql(\"drop table if exists just_usa_view_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark DF and SQL Integration Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.read.json(flightDataJson2015).createOrReplaceTempView(\"some_sql_view\") # DF => SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql\\\n",
    "(\n",
    "\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\"\n",
    ")\\\n",
    ".where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\\n",
    ".count() # SQL => DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Creating Spark Managed Tables Example_:\n",
    "-  USING syntax uses Spark Serialization whereas STORED AS uses Hive SerDe configuration which is much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql\\\n",
    "(\n",
    "\"\"\"\n",
    "CREATE TABLE flights (DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "USING JSON OPTIONS (path '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/json/2015-summary.json')\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|  DEST_COUNTRY_NAME|   string|   null|\n",
      "|ORIGIN_COUNTRY_NAME|   string|   null|\n",
      "|              count|   bigint|   null|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe flights\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from flights\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql\\\n",
    "(\n",
    "\"\"\"\n",
    "CREATE TABLE flights_csv (DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING COMMENT \"remember, the US will be most prevalent\",\n",
    "count LONG) USING csv OPTIONS (header true, path '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/csv/2015-summary.csv')\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------------------------------+\n",
      "|col_name           |data_type|comment                                |\n",
      "+-------------------+---------+---------------------------------------+\n",
      "|DEST_COUNTRY_NAME  |string   |null                                   |\n",
      "|ORIGIN_COUNTRY_NAME|string   |remember, the US will be most prevalent|\n",
      "|count              |bigint   |null                                   |\n",
      "+-------------------+---------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe flights_csv\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from flights_csv\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE flights_from_select USING parquet AS SELECT * FROM flights\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS flights_from_select AS SELECT * FROM flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Creating Spark Managed Partitioned Table Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql\\\n",
    "(\n",
    "\"\"\"\n",
    "CREATE TABLE partitioned_flights USING parquet PARTITIONED BY (DEST_COUNTRY_NAME)\n",
    "AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----------------+\n",
      "|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME|\n",
      "+-------------------+-----+-----------------+\n",
      "|            Romania|   15|    United States|\n",
      "|            Croatia|    1|    United States|\n",
      "|            Ireland|  344|    United States|\n",
      "+-------------------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from partitioned_flights\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-------+\n",
      "|col_name               |data_type|comment|\n",
      "+-----------------------+---------+-------+\n",
      "|ORIGIN_COUNTRY_NAME    |string   |null   |\n",
      "|count                  |bigint   |null   |\n",
      "|DEST_COUNTRY_NAME      |string   |null   |\n",
      "|# Partition Information|         |       |\n",
      "|# col_name             |data_type|comment|\n",
      "|DEST_COUNTRY_NAME      |string   |null   |\n",
      "+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe partitioned_flights\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|partition                 |\n",
      "+--------------------------+\n",
      "|DEST_COUNTRY_NAME=Algeria |\n",
      "|DEST_COUNTRY_NAME=Angola  |\n",
      "|DEST_COUNTRY_NAME=Anguilla|\n",
      "+--------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show PARTITIONS partitioned_flights\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Creating External Table Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql\\\n",
    "(\n",
    "\"\"\"\n",
    "CREATE EXTERNAL TABLE hive_flights (DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/Users/grp/sparkTheDefinitiveGuide/data/flight-data-hive/'\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+\n",
      "|database|          tableName|isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "|sparksql|            flights|      false|\n",
      "|sparksql|        flights_csv|      false|\n",
      "|sparksql|flights_from_select|      false|\n",
      "|sparksql|       hive_flights|      false|\n",
      "|sparksql|partitioned_flights|      false|\n",
      "|        |          complexdf|       true|\n",
      "|        |         datatable2|       true|\n",
      "|        |          datetable|       true|\n",
      "|        |           dfnonull|       true|\n",
      "|        |            dftable|       true|\n",
      "|        |         dfwithdate|       true|\n",
      "|        |    graduateprogram|       true|\n",
      "|        |             person|       true|\n",
      "|        |      some_sql_view|       true|\n",
      "|        |        sparkstatus|       true|\n",
      "+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Insert and Insert Partition Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql\\\n",
    "(\n",
    "\"\"\"\n",
    "INSERT INTO flights_from_select SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql\\\n",
    "(\n",
    "\"\"\"\n",
    "INSERT INTO partitioned_flights\n",
    "PARTITION (DEST_COUNTRY_NAME=\"UNITED STATES\")\n",
    "SELECT count, ORIGIN_COUNTRY_NAME FROM flights\n",
    "WHERE DEST_COUNTRY_NAME='UNITED STATES'\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Metadata Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"REFRESH table partitioned_flights\")\n",
    "spark.sql(\"MSCK REPAIR TABLE partitioned_flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Dropping Tables Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspark.sql(\"DROP TABLE flights\")\\nspark.sql(\"DROP TABLE IF EXISTS flights\")\\n'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "spark.sql(\"DROP TABLE flights\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS flights\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Cache/UnCache Tables Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspark.sql(\"CACHE TABLE flights\")\\nspark.sql(\"UNCACHE TABLE flights\")\\n'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "spark.sql(\"CACHE TABLE flights\")\n",
    "spark.sql(\"UNCACHE TABLE flights\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Views Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE VIEW just_usa_view AS\n",
    "SELECT * FROM flights WHERE dest_country_name = 'United States'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TEMP VIEW just_usa_view_temp AS\n",
    "SELECT * FROM flights WHERE dest_country_name = 'United States'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE GLOBAL TEMP VIEW just_usa_global_view_temp AS\n",
    "SELECT * FROM flights WHERE dest_country_name = 'United States'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW just_usa_view_temp AS\n",
    "SELECT * FROM flights WHERE dest_country_name = 'United States'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+\n",
      "|database|          tableName|isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "|sparksql|            flights|      false|\n",
      "|sparksql|        flights_csv|      false|\n",
      "|sparksql|flights_from_select|      false|\n",
      "|sparksql|       hive_flights|      false|\n",
      "|sparksql|      just_usa_view|      false|\n",
      "|sparksql|partitioned_flights|      false|\n",
      "|        |          complexdf|       true|\n",
      "|        |         datatable2|       true|\n",
      "|        |          datetable|       true|\n",
      "|        |           dfnonull|       true|\n",
      "|        |            dftable|       true|\n",
      "|        |         dfwithdate|       true|\n",
      "|        |    graduateprogram|       true|\n",
      "|        | just_usa_view_temp|       true|\n",
      "|        |             person|       true|\n",
      "|        |      some_sql_view|       true|\n",
      "|        |        sparkstatus|       true|\n",
      "+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Database Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspark.sql(\"show databases\").show()\\nspark.sql(\"create database example\")\\nspark.sql(\"use example\")\\nspark.sql(\"select * from table\").show()\\nspark.sql(\"select * from example.table\").show()\\nspark.sql(\"select current_database()\").show()\\nspark.sql(\"drop database if exists example\")\\n'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "spark.sql(\"show databases\").show()\n",
    "spark.sql(\"create database example\")\n",
    "spark.sql(\"use example\")\n",
    "spark.sql(\"select * from table\").show()\n",
    "spark.sql(\"select * from example.table\").show()\n",
    "spark.sql(\"select current_database()\").show()\n",
    "spark.sql(\"drop database if exists example\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|          sparksql|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select current_database()\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Select Statements Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSELECT [ALL|DISTINCT] named_expression[, named_expression, ...]\\n    FROM relation[, relation, ...]\\n    [lateral_view[, lateral_view, ...]]\\n    [WHERE boolean_expression]\\n    [aggregation [HAVING boolean_expression]]\\n    [ORDER BY sort_expressions]\\n    [CLUSTER BY expressions]\\n    [DISTRIBUTE BY expressions]\\n    [SORT BY sort_expressions]\\n    [WINDOW named_window[, WINDOW named_window, ...]]\\n    [LIMIT num_rows]\\n\\nnamed_expression:\\n    : expression [AS alias]\\n\\nrelation:\\n    | join_relation\\n    | (table_name|query|relation) [sample] [AS alias]\\n    : VALUES (expressions)[, (expressions), ...]\\n          [AS (column_name[, column_name, ...])]\\n\\nexpressions:\\n    : expression[, expression, ...]\\n\\nsort_expressions:\\n    : expression [ASC|DESC][, expression [ASC|DESC], ...]\\n'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SELECT [ALL|DISTINCT] named_expression[, named_expression, ...]\n",
    "    FROM relation[, relation, ...]\n",
    "    [lateral_view[, lateral_view, ...]]\n",
    "    [WHERE boolean_expression]\n",
    "    [aggregation [HAVING boolean_expression]]\n",
    "    [ORDER BY sort_expressions]\n",
    "    [CLUSTER BY expressions]\n",
    "    [DISTRIBUTE BY expressions]\n",
    "    [SORT BY sort_expressions]\n",
    "    [WINDOW named_window[, WINDOW named_window, ...]]\n",
    "    [LIMIT num_rows]\n",
    "\n",
    "named_expression:\n",
    "    : expression [AS alias]\n",
    "\n",
    "relation:\n",
    "    | join_relation\n",
    "    | (table_name|query|relation) [sample] [AS alias]\n",
    "    : VALUES (expressions)[, (expressions), ...]\n",
    "          [AS (column_name[, column_name, ...])]\n",
    "\n",
    "expressions:\n",
    "    : expression[, expression, ...]\n",
    "\n",
    "sort_expressions:\n",
    "    : expression [ASC|DESC][, expression [ASC|DESC], ...]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _CASE ... WHEN ... THEN Statements Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|CASE WHEN (DEST_COUNTRY_NAME = UNITED STATES) THEN 1 WHEN (DEST_COUNTRY_NAME = Egypt) THEN 0 ELSE -1 END|\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                      -1|\n",
      "|                                                                                                      -1|\n",
      "|                                                                                                      -1|\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  CASE WHEN DEST_COUNTRY_NAME = 'UNITED STATES' THEN 1\n",
    "       WHEN DEST_COUNTRY_NAME = 'Egypt' THEN 0\n",
    "       ELSE -1 END\n",
    "FROM partitioned_flights\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Structs Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|country                 |count|\n",
      "+------------------------+-----+\n",
      "|[United States, Romania]|15   |\n",
      "|[United States, Croatia]|1    |\n",
      "|[United States, Ireland]|344  |\n",
      "+------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|count|\n",
      "+-----------------+-----+\n",
      "|United States    |15   |\n",
      "|United States    |1    |\n",
      "|United States    |344  |\n",
      "+-----------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark\\\n",
    ".sql(\\\n",
    "    \"\"\"\n",
    "    CREATE VIEW IF NOT EXISTS nested_data AS\n",
    "    SELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, count FROM flights\n",
    "    \"\"\")\n",
    "spark.sql(\"select * from nested_data\").show(3, False)\n",
    "spark.sql(\"select country.DEST_COUNTRY_NAME, count from nested_data\").show(3, False)\n",
    "spark.sql(\"select country.*, count from nested_data\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Lists Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------------+\n",
      "|new_name|flight_counts|origin_set     |\n",
      "+--------+-------------+---------------+\n",
      "|Algeria |[4]          |[United States]|\n",
      "|Angola  |[15]         |[United States]|\n",
      "|Austria |[62]         |[United States]|\n",
      "+--------+-------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+--------------+\n",
      "|DEST_COUNTRY_NAME|array(1, 2, 3)|\n",
      "+-----------------+--------------+\n",
      "|United States    |[1, 2, 3]     |\n",
      "|United States    |[1, 2, 3]     |\n",
      "|United States    |[1, 2, 3]     |\n",
      "+-----------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+----------------------+\n",
      "|new_name|collect_list(count)[0]|\n",
      "+--------+----------------------+\n",
      "|Algeria |4                     |\n",
      "|Angola  |15                    |\n",
      "|Austria |62                    |\n",
      "+--------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+-----------------+\n",
      "|col|DEST_COUNTRY_NAME|\n",
      "+---+-----------------+\n",
      "|4  |Algeria          |\n",
      "|15 |Angola           |\n",
      "|62 |Austria          |\n",
      "+---+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark\\\n",
    ".sql(\\\n",
    "    \"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME as new_name, collect_list(count) as flight_counts,\n",
    "    collect_set(ORIGIN_COUNTRY_NAME) as origin_set\n",
    "    FROM flights GROUP BY DEST_COUNTRY_NAME\n",
    "    \"\"\")\\\n",
    ".show(3, False)\n",
    "\n",
    "spark.sql(\"SELECT DEST_COUNTRY_NAME, ARRAY(1, 2, 3) FROM flights\").show(3, False)\n",
    "\n",
    "spark\\\n",
    ".sql(\\\n",
    "    \"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME as new_name, collect_list(count)[0]\n",
    "    FROM flights GROUP BY DEST_COUNTRY_NAME\n",
    "    \"\"\")\\\n",
    ".show(3, False)\n",
    "\n",
    "spark\\\n",
    ".sql(\\\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW flights_agg AS\n",
    "    SELECT DEST_COUNTRY_NAME, collect_list(count) as collected_counts\n",
    "    FROM flights GROUP BY DEST_COUNTRY_NAME\n",
    "    \"\"\")\n",
    "\n",
    "spark.sql(\"SELECT explode(collected_counts), DEST_COUNTRY_NAME FROM flights_agg\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Function Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|function|\n",
      "+--------+\n",
      "|       !|\n",
      "|       %|\n",
      "|       &|\n",
      "+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+\n",
      "|function|\n",
      "+--------+\n",
      "|       !|\n",
      "|       %|\n",
      "|       &|\n",
      "+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+\n",
      "|function|\n",
      "+--------+\n",
      "|power3py|\n",
      "+--------+\n",
      "\n",
      "+---------+\n",
      "| function|\n",
      "+---------+\n",
      "|   second|\n",
      "|sentences|\n",
      "|      sha|\n",
      "+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------+\n",
      "|    function|\n",
      "+------------+\n",
      "|collect_list|\n",
      "| collect_set|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(function_desc='Function: collect_list'),\n",
       " Row(function_desc='Class: org.apache.spark.sql.catalyst.expressions.aggregate.CollectList'),\n",
       " Row(function_desc='Usage: collect_list(expr) - Collects and returns a list of non-unique elements.')]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SHOW FUNCTIONS\").show(3) # list of spark functions\n",
    "spark.sql(\"SHOW SYSTEM FUNCTIONS\").show(3) # spark built in functions\n",
    "spark.sql(\"SHOW USER FUNCTIONS\").show(3) # user defined functions\n",
    "spark.sql(\"SHOW FUNCTIONS 's*'\").show(3) # functions that begin with 's'\n",
    "spark.sql(\"SHOW FUNCTIONS LIKE 'collect*'\").show(3) # functions that contain 'collect'\n",
    "spark.sql(\"DESCRIBE FUNCTION collect_list\").collect() # describes function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Subquery Example_:\n",
    "-  correlated subqueries\n",
    "-  uncorrelated subqueries\n",
    "-  predicate subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncorrelated predicate subquery:\n",
      "+-----------------+\n",
      "|dest_country_name|\n",
      "+-----------------+\n",
      "|United States    |\n",
      "|Canada           |\n",
      "|Mexico           |\n",
      "|United Kingdom   |\n",
      "|Japan            |\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|Egypt            |United States      |15   |\n",
      "|Costa Rica       |United States      |588  |\n",
      "|Senegal          |United States      |40   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "correlated predicate subquery:\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "uncorrelated scalar queries:\n",
      "+-----------------+-------------------+-----+-------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|maximum|\n",
      "+-----------------+-------------------+-----+-------+\n",
      "|United States    |Romania            |15   |370002 |\n",
      "|United States    |Croatia            |1    |370002 |\n",
      "|United States    |Ireland            |344  |370002 |\n",
      "+-----------------+-------------------+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# uncorrelated predicate subquery\n",
    "print(\"uncorrelated predicate subquery:\")\n",
    "\n",
    "spark\\\n",
    ".sql(\\\n",
    "    \"\"\"\n",
    "    SELECT dest_country_name FROM flights\n",
    "        GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5\n",
    "    \"\"\")\\\n",
    ".show(5, False)\n",
    "\n",
    "spark\\\n",
    ".sql(\\\n",
    "    \"\"\"\n",
    "    SELECT * FROM flights WHERE origin_country_name IN \n",
    "        (SELECT dest_country_name FROM flights GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5)\n",
    "    \"\"\")\\\n",
    ".show(3, False)\n",
    "\n",
    "# correlated predicate subquery\n",
    "print(\"correlated predicate subquery:\")\n",
    "\n",
    "spark\\\n",
    ".sql(\\\n",
    "    \"\"\"\n",
    "    SELECT * FROM flights f1\n",
    "    WHERE EXISTS (SELECT 1 FROM flights f2\n",
    "                WHERE f1.dest_country_name = f2.origin_country_name)\n",
    "    AND EXISTS (SELECT 1 FROM flights f2\n",
    "                WHERE f2.dest_country_name = f1.origin_country_name)\n",
    "    \"\"\")\\\n",
    ".show(3, False)\n",
    "\n",
    "# uncorrelated scalar queries\n",
    "print(\"uncorrelated scalar queries:\")\n",
    "\n",
    "spark.sql(\"SELECT *, (SELECT max(count) FROM flights) AS maximum FROM flights\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL Configuration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET spark.sql.shuffle.partitions=8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #11 - Datasets_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  solely JVM language feature that only works with Scala and Java programming language\n",
    "-  ability to define the object that each row in your Dataset will consist of\n",
    "-  Scala:\n",
    "    -  define case class object that holds schema\n",
    "    -  Case Class:\n",
    "        -  immutable\n",
    "        -  comparison by structure (schema) instead of value\n",
    "-  Java:\n",
    "    -  define Java Bean\n",
    "-  Spark converts the Spark Row format to the object specified (Scala case class or Java class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #11 Exercises (Scala/Java Datasets)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncase class Flight(DEST_COUNTRY_NAME: String,\\n                  ORIGIN_COUNTRY_NAME: String, count: BigInt)\\n\\n\\n// COMMAND ----------\\n\\nval flightsDF = spark.read\\n  .parquet(\"/data/flight-data/parquet/2010-summary.parquet/\")\\nval flights = flightsDF.as[Flight]\\n\\n\\n// COMMAND ----------\\n\\nflights.show(2)\\n\\n\\n// COMMAND ----------\\n\\nflights.first.DEST_COUNTRY_NAME // United States\\n\\n\\n// COMMAND ----------\\n\\ndef originIsDestination(flight_row: Flight): Boolean = {\\n  return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME\\n}\\n\\n\\n// COMMAND ----------\\n\\nflights.filter(flight_row => originIsDestination(flight_row)).first()\\n\\n\\n// COMMAND ----------\\n\\nflights.collect().filter(flight_row => originIsDestination(flight_row))\\n\\n\\n// COMMAND ----------\\n\\nval destinations = flights.map(f => f.DEST_COUNTRY_NAME)\\n\\n\\n// COMMAND ----------\\n\\nval localDestinations = destinations.take(5)\\n\\n\\n// COMMAND ----------\\n\\ncase class FlightMetadata(count: BigInt, randomData: BigInt)\\n\\nval flightsMeta = spark.range(500).map(x => (x, scala.util.Random.nextLong))\\n  .withColumnRenamed(\"_1\", \"count\").withColumnRenamed(\"_2\", \"randomData\")\\n  .as[FlightMetadata]\\n\\n\\n// COMMAND ----------\\n\\nval flights2 = flights\\n  .joinWith(flightsMeta, flights.col(\"count\") === flightsMeta.col(\"count\"))\\n\\n\\n// COMMAND ----------\\n\\nflights2.selectExpr(\"_1.DEST_COUNTRY_NAME\")\\n\\n\\n// COMMAND ----------\\n\\nflights2.take(2)\\n\\n\\n// COMMAND ----------\\n\\nval flights2 = flights.join(flightsMeta, Seq(\"count\"))\\n\\n\\n// COMMAND ----------\\n\\nval flights2 = flights.join(flightsMeta.toDF(), Seq(\"count\"))\\n\\n\\n// COMMAND ----------\\n\\nflights.groupBy(\"DEST_COUNTRY_NAME\").count()\\n\\n\\n// COMMAND ----------\\n\\nflights.groupByKey(x => x.DEST_COUNTRY_NAME).count()\\n\\n\\n// COMMAND ----------\\n\\nflights.groupByKey(x => x.DEST_COUNTRY_NAME).count().explain\\n\\n\\n// COMMAND ----------\\n\\ndef grpSum(countryName:String, values: Iterator[Flight]) = {\\n  values.dropWhile(_.count < 5).map(x => (countryName, x))\\n}\\nflights.groupByKey(x => x.DEST_COUNTRY_NAME).flatMapGroups(grpSum).show(5)\\n\\n\\n// COMMAND ----------\\n\\ndef grpSum2(f:Flight):Integer = {\\n  1\\n}\\nflights.groupByKey(x => x.DEST_COUNTRY_NAME).mapValues(grpSum2).count().take(5)\\n\\n\\n// COMMAND ----------\\n\\ndef sum2(left:Flight, right:Flight) = {\\n  Flight(left.DEST_COUNTRY_NAME, null, left.count + right.count)\\n}\\nflights.groupByKey(x => x.DEST_COUNTRY_NAME).reduceGroups((l, r) => sum2(l, r))\\n  .take(5)\\n\\n\\n// COMMAND ----------\\n\\nflights.groupBy(\"DEST_COUNTRY_NAME\").count().explain\\n\\n\\n// COMMAND ----------\\n\\n'"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "case class Flight(DEST_COUNTRY_NAME: String,\n",
    "                  ORIGIN_COUNTRY_NAME: String, count: BigInt)\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "val flightsDF = spark.read\n",
    "  .parquet(\"/data/flight-data/parquet/2010-summary.parquet/\")\n",
    "val flights = flightsDF.as[Flight]\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "flights.show(2)\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "flights.first.DEST_COUNTRY_NAME // United States\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "def originIsDestination(flight_row: Flight): Boolean = {\n",
    "  return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME\n",
    "}\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "flights.filter(flight_row => originIsDestination(flight_row)).first()\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "flights.collect().filter(flight_row => originIsDestination(flight_row))\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "val destinations = flights.map(f => f.DEST_COUNTRY_NAME)\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "val localDestinations = destinations.take(5)\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "case class FlightMetadata(count: BigInt, randomData: BigInt)\n",
    "\n",
    "val flightsMeta = spark.range(500).map(x => (x, scala.util.Random.nextLong))\n",
    "  .withColumnRenamed(\"_1\", \"count\").withColumnRenamed(\"_2\", \"randomData\")\n",
    "  .as[FlightMetadata]\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "val flights2 = flights\n",
    "  .joinWith(flightsMeta, flights.col(\"count\") === flightsMeta.col(\"count\"))\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "flights2.selectExpr(\"_1.DEST_COUNTRY_NAME\")\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "flights2.take(2)\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "val flights2 = flights.join(flightsMeta, Seq(\"count\"))\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "val flights2 = flights.join(flightsMeta.toDF(), Seq(\"count\"))\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "flights.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).count()\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).count().explain\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "def grpSum(countryName:String, values: Iterator[Flight]) = {\n",
    "  values.dropWhile(_.count < 5).map(x => (countryName, x))\n",
    "}\n",
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).flatMapGroups(grpSum).show(5)\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "def grpSum2(f:Flight):Integer = {\n",
    "  1\n",
    "}\n",
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).mapValues(grpSum2).count().take(5)\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "def sum2(left:Flight, right:Flight) = {\n",
    "  Flight(left.DEST_COUNTRY_NAME, null, left.count + right.count)\n",
    "}\n",
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).reduceGroups((l, r) => sum2(l, r))\n",
    "  .take(5)\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "flights.groupBy(\"DEST_COUNTRY_NAME\").count().explain\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
