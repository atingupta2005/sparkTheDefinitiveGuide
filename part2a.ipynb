{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: The Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Structured APIs - DataFrames, SQL, and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flightDataJson2015 = '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/json/2015-summary.json'\n",
    "flightDataJson = '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/json/*-summary.json'\n",
    "retailData20101201 = '/Users/grp/sparkTheDefinitiveGuide/data/retail-data/by-day/2010-12-01.csv'\n",
    "retailDataAll = '/Users/grp/sparkTheDefinitiveGuide/data/retail-data/all/*.csv'\n",
    "flightDataCSV2010 = '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/csv/2010-summary.csv'\n",
    "flightDataJson2010 = '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/json/2010-summary.json'\n",
    "flightDataParquet2010 = '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/parquet/2010-summary.parquet'\n",
    "flightDataORC2010 = '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/orc/2010-summary.orc'\n",
    "sqliteJDBC = '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/jdbc/my-sqlite.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #4 - Structured API Overview_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Columns\n",
    "-  Rows\n",
    "-  Spark Types\n",
    "\n",
    "### _How code is executed across the cluster_:\n",
    "1. write DF/DS/SQL\n",
    "2. if valid code, Spark converts this to a **Logical Plan**\n",
    "3. Spark transforms this **Logical Plan** to a **Physical Plan**, checking for optimizations along the way\n",
    "4. Spark executes this **Physical Plan** (RDD manipulations) across the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Catalyst Optimizer analyzes and decides how the code should be executed via a plan_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Plan Execution_:\n",
    "1.  Convert user's code into an Unresolved Logical Plan\n",
    "2.  Unresolved Logical Plan uses the Catalog to that check object (DF/DS/SQL) information is valid\n",
    "3.  If valid, Catalyst Optimizer collects information that attempts to optimize the Logical Plan\n",
    "4.  After Logical Plan is created, Spark plans out the Physical Plan\n",
    "5.  Physical Plan maps process of how logic will be executed on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #5 - Basic Structured Operations_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions:\n",
    "-  **Schema** (defines the column names and types of a DF):\n",
    "    - schema on read is fine for ad hoc use cases however inferring the schema can be expensive and or incorrect\n",
    "    - it is recommended to define schema for production ETL use cases   \n",
    "    <br>\n",
    "-  **Structure**:\n",
    "    - StructType [schema made up of a number of fields]\n",
    "    - StructField [contain name, type, boolean flag specifying whether a column can contain missing or null values]\n",
    "    - Metadata [way of storing information about this column]   \n",
    "    <br>\n",
    "-  **Expressions**:\n",
    "    - operations that select, manipulate, and remove columns from DFs\n",
    "    - set of transformations on one or more values in a record in a DF\n",
    "    - same performance is achieved via DF code vs SQL expression   \n",
    "    <br>\n",
    "-  **Columns**:\n",
    "    - label that represents a value computed on a per record basis by means of an expression\n",
    "    - columns are resolved when compared to column names maintained in the **catalog** of the **analyzer phase**   \n",
    "    <br>\n",
    "-  **Rows**:\n",
    "    -  each row in a DF is a single record of type Row\n",
    "    -  Row objects internally arrays of bytes   \n",
    "    <br>\n",
    "-  **Repartition**:\n",
    "    -  repartition forces a full shuffle of the data\n",
    "    -  typically only use repartition when the future # of partitions is > current # of partitions\n",
    "    -  can be used when paritioning by a set of columns (ex: frequent filter on same column)   \n",
    "    <br>\n",
    "-  **Coalesce**:\n",
    "    -  does not cause a shuffle instead tries to combine partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Transformations:\n",
    "-  add rows or columns\n",
    "-  remove rows or columns\n",
    "-  transform a row into a column\n",
    "-  transform a column into a row\n",
    "-  change order of rows based on the values in columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #5 Exercises (DataFrames)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Schema Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(flightDataJson2015).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Create Schema Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myManualSchema = StructType([\\\n",
    "                            StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\\\n",
    "                            StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\\\n",
    "                            StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\\\n",
    "                            ])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    ".load(flightDataJson2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Columns Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'someColumnName'>\n",
      "Column<b'someColumnName'>\n"
     ]
    }
   ],
   "source": [
    "print(col(\"someColumnName\"))\n",
    "print(column(\"someColumnName\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(flightDataJson2015).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Expressions Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df code\n",
    "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sql code\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Row Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# accessing row elements\n",
    "print(myRow[0])\n",
    "print(myRow[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Creating DF Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(flightDataJson2015)\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Convert Rows to DF Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(some='Hello', col=None, names=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myManualSchema = StructType([\\\n",
    "                            StructField(\"some\", StringType(), True),\\\n",
    "                            StructField(\"col\", StringType(), True),\\\n",
    "                            StructField(\"names\", LongType(), False)\\\n",
    "                            ])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDF = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _select & expr Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\\\n",
    "         expr(\"DEST_COUNTRY_NAME\"),\\\n",
    "         col(\"DEST_COUNTRY_NAME\"),\\\n",
    "         column(\"DEST_COUNTRY_NAME\"))\\\n",
    ".show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\").alias(\"DEST_COUNTRY_NAME\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _selectExpr Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shorthand to select(expr(...))\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _literal [lit] & withColumn Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "|    United States|            Ireland|  344|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "|    United States|            Ireland|  344|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _withColumnRenamed Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Columns w/ Escape Characters Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfWithLongColName = df.withColumn(\"This Long Column-Name\", expr(\"ORIGIN_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "|              Ireland|Ireland|\n",
      "+---------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# backticks (`) required when referencing a column with whitespaces/escape characters in an expression [expr]\n",
    "dfWithLongColName.selectExpr(\"`This Long Column-Name`\",\\\n",
    "                            \"`This Long Column-Name` as `new col`\")\\\n",
    ".show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Removing Columns [drop] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count: bigint, This Long Column-Name: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Changing Column Types [cast] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: bigint]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"count2\", col(\"count\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Filter Rows [filter/where] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"count\") < 2).show(3)\n",
    "df.where(\"count < 2\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use WHERE for multiple \"AND\" filters since Spark performs all FILTER operations at the same time\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Unique Rows [distinct] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Random Samples [sample] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Random Split [randomSplit] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "60\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "print(dataFrames[0].count() > dataFrames[1].count())\n",
    "print(dataFrames[0].count())\n",
    "print(dataFrames[1].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Union Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = df.schema\n",
    "newRows = [\\\n",
    "          Row(\"New Country\", \"Other Country\", 0),\\\n",
    "          Row(\"New Country 2\", \"Other Country 3\", 999999999)\\\n",
    "          ]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(newDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.union(newDF).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.union(newDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Sorting Rows [sort & orderBy] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New Country|      Other Country|    0|\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New Country|      Other Country|    0|\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# default sort is ascending order\n",
    "df.sort(\"count\").show(3)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|    count|\n",
      "+-----------------+-------------------+---------+\n",
      "|    New Country 2|    Other Country 3|999999999|\n",
      "|    United States|      United States|   370002|\n",
      "|    United States|             Canada|     8483|\n",
      "|           Canada|      United States|     8399|\n",
      "|    United States|             Mexico|     7187|\n",
      "+-----------------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Partition Sorts [sortWithinPartitions] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(flightDataJson).sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Limit [limit] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|    count|\n",
      "+-----------------+-------------------+---------+\n",
      "|    New Country 2|    Other Country 3|999999999|\n",
      "|    United States|      United States|   370002|\n",
      "|    United States|             Canada|     8483|\n",
      "+-----------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\").desc()).limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Repartitioning [repartition & coalesce] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ex: shuffle data into 5 partitions based on column then coalesce partitions into 2\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Collect Rows to Driver Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)\n",
      "Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)\n",
      "\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)\n",
      "Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)\n",
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62)\n",
      "Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588)\n",
      "Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40)\n",
      "Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)\n"
     ]
    }
   ],
   "source": [
    "collectDF = df.limit(10)\n",
    "for i in collectDF.take(5): print(i)\n",
    "print(\"\\n\")\n",
    "collectDF.show()\n",
    "for i in collectDF.collect(): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x109e8e390>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #6 - Working with Different Types of Data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Documentation:\n",
    "-  http://spark.apache.org/docs/latest/api/python/#package\n",
    "-  http://spark.apache.org/docs/latest/api/scala/#package   \n",
    "\n",
    "### **Data**:   \n",
    "-  **Booleans**:\n",
    "   -  filters\n",
    "   -  conditionals\n",
    "   -  elements (and, or, true, false)\n",
    "   -  always chain together \"and\" filters as sequential filter   \n",
    "   <br>\n",
    "-  **Numbers**:\n",
    "   -  statFunctions package is useful for statistical inference and functions   \n",
    "<br>\n",
    "-  **Strings**:\n",
    "    -  regular expressions   \n",
    "    <br>\n",
    "-  **Dates & Timestamps**:\n",
    "    -  Spark follows Java SimpleDateFormat standard - https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html\n",
    "    -  correct format recommended is 'yyyy-MM-dd'\n",
    "    -  to_date function [convert string to a date]\n",
    "    -  Spark will return unparsed \"error\" to_date conversions as **null** due to different date/timestamp formats      \n",
    "    <br>\n",
    "-  **Handling Nulls**:\n",
    "    -  Spark recommends to use 'null' for missing/empty data instead of empty strings/other values for optimization purposes\n",
    "    -  options (1. drop nulls; 2. fill nulls with a new value)\n",
    "    -  if column(s) in schema are declared to not have nulls (nullable = false) it does not mean nulls are forbidden to be in that column ... this feature helps spark sql optimize handling for that column ... having null values in columns with (nullable = false) can lead to strange results and performance\n",
    "    -  nulls can be ordered with the following functions (**asc_nulls_first, desc_nulls_first, asc_nulls_last_desc_nulls_last**)   \n",
    "<br>\n",
    "-  **Complex Types**:\n",
    "    -  _structs_ [\"DFs within DFs\" w/ dot syntax]\n",
    "    -  _arrays_ [list of elements]\n",
    "    -  _maps_ [key-value pairs]\n",
    "    - _JSON_ [semi-structured nested dictionary data]   \n",
    "<br>\n",
    "-  **UDFs**:\n",
    "    -  ability to write own custom transformations\n",
    "    -  Spark serializes the UDF on the driver and transfers it over the network to all executor processes hence there can be performance penalties (Python) because:\n",
    "        1. Spark starts a Python process on the worker\n",
    "        2. then Spark serializes all of the data to a format Python can understand\n",
    "        3. then executes the UDF row by row on the data in the Python process\n",
    "        4. then returns the results of the row operations back to the JVM/Spark\n",
    "    -  Python data serialization is expensive bc Spark cannot manage the memory of the worker when the UDF is sent to the workers to be serialized\n",
    "    -  ***Scala/Java UDFs do not have a separate process like Python hence the logic is computed in the JVM***\n",
    "    -  it is a best practice to define return type of UDF since Spark and Python Data Types can be different (_see UDF example_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #6 Exercises (DataFrames)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(retailData20101201)\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Convert to Spark Types Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Boolean Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    ".select(\"InvoiceNo\", \"Description\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|\n",
      "|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "|536367   |22748    |POPPY'S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"InvoiceNo = 536365\").show(5, False)\n",
    "df.where(\"InvoiceNo != 536365\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Boolean Substring [instr] Expression [isin] Filter Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show() # isin() means boolean expression / pipe means \"OR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|   Description|unitPrice|isExpensive|\n",
      "+--------------+---------+-----------+\n",
      "|DOTCOM POSTAGE|   569.77|       true|\n",
      "|DOTCOM POSTAGE|   607.49|       true|\n",
      "+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DOTCodeFilter = col(\"StockCode\") == 'DOT'\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"Description\", \"unitPrice\", \"isExpensive\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|   Description|UnitPrice|isExpensive|\n",
      "+--------------+---------+-----------+\n",
      "|DOTCOM POSTAGE|   569.77|       true|\n",
      "|DOTCOM POSTAGE|   607.49|       true|\n",
      "+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"Description\", \"UnitPrice\", \"isExpensive\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Boolean Expression [eqNullSafe] for NULLs Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|       null|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n",
      "|   536545|    21134|       null|       1|2010-12-01 14:32:00|      0.0|      null|United Kingdom|\n",
      "|   536546|    22145|       null|       1|2010-12-01 14:33:00|      0.0|      null|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"Description\").eqNullSafe(None)).show(3)\n",
    "df.where(col(\"Description\").eqNullSafe(\"null\")).show(3)\n",
    "df.where(col(\"Description\").eqNullSafe(\"NULL\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|(Description <=> NULL)|\n",
      "+----------------------+\n",
      "|                 false|\n",
      "|                  true|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"Description\"].eqNullSafe(None)).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Power Expression [pow] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "+----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "+----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# via select expression\n",
    "df.selectExpr(\"CustomerId\", \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Round Up [round] & Round Down [bround] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, round, bround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Column Correlation [corr] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Summary Stats [describe] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(summary='count', InvoiceNo='3108', StockCode='3108', Description='3098', Quantity='3108', UnitPrice='3108', CustomerID='1968', Country='3108')\n",
      "Row(summary='mean', InvoiceNo='536516.684944841', StockCode='27834.304044117645', Description=None, Quantity='8.627413127413128', UnitPrice='4.151946589446603', CustomerID='15661.388719512195', Country=None)\n",
      "Row(summary='stddev', InvoiceNo='72.89447869788873', StockCode='17407.897548583845', Description=None, Quantity='26.371821677029203', UnitPrice='15.638659854603892', CustomerID='1854.4496996893627', Country=None)\n",
      "Row(summary='min', InvoiceNo='536365', StockCode='10002', Description=' 4 PURPLE FLOCK DINNER CANDLES', Quantity='-24', UnitPrice='0.0', CustomerID='12431.0', Country='Australia')\n",
      "Row(summary='max', InvoiceNo='C536548', StockCode='POST', Description='ZINC WILLIE WINKIE  CANDLE STICK', Quantity='600', UnitPrice='607.49', CustomerID='18229.0', Country='United Kingdom')\n"
     ]
    }
   ],
   "source": [
    "for i in df.describe().collect(): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manual import\n",
    "from pyspark.sql.functions import count, mean, stddev_pop, min, max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Median Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colName = \"UnitPrice\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _View Frequent Column Value Occurences [stat.freqItems] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(StockCode_freqItems=['90214E', '20728', '20755', '21703', '22113', '22524', '22041', '72803A', '72798C', '90181B', '21756', '22694', '90206C', '20970', '21624', '90209C', '84744', '82494L', '22952', '20682', '22583', '21705', '20679', '22220', '90177E', '90214A', '22448', '90214S', '22121', '22802', '84970L', '72818', '90192', '90200C', '22910', '21380', '90211A', '21137', '35271S', '84926A', '20765', '22384', '21524', '22165', '22366', '21221', '21704', '22519', '85035C', '21967', '22114', '22909', '22900', '22447', '21577', '21877', '20726', '85034A', 'DOT', '84658', '21472', '22804', '22222', '72802C', '21739', '22467', '90214H', '22785', '22446', '22197', '20665', '21733', '22731', '21709', '22086', '40001', '85123A'], Quantity_freqItems=[200, 128, 23, 32, 50, 600, 8, 17, 80, -1, -10, 11, 56, 47, 20, -7, 2, 5, 480, -4, 14, 432, 100, 64, 40, 13, 4, -5, 22, 16, -2, 7, 70, 384, 25, 34, 10, 1, 288, 216, 28, 252, 19, 120, 192, 60, 96, 72, 144, 36, 27, 9, 18, 48, 21, 12, 3, -6, -24, 30, 15, 33, 6, 24, -12, -3])\n"
     ]
    }
   ],
   "source": [
    "# be careful; output may be large\n",
    "for i in df.stat.freqItems([\"StockCode\", \"Quantity\"]).collect(): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Unique ID [monotonically...] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "|                            2|\n",
      "|                            3|\n",
      "|                            4|\n",
      "+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(monotonically_increasing_id()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Capitalize per Whitespace [initcap] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|Description                       |initcap(Description)              |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|White Hanging Heart T-light Holder|\n",
      "|WHITE METAL LANTERN               |White Metal Lantern               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |Cream Cupid Hearts Coat Hanger    |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Description\", initcap(col(\"Description\"))).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Uppercase & Lowercase [upper / lower] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|Description                       |lower(Description)                |upper(lower(Description))         |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|white hanging heart t-light holder|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |white metal lantern               |WHITE METAL LANTERN               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |cream cupid hearts coat hanger    |CREAM CUPID HEARTS COAT HANGER    |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Description\"),\\\n",
    "         lower(col(\"Description\")),\\\n",
    "         upper(lower(col(\"Description\")))).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Removing Spaces [lpad, ltrim, rpad, rtrim, trim] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+---+----------+\n",
      "|   ltrim|   rtrim| trim| lp|        rp|\n",
      "+--------+--------+-----+---+----------+\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLOxxxxx|\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLOxxxxx|\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLOxxxxx|\n",
      "+--------+--------+-----+---+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\\\n",
    "         ltrim(lit(\"   HELLO   \")).alias(\"ltrim\"),\\\n",
    "         rtrim(lit(\"   HELLO   \")).alias(\"rtrim\"),\\\n",
    "         trim(lit(\"   HELLO   \")).alias(\"trim\"),\\\n",
    "         lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\\\n",
    "         rpad(lit(\"HELLO\"), 10, \"x\").alias(\"rp\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RegEx Example_:\n",
    "-  regexp_extract\n",
    "-  regexp_replace\n",
    "-  translate\n",
    "-  instr\n",
    "-  locate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+\n",
      "|color_clean                        |Description                        |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|COLOR METAL LANTERN                |WHITE METAL LANTERN                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|COLOR WOOLLY HOTTIE COLOR HEART.   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\" # replace any of these colors with the word \"COLOR\"\n",
    "df.select(\\\n",
    "         regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\\\n",
    "         col(\"Description\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|translate(Description, LET, 137)  |Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHI73 M37A1 1AN73RN               |WHITE METAL LANTERN               |\n",
      "|CR3AM CUPID H3AR7S COA7 HANG3R    |CREAM CUPID HEARTS COAT HANGER    |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(translate(col(\"Description\"), \"LET\", \"137\"), col(\"Description\")).show(3, False) # replace characters w/ intergers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------+\n",
      "|color_clean|Description                        |\n",
      "+-----------+-----------------------------------+\n",
      "|WHITE      |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHITE      |WHITE METAL LANTERN                |\n",
      "|           |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|           |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|RED        |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\\\n",
    "         regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\\\n",
    "         col(\"Description\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    ".where(\"hasSimpleColor\")\\\n",
    ".select(\"Description\")\\\n",
    ".show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, locate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|Description                      |\n",
      "+---------------------------------+\n",
      "|BLUE COAT RACK PARIS FASHION     |\n",
      "|JUMBO  BAG BAROQUE BLACK WHITE   |\n",
      "|BLUE 3 PIECE POLKADOT CUTLERY SET|\n",
      "+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "\n",
    "def color_locator(column, color_string):\n",
    "  return locate(color_string.upper(), column)\\\n",
    "          .cast(\"boolean\")\\\n",
    "          .alias(\"is_\" + color_string)\n",
    "\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(expr(\"*\")) # has to a be Column type\n",
    "\n",
    "df.select(*selectedColumns).where(expr(\"is_black or is_blue\"))\\\n",
    "  .select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS `is_black`'>,\n",
       " Column<b'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS `is_white`'>,\n",
       " Column<b'CAST(locate(RED, Description, 1) AS BOOLEAN) AS `is_red`'>,\n",
       " Column<b'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS `is_green`'>,\n",
       " Column<b'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS `is_blue`'>,\n",
       " Column<b'unresolvedstar()'>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Dates & Timestamps Example_:\n",
    "-  Dates [calendar dates]\n",
    "-  Timestamps [date and time information]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------+\n",
      "|id |today     |now                   |\n",
      "+---+----------+----------------------+\n",
      "|0  |2018-09-30|2018-09-30 15:20:41.86|\n",
      "+---+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.limit(1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|     today|date_sub(today, 5)|date_add(today, 5)|\n",
      "+----------+------------------+------------------+\n",
      "|2018-09-30|        2018-09-25|        2018-10-05|\n",
      "+----------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(col(\"today\"), date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Difference Between 2 Dates [datediff] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of days between 2 dates\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    ".select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|months_between(start, end)|\n",
      "+--------------------------+\n",
      "|              -84.19354839|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of months between 2 dates\n",
    "dateDF.select(\\\n",
    "             to_date(lit(\"2000-01-01\")).alias(\"start\"),\\\n",
    "             to_date(lit(\"2007-01-07\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\")))\\\n",
    ".show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _DateFormat Debugging Part 1 Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\")).select(to_date(col(\"date\"))).show(1)\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1)\n",
    "# month slot does not have '20' hence null error\n",
    "# unable to know if '12' is 12th day or Dec as well as if '11' is 11th day or Nov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _DateFormat Debugging Part 2 Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\\\n",
    "                                   to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\\\n",
    "                                   to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dataTable2\")\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|to_timestamp(`date`, 'yyyy-dd-MM')|\n",
      "+----------------------------------+\n",
      "|               2017-11-12 00:00:00|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n",
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()\n",
    "cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Selecting non-null values [coalesce] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|coalesce(Description, Customerid) |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(coalesce(col(\"Description\"), col(\"Customerid\"))).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Deleting Rows w/ Null [drop] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3108\n",
      "1968\n",
      "1968\n",
      "3108\n",
      "3108\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(df.na.drop().count()) # drops rows if any of the values are NULL\n",
    "print(df.na.drop(\"any\").count()) # drops rows if any of the values are NULL\n",
    "print(df.na.drop(\"all\").count()) # drops rows only if all values are NULLs or NaNs\n",
    "print(df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"]).count()) # drops rows for certain subset of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Filling Rows w/ Null [fill] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type string column\n",
    "df.na.fill(\"All Null values become this string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(9999, subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map key (column) - value (what to fill nulls with) fill\n",
    "fill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Replace Values w/ Values [replace] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|     UK|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|     UK|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|     UK|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)\n",
    "df.na.replace([\"United Kingdom\"], [\"UK\"], \"Country\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Ordering Nulls Example_:\n",
    "-  asc_nulls_first\n",
    "-  desc_nulls_first\n",
    "-  asc_nulls_last\n",
    "-  desc_nulls_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Struct Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[complex: struct<Description:string,InvoiceNo:string>, InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- complex: struct (nullable = false)\n",
      " |    |-- Description: string (nullable = true)\n",
      " |    |-- InvoiceNo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "complexDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|complex                                     |\n",
      "+--------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER, 536365]|\n",
      "+--------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------------------+\n",
      "|complex.Description               |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------------------+---------+\n",
      "|Description                       |InvoiceNo|\n",
      "+----------------------------------+---------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |\n",
      "+----------------------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\").show(1, False)\n",
    "complexDF.select(col(\"complex\").getField(\"Description\")).show(1, False)\n",
    "complexDF.select(\"complex.*\").show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Array (Convert Every Description Token into Row) [explode] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|split(Description,  )                   |\n",
      "+----------------------------------------+\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "|[WHITE, METAL, LANTERN]                 |\n",
      "|[CREAM, CUPID, HEARTS, COAT, HANGER]    |\n",
      "+----------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "|       CREAM|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"Description\"), \" \")).show(3, False)\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\")).selectExpr(\"array_col[0]\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrayType: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayDF = df.select(split(col(\"Description\"), \" \").alias(\"arrayType\"))\n",
    "arrayDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|size(split(Description,  ))|\n",
      "+---------------------------+\n",
      "|                          5|\n",
      "|                          3|\n",
      "|                          5|\n",
      "+---------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(size(split(col(\"Description\"), \" \"))).show(3) # shows each row's array length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|array_contains(split(Description,  ), WHITE)|\n",
      "+--------------------------------------------+\n",
      "|                                        true|\n",
      "|                                        true|\n",
      "|                                       false|\n",
      "+--------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(3) # search within array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+--------+\n",
      "|Description                       |InvoiceNo|exploded|\n",
      "+----------------------------------+---------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HEART   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |T-LIGHT |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HOLDER  |\n",
      "+----------------------------------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Description\", \"InvoiceNo\", \"exploded\")\\\n",
    ".show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Map Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|complex_map                                   |\n",
      "+----------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER -> 536365]|\n",
      "|[WHITE METAL LANTERN -> 536365]               |\n",
      "|[CREAM CUPID HEARTS COAT HANGER -> 536365]    |\n",
      "+----------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- complex_map: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF = df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n",
    "mapDF.show(3, False)\n",
    "mapDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|null                            |\n",
      "|536365                          |\n",
      "|null                            |\n",
      "+--------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF.selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------+\n",
      "|key                               |value |\n",
      "+----------------------------------+------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365|\n",
      "|WHITE METAL LANTERN               |536365|\n",
      "|CREAM CUPID HEARTS COAT HANGER    |536365|\n",
      "+----------------------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF.selectExpr(\"explode(complex_map)\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _JSON Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|jsonString                                 |\n",
      "+-------------------------------------------+\n",
      "|{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}|\n",
      "+-------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- jsonString: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "  '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\n",
    "jsonDF.show(1, False)\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|col|c0                     |\n",
      "+---+-----------------------+\n",
      "|1  |{\"myJSONValue\":[1,2,3]}|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.select(\\\n",
    "              get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[0]\").alias(\"col\"),\\\n",
    "              json_tuple(col(\"jsonString\"), \"myJSONKey\"))\\\n",
    ".show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|structstojson(myStruct)                                                  |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}    |\n",
      "+-------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\").select(to_json(col(\"myStruct\"))).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "|jsontostructs(newJSON)                      |newJSON                                                                  |\n",
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "|[536365, WHITE HANGING HEART T-LIGHT HOLDER]|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
      "|[536365, WHITE METAL LANTERN]               |{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
      "|[536365, CREAM CUPID HEARTS COAT HANGER]    |{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}    |\n",
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parseSchema = StructType((\n",
    "  StructField(\"InvoiceNo\",StringType(),True),\n",
    "  StructField(\"Description\",StringType(),True)))\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    ".select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _UDF Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "  return double_value ** 3\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.select(power3udf(col(\"num\"))).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|power3py(num)|\n",
      "+-------------+\n",
      "|         null|\n",
      "|         null|\n",
      "|         null|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register UDF type\n",
    "spark.udf.register(\"power3py\", power3, DoubleType())\n",
    "# via expression\n",
    "udfExampleDF.selectExpr(\"power3py(num)\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|power3py(num)|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            1|\n",
      "|            8|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register UDF type\n",
    "spark.udf.register(\"power3py\", power3, IntegerType())\n",
    "# via expression\n",
    "udfExampleDF.selectExpr(\"power3py(num)\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #7 - Aggregations_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  specify _key_ or _grouping_\n",
    "-  specify _aggregation function_ for column(s) transformation\n",
    "-  a _\"group by\"_ takes data where every row can only go in one grouping\n",
    "-  majority of Spark aggregation functions are in the **org.apache.spark.sql.functions (pyspark.sql.functions)** package\n",
    "-  **pyspark.sql.functions** documentation: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions   \n",
    "<br>\n",
    "-  **Grouping Types**:\n",
    "    -  aggregation via select statement ex: (df.count()):\n",
    "        - covers DF level aggregations   \n",
    "        <br>\n",
    "    - \"group by\" on 1 or more keys / 1 or more aggregation functions to transform column(s) ex: (df.groupBy(...)):\n",
    "        - covers grouping data on columns(s) and perform calculations (aggs) on other column(s) assigned to specific group by clause   \n",
    "        <br>\n",
    "    -  \"window\" ex: (rolling averages with each row representing 1 day):\n",
    "        -  computes aggregation of specific window of data\n",
    "        -  returns value for every input row of a table based on a group of rows (frame)\n",
    "        -  typically contains a \"partitions\" (how data is broken up in group)\n",
    "        -  functions:\n",
    "             - ranking (rank vs dense_rank)\n",
    "             - analytic\n",
    "             - aggregate   \n",
    "         <br>\n",
    "    -  \"grouping set\":\n",
    "        -  covers aggregation across multiple groups ex: (total quantity by stock codes and customers)   \n",
    "    <br>\n",
    "    -  \"rollup\":\n",
    "        -  multidimensional aggregation that performs a variety of group-by calculations ex: (aggregation across groups by time)\n",
    "        -  \"null\" values indicate grand totals across columns (null in all columns specifies total aggregation across those cols)   \n",
    "    <br>\n",
    "    -  \"cube\":\n",
    "        -  rollup at a deeper level   \n",
    "##### must filter out NULL values for aggregation levels on cubes, rollups, and grouping sets or else code will compute incorrect results   \n",
    "<br>\n",
    "    -  \"pivot\":\n",
    "        -  convert a row into a column   \n",
    "        <br>\n",
    "    -  \"UDAFs\":\n",
    "        -  user-defined aggregation functions are designed to define custom aggregation functions over groups of input data (as opposed to single rows)\n",
    "        -  currently only available in Scala and Java\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #7 Exercises (DataFrames)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repartition data to have less partition because of small data volume stored in many small files\n",
    "# cache DF for in memory access\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(retailDataAll)\\\n",
    ".coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Count Example_:\n",
    "- count\n",
    "- countDistinct\n",
    "- approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n",
      "None\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n",
      "None\n",
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n",
      "None\n",
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.select(count(\"StockCode\")).show()) # Spark will not count NULLs in individual col\n",
    "print(df.select(count(\"*\")).show()) # Spark will count NULLs\n",
    "\n",
    "print(df.select(countDistinct(\"StockCode\")).show()) # count unique values in col\n",
    "\n",
    "print(df.select(approx_count_distinct(\"StockCode\", 0.1)).show()) # approx count w/ degree of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Boundry Example_:\n",
    "- first and last\n",
    "- min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "from pyspark.sql.functions import min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n",
      "None\n",
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.select(first(\"StockCode\"), last(\"StockCode\")).show()) # return first and last col value in row in DF\n",
    "\n",
    "print(df.select(min(\"Quantity\"), max(\"Quantity\")).show()) # return min and max value in DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Sum Example_:\n",
    "-  sum\n",
    "-  sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n",
      "None\n",
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.select(sum(\"Quantity\")).show()) # sum values in col\n",
    "\n",
    "print(df.select(sumDistinct(\"Quantity\")).show()) # sum distinct set of values in col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Average Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(total_transactions_COUNT=541909, total_purchases_SUM=5176450, (total_purchases_SUM / total_transactions_COUNT)=9.55224954743324, avg_purchases_AVG=9.55224954743324, mean_purchases_MEAN=9.55224954743324)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions_COUNT\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases_SUM\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases_AVG\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases_MEAN\"))\\\n",
    "  .selectExpr(\n",
    "    \"total_transactions_COUNT\",\n",
    "    \"total_purchases_SUM\",\n",
    "    \"total_purchases_SUM/total_transactions_COUNT\",\n",
    "    \"avg_purchases_AVG\",\n",
    "    \"mean_purchases_MEAN\")\\\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Stats Example_:\n",
    "##### Var and Stddev measure the spread of the data around the mean\n",
    "##### Skewness and Kurtosis measure extreme points in data and are used in probability distribution of a random varaible\n",
    "##### Corr and Covar compares the interactions of the values in 2 different variables\n",
    "\n",
    "-  variance (sample and population) __average of squared differences from the mean__\n",
    "-  standard deviation (sample and population) __square root of the variance__\n",
    "-  skewness __measure of asymmetry of values in data around the mean__\n",
    "-  kurtosis __measure of the tail of data__\n",
    "-  covariance (sample and population) __measures variability between 2 different varaibles__\n",
    "-  correlation __measures the Pearson correlation coefficient between -1 and +1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609056|47559.391409298754|  218.08095663447796|   218.08115785023418|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052562|119768.05495536952|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085635685E-4|             1052.7280543902734|            1052.7260778741693|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"), covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Complex Types Example_:\n",
    "-  collect_list [collects list of present values in col]\n",
    "-  collect_set [collects list of unique present values in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Group By Example_:\n",
    "-  groupBy\n",
    "-  agg\n",
    "-  grouping with maps [key is the column; value is the aggregation function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536366|     17850|    2|\n",
      "|   536367|     13047|   12|\n",
      "|   536369|     13047|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536370|  20|             20|\n",
      "|   536380|   1|              1|\n",
      "|   536384|  13|             13|\n",
      "+---------+----+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536370|             22.45|   8.935742834258381|\n",
      "|   536380|              24.0|                 0.0|\n",
      "|   536384|14.615384615384615|  15.750645708563392|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Window Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window\\\n",
    ".partitionBy(\"CustomerId\", \"date\")\\\n",
    ".orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "purchaseDenseRank = dense_rank().over(windowSpec) # dense rank avoids ranking ties\n",
    "purchaseRank = rank().over(windowSpec) # rank does tied values (duplicate rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----+-----+-----------+\n",
      "|CustomerId|      date|Quantity|rank|dRank|maxPurchase|\n",
      "+----------+----------+--------+----+-----+-----------+\n",
      "|     12346|2011-01-18|   74215|   1|    1|      74215|\n",
      "|     12346|2011-01-18|  -74215|   2|    2|      74215|\n",
      "|     12347|2011-12-07|      24|   1|    1|         24|\n",
      "|     12347|2011-12-07|      24|   1|    1|         24|\n",
      "|     12347|2011-12-07|      24|   1|    1|         24|\n",
      "|     12347|2011-12-07|      12|   8|    4|         24|\n",
      "|     12347|2011-12-07|      20|   5|    2|         24|\n",
      "|     12347|2011-12-07|      20|   5|    2|         24|\n",
      "|     12347|2011-12-07|      16|   7|    3|         24|\n",
      "|     12347|2011-12-07|      12|   8|    4|         24|\n",
      "+----------+----------+--------+----+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, date, Quantity,\n",
    "  rank(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                       ORDER BY Quantity DESC NULLS LAST\n",
    "                       ROWS BETWEEN\n",
    "                         UNBOUNDED PRECEDING AND\n",
    "                         CURRENT ROW) as rank,\n",
    "\n",
    "  dense_rank(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                             ORDER BY Quantity DESC NULLS LAST\n",
    "                             ROWS BETWEEN\n",
    "                               UNBOUNDED PRECEDING AND\n",
    "                               CURRENT ROW) as dRank,\n",
    "\n",
    "  max(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                      ORDER BY Quantity DESC NULLS LAST\n",
    "                      ROWS BETWEEN\n",
    "                        UNBOUNDED PRECEDING AND\n",
    "                        CURRENT ROW) as maxPurchase\n",
    "FROM dfWithDate WHERE CustomerId IS NOT NULL ORDER BY CustomerId\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Grouping Set (via Spark SQL) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "+----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "+----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "\"\"\").show(3)\n",
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode))\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Rollup Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------+\n",
      "|      Date|    Country|total_quantity|\n",
      "+----------+-----------+--------------+\n",
      "|      null|       null|       5176450|\n",
      "|2010-12-01|  Australia|           107|\n",
      "|2010-12-01|Netherlands|            97|\n",
      "|2010-12-01|     France|           449|\n",
      "|2010-12-01|    Germany|           117|\n",
      "+----------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      null|   null|       5176450|\n",
      "|2010-12-01|   null|         26814|\n",
      "|2010-12-02|   null|         21023|\n",
      "+----------+-------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+-------+--------------+\n",
      "|Date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|null|   null|       5176450|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    ".selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n",
    ".orderBy(\"Date\")\n",
    "rolledUpDF.show(5)\n",
    "rolledUpDF.where(\"Country IS NULL\").show(3)\n",
    "rolledUpDF.where(\"Date IS NULL\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Cube Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-------------+\n",
      "|Date|           Country|sum(Quantity)|\n",
      "+----+------------------+-------------+\n",
      "|null|           Lebanon|          386|\n",
      "|null|           Denmark|         8188|\n",
      "|null|             Italy|         7999|\n",
      "|null|            Norway|        19247|\n",
      "|null|       Switzerland|        30325|\n",
      "|null|    Czech Republic|          592|\n",
      "|null|European Community|          497|\n",
      "|null|         Lithuania|          652|\n",
      "|null|            Poland|         3653|\n",
      "|null|           Iceland|         2458|\n",
      "+----+------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\"))).select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Pivot Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- Australia_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Australia_sum(UnitPrice): double (nullable = true)\n",
      " |-- Australia_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Austria_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Austria_sum(UnitPrice): double (nullable = true)\n",
      " |-- Austria_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Bahrain_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Bahrain_sum(UnitPrice): double (nullable = true)\n",
      " |-- Bahrain_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Belgium_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Belgium_sum(UnitPrice): double (nullable = true)\n",
      " |-- Belgium_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Brazil_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Brazil_sum(UnitPrice): double (nullable = true)\n",
      " |-- Brazil_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Canada_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Canada_sum(UnitPrice): double (nullable = true)\n",
      " |-- Canada_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Channel Islands_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Channel Islands_sum(UnitPrice): double (nullable = true)\n",
      " |-- Channel Islands_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Cyprus_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Cyprus_sum(UnitPrice): double (nullable = true)\n",
      " |-- Cyprus_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Czech Republic_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Czech Republic_sum(UnitPrice): double (nullable = true)\n",
      " |-- Czech Republic_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Denmark_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Denmark_sum(UnitPrice): double (nullable = true)\n",
      " |-- Denmark_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- EIRE_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- EIRE_sum(UnitPrice): double (nullable = true)\n",
      " |-- EIRE_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- European Community_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- European Community_sum(UnitPrice): double (nullable = true)\n",
      " |-- European Community_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Finland_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Finland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Finland_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- France_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- France_sum(UnitPrice): double (nullable = true)\n",
      " |-- France_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Germany_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Germany_sum(UnitPrice): double (nullable = true)\n",
      " |-- Germany_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Greece_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Greece_sum(UnitPrice): double (nullable = true)\n",
      " |-- Greece_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Hong Kong_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Hong Kong_sum(UnitPrice): double (nullable = true)\n",
      " |-- Hong Kong_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Iceland_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Iceland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Iceland_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Israel_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Israel_sum(UnitPrice): double (nullable = true)\n",
      " |-- Israel_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Italy_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Italy_sum(UnitPrice): double (nullable = true)\n",
      " |-- Italy_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Japan_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Japan_sum(UnitPrice): double (nullable = true)\n",
      " |-- Japan_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Lebanon_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Lebanon_sum(UnitPrice): double (nullable = true)\n",
      " |-- Lebanon_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Lithuania_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Lithuania_sum(UnitPrice): double (nullable = true)\n",
      " |-- Lithuania_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Malta_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Malta_sum(UnitPrice): double (nullable = true)\n",
      " |-- Malta_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Netherlands_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Netherlands_sum(UnitPrice): double (nullable = true)\n",
      " |-- Netherlands_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Norway_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Norway_sum(UnitPrice): double (nullable = true)\n",
      " |-- Norway_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Poland_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Poland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Poland_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Portugal_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Portugal_sum(UnitPrice): double (nullable = true)\n",
      " |-- Portugal_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- RSA_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- RSA_sum(UnitPrice): double (nullable = true)\n",
      " |-- RSA_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Saudi Arabia_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Saudi Arabia_sum(UnitPrice): double (nullable = true)\n",
      " |-- Saudi Arabia_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Singapore_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Singapore_sum(UnitPrice): double (nullable = true)\n",
      " |-- Singapore_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Spain_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Spain_sum(UnitPrice): double (nullable = true)\n",
      " |-- Spain_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Sweden_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Sweden_sum(UnitPrice): double (nullable = true)\n",
      " |-- Sweden_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Switzerland_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Switzerland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Switzerland_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- USA_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- USA_sum(UnitPrice): double (nullable = true)\n",
      " |-- USA_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- United Arab Emirates_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- United Arab Emirates_sum(UnitPrice): double (nullable = true)\n",
      " |-- United Arab Emirates_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- United Kingdom_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- United Kingdom_sum(UnitPrice): double (nullable = true)\n",
      " |-- United Kingdom_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      " |-- Unspecified_sum(CAST(Quantity AS BIGINT)): long (nullable = true)\n",
      " |-- Unspecified_sum(UnitPrice): double (nullable = true)\n",
      " |-- Unspecified_sum(CAST(CustomerID AS BIGINT)): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+\n",
      "|      date|USA_sum(CAST(Quantity AS BIGINT))|\n",
      "+----------+---------------------------------+\n",
      "|2011-12-07|                             null|\n",
      "|2011-12-06|                             null|\n",
      "|2011-12-08|                             -196|\n",
      "+----------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\", \"USA_sum(CAST(Quantity AS BIGINT))\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #8 - Joins_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  brings together 2 sets of data (left dataset and right dataset) by comparing the value of 1 or more keys of the left dataset and right dataset\n",
    "-  equi-join (compares whether the specified keys in your left and right datasets are equal and discards rows that do not have matching keys)   \n",
    "<br>\n",
    "- **Types**:\n",
    "    -  inner joins (keep rows with keys that exist in the left and right datasets)\n",
    "    -  full outer joins (keep rows with keys in either the left or right datasets)\n",
    "    -  left outer joins (keep rows with keys in the left dataset)\n",
    "    -  right outer joins (keep rows with keys in the right dataset)\n",
    "    -  left semi joins (keep rows in the left (only left) dataset where the key appears in the right dataset)\n",
    "    -  left anti joins (keep rows in the left (only left) dataset where they DO NOT appear in the right dataset)\n",
    "    -  natural joins (performs join that matches columns between the 2 datasets with the same names)\n",
    "    -  cross _cartesian_ joins (match every row in the left dataset with every row in the right dataset)   \n",
    "    <br>\n",
    "-  **How Spark Performs Joins**:\n",
    "    -  node-to-node communication strategy\n",
    "    -  per node computation strategy   \n",
    "    <br>\n",
    "-  **Communication Strategies**:\n",
    "    -  **shuffle join** [occurs on \"big table\" to \"big table\" joins where during shuffle every node talks to every other node and they share data according to which node has a certain key or set of keys being joined]\n",
    "    -  **broadcast join** [can be useful for \"small table\" to \"big table\" joins because table is small enough to fit into memory of single worker node]\n",
    "        -  plan is to replicate small DF onto every worker node in the cluster to avoid \"all to all (node to node) communication during the entire join process so worker can perform their own work thus not needing to communicate with other slaves\n",
    "        -  broadcasting tables too large can crash the driver\n",
    "        -  Spark tries to optimize broadcast automatically and can be confimed via EXPLAIN PLAN however can also specifiy broadcast join\n",
    "###### it is recommended to let Spark optimize computation on \"small table\" to \"small table\" joins\n",
    "#### Additional Optimization Technique: ***consider partitioning your data \"correctly\" prior to a join as the execution performance can significantly improve thus even if a shuffle is planned Spark can avoid a shuffle is data from the DFs are already located on the same machine***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #8 Exercises (DataFrames)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")])\\\n",
    "  .toDF(\"id\", \"status\")\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person table:\n",
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n",
      "graduateProgram table:\n",
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n",
      "sparkStatus table:\n",
      "+---+--------------+\n",
      "| id|        status|\n",
      "+---+--------------+\n",
      "|500|Vice President|\n",
      "|250|    PMC Member|\n",
      "|100|   Contributor|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"person table:\")\n",
    "person.show()\n",
    "print(\"graduateProgram table:\")\n",
    "graduateProgram.show()\n",
    "print(\"sparkStatus table:\")\n",
    "sparkStatus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _Inner Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              name  graduate_program     spark_status  id   degree  \\\n",
       "0   0     Bill Chambers                 0            [100]   0  Masters   \n",
       "1   1     Matei Zaharia                 1  [500, 250, 100]   1    Ph.D.   \n",
       "2   2  Michael Armbrust                 1       [250, 100]   1    Ph.D.   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  join only rows on keys in both datasets that evaluate to true\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram[\"id\"]\n",
    "person.join(graduateProgram, joinExpression).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, name, graduate_program, spark_status, id, degree, department, school]\n",
       "Index: []"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of key that does not exist in either dataset hence returns no rows\n",
    "wrongJoinExpression = person[\"name\"] == graduateProgram[\"school\"]\n",
    "person.join(graduateProgram, wrongJoinExpression).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              name  graduate_program     spark_status  id   degree  \\\n",
       "0   0     Bill Chambers                 0            [100]   0  Masters   \n",
       "1   1     Matei Zaharia                 1  [500, 250, 100]   1    Ph.D.   \n",
       "2   2  Michael Armbrust                 1       [250, 100]   1    Ph.D.   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# via specifying join type\n",
    "joinType = \"inner\"\n",
    "person.join(graduateProgram, joinExpression, joinType).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Outer Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id              name  graduate_program     spark_status  id   degree  \\\n",
       "0  0.0     Bill Chambers               0.0            [100]   0  Masters   \n",
       "1  NaN              None               NaN             None   2  Masters   \n",
       "2  1.0     Matei Zaharia               1.0  [500, 250, 100]   1    Ph.D.   \n",
       "3  2.0  Michael Armbrust               1.0       [250, 100]   1    Ph.D.   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  \n",
       "3                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  join rows on keys in both datasets and includes rows that evaluate to true or false (listed as nulls)\n",
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Left Outer Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[250, 100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   degree             department       school   id              name  \\\n",
       "0   0  Masters  School of Information  UC Berkeley  0.0     Bill Chambers   \n",
       "1   2  Masters                   EECS  UC Berkeley  NaN              None   \n",
       "2   1    Ph.D.                   EECS  UC Berkeley  1.0     Matei Zaharia   \n",
       "3   1    Ph.D.                   EECS  UC Berkeley  2.0  Michael Armbrust   \n",
       "\n",
       "   graduate_program     spark_status  \n",
       "0               0.0            [100]  \n",
       "1               NaN             None  \n",
       "2               1.0  [500, 250, 100]  \n",
       "3               1.0       [250, 100]  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join rows on keys in both datasets and includes ALL rows from left dataset\n",
    "# as well as any rows in the right dataset that have a match in left dataset\n",
    "# no equivalent row in the right dataset will be listed as null\n",
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Right Outer Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id              name  graduate_program     spark_status  id   degree  \\\n",
       "0  0.0     Bill Chambers               0.0            [100]   0  Masters   \n",
       "1  NaN              None               NaN             None   2  Masters   \n",
       "2  1.0     Matei Zaharia               1.0  [500, 250, 100]   1    Ph.D.   \n",
       "3  2.0  Michael Armbrust               1.0       [250, 100]   1    Ph.D.   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  \n",
       "3                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join rows on keys in both datasets and includes ALL rows from right dataset\n",
    "# as well as any rows in the left dataset that have a match in right dataset\n",
    "# no equivalent row in the left dataset will be listed as null\n",
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Left Semi Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# similar to filter on DF via key specified\n",
    "# compare values (key) to see if the key value exists in the right dataset\n",
    "# if values exists those rows will be kept in the result even if there is a duplicate key in left dataset\n",
    "joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  2|Masters|                EECS|      UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n",
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how duplicates are treated example\n",
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "    (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "gradProgram2.show()\n",
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _Left Anti Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# similar to NOT IN filter on DF via key specified\n",
    "# compare values (key) to see if the key value exists in the right dataset\n",
    "# opposite of semi joins ... only keep values that DO NOT have a corresponding key in the right dataset\n",
    "joinType = \"left_anti\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _Natural Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   degree             department       school              name  \\\n",
       "0   0  Masters  School of Information  UC Berkeley     Bill Chambers   \n",
       "1   2  Masters                   EECS  UC Berkeley  Michael Armbrust   \n",
       "2   1    Ph.D.                   EECS  UC Berkeley     Matei Zaharia   \n",
       "\n",
       "   graduate_program     spark_status  \n",
       "0                 0            [100]  \n",
       "1                 1       [250, 100]  \n",
       "2                 1  [500, 250, 100]  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finds matching columns and returns the results ... always use this join with CAUTION\n",
    "spark.sql(\"select * from graduateProgram NATURAL JOIN person\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Cross Cartesian Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>2</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   degree             department       school  id              name  \\\n",
       "0   0  Masters  School of Information  UC Berkeley   0     Bill Chambers   \n",
       "1   1    Ph.D.                   EECS  UC Berkeley   1     Matei Zaharia   \n",
       "2   1    Ph.D.                   EECS  UC Berkeley   2  Michael Armbrust   \n",
       "\n",
       "   graduate_program     spark_status  \n",
       "0                 0            [100]  \n",
       "1                 1  [500, 250, 100]  \n",
       "2                 1       [250, 100]  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join every row in the left dataset to every row in the right dataset\n",
    "# 1,000 left rows * 1,000 right rows = 100,000 rows returned so CAUTION this join\n",
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>2</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           name  graduate_program     spark_status  id   degree  \\\n",
       "0   0  Bill Chambers                 0            [100]   0  Masters   \n",
       "1   0  Bill Chambers                 0            [100]   2  Masters   \n",
       "2   0  Bill Chambers                 0            [100]   1    Ph.D.   \n",
       "3   1  Matei Zaharia                 1  [500, 250, 100]   0  Masters   \n",
       "4   1  Matei Zaharia                 1  [500, 250, 100]   2  Masters   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  \n",
       "3  School of Information  UC Berkeley  \n",
       "4                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.crossJoin(graduateProgram).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Joins On Complex Types Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------------+---------------+---+--------------+\n",
      "|personId|         name|graduate_program|   spark_status| id|        status|\n",
      "+--------+-------------+----------------+---------------+---+--------------+\n",
      "|       0|Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "+--------+-------------+----------------+---------------+---+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.withColumnRenamed(\"id\", \"personId\")\\\n",
    ".join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Handling Duplicate Column Names Example_:\n",
    "1.  different join expression\n",
    "2.  dropping column after join\n",
    "3.  renaming column before join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n",
    "joinExpr = gradProgramDupe[\"graduate_program\"] == person[\"graduate_program\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              name  graduate_program     spark_status  graduate_program  \\\n",
       "0   0     Bill Chambers                 0            [100]                 0   \n",
       "1   1     Matei Zaharia                 1  [500, 250, 100]                 1   \n",
       "2   2  Michael Armbrust                 1       [250, 100]                 1   \n",
       "\n",
       "    degree             department       school  \n",
       "0  Masters  School of Information  UC Berkeley  \n",
       "1    Ph.D.                   EECS  UC Berkeley  \n",
       "2    Ph.D.                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nperson.join(gradProgramDupe, joinExpr).select(\"graduate_program\").show() # triggers error\\n'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# challenge arrises when selecting duplicate column\n",
    "'''\n",
    "person.join(gradProgramDupe, joinExpr).select(\"graduate_program\").show() # triggers error\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "|graduate_program| id|            name|   spark_status| degree|          department|     school|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "|               0|  0|   Bill Chambers|          [100]|Masters|School of Informa...|UC Berkeley|\n",
      "|               1|  1|   Matei Zaharia|[500, 250, 100]|  Ph.D.|                EECS|UC Berkeley|\n",
      "|               1|  2|Michael Armbrust|     [250, 100]|  Ph.D.|                EECS|UC Berkeley|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "person.join(gradProgramDupe, \"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "| id|            name|   spark_status|graduate_program| degree|          department|     school|\n",
      "+---+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|          [100]|               0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "person.join(gradProgramDupe, joinExpr).drop(person[\"graduate_program\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>graduate_program</th>\n",
       "      <th>spark_status</th>\n",
       "      <th>grad_id</th>\n",
       "      <th>degree</th>\n",
       "      <th>department</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bill Chambers</td>\n",
       "      <td>0</td>\n",
       "      <td>[100]</td>\n",
       "      <td>0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>School of Information</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Matei Zaharia</td>\n",
       "      <td>1</td>\n",
       "      <td>[500, 250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Michael Armbrust</td>\n",
       "      <td>1</td>\n",
       "      <td>[250, 100]</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph.D.</td>\n",
       "      <td>EECS</td>\n",
       "      <td>UC Berkeley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              name  graduate_program     spark_status  grad_id   degree  \\\n",
       "0   0     Bill Chambers                 0            [100]        0  Masters   \n",
       "1   1     Matei Zaharia                 1  [500, 250, 100]        1    Ph.D.   \n",
       "2   2  Michael Armbrust                 1       [250, 100]        1    Ph.D.   \n",
       "\n",
       "              department       school  \n",
       "0  School of Information  UC Berkeley  \n",
       "1                   EECS  UC Berkeley  \n",
       "2                   EECS  UC Berkeley  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\n",
    "joinExpr = person[\"graduate_program\"] == gradProgram3[\"grad_id\"]\n",
    "person.join(gradProgram3, joinExpr).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Broadcast Join Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [graduate_program#7770L], [id#7784L], Inner, BuildRight\n",
      ":- *(2) Project [_1#7760L AS id#7768L, _2#7761 AS name#7769, _3#7762L AS graduate_program#7770L, _4#7763 AS spark_status#7771]\n",
      ":  +- *(2) Filter isnotnull(_3#7762L)\n",
      ":     +- Scan ExistingRDD[_1#7760L,_2#7761,_3#7762L,_4#7763]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))\n",
      "   +- *(1) Project [_1#7776L AS id#7784L, _2#7777 AS degree#7785, _3#7778 AS department#7786, _4#7779 AS school#7787]\n",
      "      +- *(1) Filter isnotnull(_1#7776L)\n",
      "         +- Scan ExistingRDD[_1#7776L,_2#7777,_3#7778,_4#7779]\n"
     ]
    }
   ],
   "source": [
    "joinExpr = person[\"graduate_program\"] == graduateProgram[\"id\"]\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
