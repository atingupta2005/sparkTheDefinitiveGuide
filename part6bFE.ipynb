{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: The Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: Advanced Analytics and Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simpleML = '/Users/grp/sparkTheDefinitiveGuide/data/simple-ml/'\n",
    "retailDataByDay = '/Users/grp/sparkTheDefinitiveGuide/data/retail-data/by-day/*.csv'\n",
    "simpleMLInt = '/Users/grp/sparkTheDefinitiveGuide/data/simple-ml-integers/'\n",
    "simpleMLScale = '/Users/grp/sparkTheDefinitiveGuide/data/simple-ml-scaling/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #25 - Preprocessing and Feature Engineering_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  Data Feature Format Per Use Case:\n",
    "    -  Classification / Regression [column for label of type _Double_; column of type _Vector_ (dense or sparse) for features]\n",
    "    -  Recommendation [column of users; column of items; column of ratings]\n",
    "    -  Unsupervised [column of type _Vector_ (dense or sparse) for features]\n",
    "    -  Graph [DF of vertices; DF of edges]   \n",
    "    <br>\n",
    "-  Transformers:\n",
    "    -  best way to convert raw data (preprocessing) to needed format data for ML\n",
    "    -  are functions that accept a DF as an argument and return a new DF as a response\n",
    "    -  available in Spark package: **pyspark.ml.feature** / **org.apache.spark.ml.feature**   \n",
    "    <br>\n",
    "-  Estimators:\n",
    "    -  when a transformation needs information (data) about the input column hence has to pass over (fit) the entire input column\n",
    "    -  require to \"fit\" the transformer to dataset then call \"transform\" on resulting object producted from \"fit\"\n",
    "    -  function must see all inputs from dataset to select a mapping of inputs (ex: StringIndexer)   \n",
    "    <br>\n",
    "-  High Level Transfomers (ex: RFormula):\n",
    "    -  handles categorical inputs via one-hot encoding (converts set of values into set of binary columns)\n",
    "    -  handles numeric columns as casted as _Double_\n",
    "    -  handles string label (target variable) columns as transformed to _Double_ with StringIndexer   \n",
    "    <br>\n",
    "-  Continuous Features:\n",
    "    -  _Bucketizer_\n",
    "    -  _QuantileDiscretizer_\n",
    "    -  _StandardScaler_\n",
    "    -  _MinMaxScaler_\n",
    "    -  _MaxAbsScaler_\n",
    "    -  _ElementwiseProduct_\n",
    "    -  _Normalizer_     \n",
    "    <br>\n",
    "-  Categorical Features:\n",
    "    -  _StringIndexer_\n",
    "    -  _OneHotEncoder_ (replaced with OneHotEncoderEstimator in Spark 2.3)\n",
    "    -  _VectorIndexer_\n",
    "    -  _IndexToString_\n",
    "    -  _Tokenizer_\n",
    "    -  _RegexTokenizer_\n",
    "    -  _StopWordsRemover_\n",
    "    -  _NGram_     \n",
    "    <br>\n",
    "-  Converting Words into Numerical Representation:\n",
    "    -  _CountVectorizer_\n",
    "    -  _HashingTF_\n",
    "    -  _IDF_\n",
    "    -  _Word2Vec_      \n",
    "    <br>\n",
    "-  Feature Manipulation:\n",
    "    -  _PCA_\n",
    "    -  _Interaction_\n",
    "    -  _Polynomial Expansion_   \n",
    "    <br>\n",
    "-  Feature Selection:\n",
    "    -  _ChiSqSelector_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #25 Exercises (FE)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(retailDataByDay)\\\n",
    ".coalesce(5)\\\n",
    ".where(\"Description IS NOT NULL\")\n",
    "\n",
    "fakeIntDF = spark.read.parquet(simpleMLInt)\n",
    "simpleDF = spark.read.json(simpleML)\n",
    "scaleDF = spark.read.parquet(simpleMLScale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.cache() # cache into memory for efficient re-use\n",
    "sales.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|int1|int2|int3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   7|   8|   9|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n",
      "None\n",
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "+---+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fakeIntDF.show(3))\n",
    "print(simpleDF.show(3))\n",
    "print(scaleDF.show(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RFormula [Estimator] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------------------------------------------------------+-----+\n",
      "|color|lab |value1|value2            |features                                                            |label|\n",
      "+-----+----+------+------------------+--------------------------------------------------------------------+-----+\n",
      "|green|good|1     |14.386294994851129|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])|1.0  |\n",
      "|blue |bad |8     |14.386294994851129|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])      |0.0  |\n",
      "|blue |bad |12    |14.386294994851129|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])    |0.0  |\n",
      "+-----+----+------+------------------+--------------------------------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "Row(color='green', lab='good', value1=1, value2=14.386294994851129, features=SparseVector(10, {1: 1.0, 2: 1.0, 3: 14.3863, 5: 1.0, 8: 14.3863}), label=1.0)\n",
      "Row(color='blue', lab='bad', value1=8, value2=14.386294994851129, features=SparseVector(10, {2: 8.0, 3: 14.3863, 6: 8.0, 9: 14.3863}), label=0.0)\n",
      "Row(color='blue', lab='bad', value1=12, value2=14.386294994851129, features=SparseVector(10, {2: 12.0, 3: 14.3863, 6: 12.0, 9: 14.3863}), label=0.0)\n"
     ]
    }
   ],
   "source": [
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "supervised.fit(simpleDF).transform(simpleDF).show(3, False)\n",
    "\n",
    "# type Row format\n",
    "for i in supervised.fit(simpleDF).transform(simpleDF).take(3): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _SQLTransformer [Transformer] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|          119|      62|   14452.0|\n",
      "|          440|     143|   16916.0|\n",
      "|          630|      72|   17633.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"__THIS__\" represents the underlying table of the input dataset\n",
    "basicTransformation = SQLTransformer()\\\n",
    "  .setStatement(\"\"\"\n",
    "    SELECT sum(Quantity), count(*), CustomerID\n",
    "    FROM __THIS__\n",
    "    GROUP BY CustomerID\n",
    "  \"\"\")\n",
    "basicTransformation.transform(sales).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _VectorAssembler [Transformer] Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_4ab98a8d8cde298cb88d__output|\n",
      "+----+----+----+--------------------------------------------+\n",
      "|   1|   2|   3|                               [1.0,2.0,3.0]|\n",
      "|   7|   8|   9|                               [7.0,8.0,9.0]|\n",
      "|   4|   5|   6|                               [4.0,5.0,6.0]|\n",
      "+----+----+----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concatenates all features into long vector to pass into an estimator\n",
    "# takes as input a number of columns of type Boolean, Double, or Vector\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Continuous Feature Examples_:\n",
    "-  Bucketing:\n",
    "    -  convert continuous data to categorical data via \"buckets or bins\"\n",
    "        -  Transfomers:\n",
    "            -  _Bucketizer_\n",
    "            -  _QuantileDiscretizer_   \n",
    "            <br>\n",
    "-  Scaling and Normalization:\n",
    "    -  normalization helps with transforming data so each point's value is a representation of its distance from the mean of that column\n",
    "    -  scaling helps with keeping data on the same scale so that values can easily be compared to one another for sensitive variations\n",
    "        -  Estimators:\n",
    "            -  _StandardScaler_\n",
    "            -  _MinMaxScaler_\n",
    "            -  _MaxAbsScaler_\n",
    "        -  Transformers:\n",
    "            -  _ElementwiseProduct_\n",
    "            -  _Normalizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|0.0|\n",
      "|1.0|\n",
      "|2.0|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "contDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------+\n",
      "| id|Bucketizer_4cb6bcff679ecbf0bcfe__output|\n",
      "+---+---------------------------------------+\n",
      "|0.0|                                    0.0|\n",
      "|1.0|                                    0.0|\n",
      "|2.0|                                    0.0|\n",
      "|3.0|                                    0.0|\n",
      "|4.0|                                    0.0|\n",
      "|5.0|                                    1.0|\n",
      "|6.0|                                    1.0|\n",
      "|7.0|                                    1.0|\n",
      "|8.0|                                    1.0|\n",
      "|9.0|                                    1.0|\n",
      "+---+---------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"bins\" continuous features into category \"buckets\"\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------+\n",
      "| id|QuantileDiscretizer_4d65a9096c72d69e239e__output|\n",
      "+---+------------------------------------------------+\n",
      "|0.0|                                             0.0|\n",
      "|1.0|                                             0.0|\n",
      "|2.0|                                             0.0|\n",
      "|3.0|                                             1.0|\n",
      "|4.0|                                             1.0|\n",
      "|5.0|                                             1.0|\n",
      "|6.0|                                             1.0|\n",
      "|7.0|                                             2.0|\n",
      "|8.0|                                             2.0|\n",
      "|9.0|                                             2.0|\n",
      "+---+------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splits data based on percentiles\n",
    "bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol(\"id\")\n",
    "fittedBucketer = bucketer.fit(contDF)\n",
    "fittedBucketer.transform(contDF).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------------------------+\n",
      "|id |features      |StandardScaler_41d69ae08066183d13f3__output                 |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n",
      "|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n",
      "|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scales input column according to range of values in that column to have zero mean and a variance of 1 in each dimension\n",
    "# transforms a dataset of Vector rows then normalizes each feature to have unit standard deviation and/or zero mean\n",
    "ss = StandardScaler().setInputCol(\"features\")\n",
    "ss.fit(scaleDF).transform(scaleDF).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------+\n",
      "| id|      features|MinMaxScaler_4cb7b624b977c83e558d__output|\n",
      "+---+--------------+-----------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                            [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                            [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                            [5.0,5.0,5.0]|\n",
      "+---+--------------+-----------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scales the values into a vector based on min/max boundry\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "fittedminMax.transform(scaleDF).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------------------------------------+\n",
      "|id |features      |MaxAbsScaler_4c60b9b27cc21512637d__output                    |\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009901,-0.3333333333333333]|\n",
      "|1  |[2.0,1.1,1.0] |[0.6666666666666666,0.10891089108910892,0.3333333333333333]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009901,-0.3333333333333333]|\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scales the values into a vector by dividing each value by the max absolute value in feature\n",
    "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
    "fittedmaScaler = maScaler.fit(scaleDF)\n",
    "fittedmaScaler.transform(scaleDF).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------------+\n",
      "|id |features      |ElementwiseProduct_43fea42384f05fefbbe5__output|\n",
      "+---+--------------+-----------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[10.0,1.5,-20.0]                               |\n",
      "|1  |[2.0,1.1,1.0] |[20.0,16.5,20.0]                               |\n",
      "|0  |[1.0,0.1,-1.0]|[10.0,1.5,-20.0]                               |\n",
      "+---+--------------+-----------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scales the values into a vector by an arbitrary value\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct().setScalingVec(scaleUpVec).setInputCol(\"features\")\n",
    "scalingUp.transform(scaleDF).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------------------------------+\n",
      "|id |features      |Normalizer_4f40bbb6be281d439ed9__output                        |\n",
      "+---+--------------+---------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
      "|1  |[2.0,1.1,1.0] |[0.48780487804878053,0.26829268292682934,0.24390243902439027]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
      "+---+--------------+---------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scales the values into a vector via a unit norm (parameter 'p')\n",
    "# helps standardize input data and improve the behavior of learning algorithms\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattanDistance.transform(scaleDF).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Categorical Feature Examples_:\n",
    "-  Indexing:\n",
    "    -  converts categorical varaible to numerical variables\n",
    "        -  Estimators:\n",
    "            -  _StringIndexer_\n",
    "            -  _OneHotEncoder_ (replaced with OneHotEncoderEstimator in Spark 2.3)\n",
    "            -  _VectorIndexer_\n",
    "        -  Transformers:\n",
    "            -  _IndexToString_   \n",
    "            <br>\n",
    "-  Text Data Transformers:\n",
    "    -  parses text for analysis\n",
    "        -  Transformers:\n",
    "            -  _Tokenizer_\n",
    "            -  _RegexTokenizer_\n",
    "            -  _StopWordsRemover_\n",
    "            -  _NGram_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nvalIndexer.setHandleInvalid(\"skip\")\\nvalIndexer.fit(simpleDF).setHandleInvalid(\"skip\")\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maps strings to different numerical ID\n",
    "# creates metadata attached to DF specifying what inputs correspond to what outputs \n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show(3)\n",
    "\n",
    "# numeric input column string indexer \n",
    "valIndexer = StringIndexer().setInputCol(\"value1\").setOutputCol(\"valueInd\")\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show(5)\n",
    "\n",
    "# example of skipping row if the input value was not a value seen during training\n",
    "'''\n",
    "valIndexer.setHandleInvalid(\"skip\")\n",
    "valIndexer.fit(simpleDF).setHandleInvalid(\"skip\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+------------------------------------------+\n",
      "|color| lab|value1|            value2|labelInd|IndexToString_4ae2b6d27efc340a63f2__output|\n",
      "+-----+----+------+------------------+--------+------------------------------------------+\n",
      "|green|good|     1|14.386294994851129|     1.0|                                      good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                                       bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                                       bad|\n",
      "+-----+----+------+------------------+--------+------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# map numeric IDs back to original values via indexed metadata\n",
    "labelReverse = IndexToString().setInputCol(\"labelInd\")\n",
    "labelReverse.transform(idxRes).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+\n",
      "|     features|label|        idxed|\n",
      "+-------------+-----+-------------+\n",
      "|[1.0,2.0,3.0]|    1|[0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|    2|[1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|    3|[0.0,8.0,9.0]|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# helps index categorical features in datasets of vectors\n",
    "# automatically finds categorical features inside of input vectors and converts them to categorical features\n",
    "idxIn = spark.createDataFrame([\\\n",
    "(Vectors.dense(1, 2, 3),1),\n",
    "(Vectors.dense(2, 5, 6),2),\n",
    "(Vectors.dense(1, 8, 9),3)])\\\n",
    ".toDF(\"features\", \"label\")\n",
    "\n",
    "indxr = VectorIndexer()\\\n",
    ".setInputCol(\"features\")\\\n",
    ".setOutputCol(\"idxed\")\\\n",
    ".setMaxCategories(2)\n",
    "\n",
    "indxr.fit(idxIn).transform(idxIn).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------------------------------+\n",
      "|color|colorInd|OneHotEncoder_47d185be83f3013989bc__output|\n",
      "+-----+--------+------------------------------------------+\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "+-----+--------+------------------------------------------+\n",
      "only showing top 7 rows\n",
      "\n",
      "Row(color='green', colorInd=1.0, OneHotEncoder_47d185be83f3013989bc__output=SparseVector(2, {1: 1.0}))\n",
      "Row(color='blue', colorInd=2.0, OneHotEncoder_47d185be83f3013989bc__output=SparseVector(2, {}))\n",
      "Row(color='blue', colorInd=2.0, OneHotEncoder_47d185be83f3013989bc__output=SparseVector(2, {}))\n",
      "Row(color='green', colorInd=1.0, OneHotEncoder_47d185be83f3013989bc__output=SparseVector(2, {1: 1.0}))\n",
      "Row(color='green', colorInd=1.0, OneHotEncoder_47d185be83f3013989bc__output=SparseVector(2, {1: 1.0}))\n",
      "Row(color='green', colorInd=1.0, OneHotEncoder_47d185be83f3013989bc__output=SparseVector(2, {1: 1.0}))\n",
      "Row(color='red', colorInd=0.0, OneHotEncoder_47d185be83f3013989bc__output=SparseVector(2, {0: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "# converts each distinct category value to a boolean flag (1 or 0)\n",
    "# vital method because just indexing does not always represent categorical variables correctly when being processed by algorithms\n",
    "# ex: algorithm will mathematically treat blue (2.0) > green (1.0) which is incorrect based on feature\n",
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
    "ohe.transform(colorLab).show(7)\n",
    "\n",
    "# type Row format\n",
    "for i in ohe.transform(colorLab).take(7): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+\n",
      "|Description                    |DescOut                              |\n",
      "+-------------------------------+-------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|\n",
      "+-------------------------------+-------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splits text into array of words based on separated (default is a whitespace) parser\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+\n",
      "|Description                    |DescOut                              |\n",
      "+-------------------------------+-------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|\n",
      "+-------------------------------+-------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------------------------+------------+\n",
      "|Description                    |DescOut     |\n",
      "+-------------------------------+------------+\n",
      "|RABBIT NIGHT LIGHT             |[ ,  ]      |\n",
      "|DOUGHNUT LIP GLOSS             |[ ,  ,  ]   |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[ ,  ,  ,  ]|\n",
      "+-------------------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splits text into array of words based on custom separated parser\n",
    "rt = RegexTokenizer()\\\n",
    ".setInputCol(\"Description\")\\\n",
    ".setOutputCol(\"DescOut\")\\\n",
    ".setPattern(\" \")\\\n",
    ".setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(3, False)\n",
    "\n",
    "# set \"gaps\" to False to return output values matching the pattern\n",
    "rt = RegexTokenizer()\\\n",
    ".setInputCol(\"Description\")\\\n",
    ".setOutputCol(\"DescOut\")\\\n",
    ".setPattern(\" \")\\\n",
    ".setGaps(False)\\\n",
    ".setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "|Description                    |DescOut                              |StopWordsRemover_4beab8172e52ec7c19e8__output|\n",
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |[rabbit, night, light]                       |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |[doughnut, lip, gloss]                       |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|[12, message, cards, envelopes]              |\n",
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filters out stop words that are not relevant to analysis to reduce noise within ML dataset\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\") # loads list of default stop words\n",
    "stops = StopWordsRemover()\\\n",
    ".setStopWords(englishStopWords)\\\n",
    ".setInputCol(\"DescOut\")\n",
    "stops.transform(tokenized).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------------------------------+\n",
      "|DescOut                              |NGram_4788880d83b933cb73a9__output   |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit, night, light]               |\n",
      "|[doughnut, lip, gloss]               |[doughnut, lip, gloss]               |\n",
      "|[12, message, cards, with, envelopes]|[12, message, cards, with, envelopes]|\n",
      "+-------------------------------------+-------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|DescOut                              |NGram_4529a99e840e07f126f7__output                     |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit night, night light]                            |\n",
      "|[doughnut, lip, gloss]               |[doughnut lip, lip gloss]                              |\n",
      "|[12, message, cards, with, envelopes]|[12 message, message cards, cards with, with envelopes]|\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------------------------------+------------------------------------------------------------+\n",
      "|DescOut                              |NGram_453aa469e65af2f713bc__output                          |\n",
      "+-------------------------------------+------------------------------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit night light]                                        |\n",
      "|[doughnut, lip, gloss]               |[doughnut lip gloss]                                        |\n",
      "|[12, message, cards, with, envelopes]|[12 message cards, message cards with, cards with envelopes]|\n",
      "+-------------------------------------+------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word combinations to capture sentence structure and word patterns\n",
    "unigram = NGram().setInputCol(\"DescOut\").setN(1) # n-gram of length 1\n",
    "bigram = NGram().setInputCol(\"DescOut\").setN(2) # n-gram of length 2\n",
    "trigram = NGram().setInputCol(\"DescOut\").setN(3) # n-gram of length 3\n",
    "unigram.transform(tokenized.select(\"DescOut\")).show(3, False)\n",
    "bigram.transform(tokenized.select(\"DescOut\")).show(3, False)\n",
    "trigram.transform(tokenized.select(\"DescOut\")).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Converting Words into Numerical Representations Examples_:\n",
    "-  takes transformed text and converts into numerical vectors\n",
    "-  measures whether or not each row contains a given word\n",
    "-  every row is a \"document\" and every word is a \"term\" and the total collection of all terms is the \"vocabulary\"\n",
    "    -  Estimators:\n",
    "        -  _CountVectorizer_\n",
    "        -  _HashingTF_\n",
    "        -  _IDF_\n",
    "        -  _Word2Vec_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer:\n",
    "-  outputs the counts of a term in a document\n",
    "    -  FIT Process:\n",
    "        -  finds set of words in all documents (row) and then counts occurrences of words in the documents\n",
    "    -  TRANSFORM Process:\n",
    "        -  counts occurences of word in each row of DF column and outputs vector with terms that occur in that row\n",
    "    -  Parameters:\n",
    "        -  minTF:\n",
    "            -  \"minimum term frequency\" for the term to be included in the vocabulary\n",
    "            -  helps with removing rare words from the vocabulary\n",
    "        -  minDF:\n",
    "            -  \"minimum - of documents\" a term must appear before being included in the vocabulary\"\n",
    "            -  helps with removing rare words from the vocabulary\n",
    "        - vocabSize:\n",
    "            -  \"total maximum vocabulary size\"\n",
    "-  How To Read The \"countVec\" Sparse Vector:\n",
    "    -  (\"vocabulary size\", [\"index of word in the vocabulary\"], [\"count of that particular word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+---------------------------------+\n",
      "|Description                    |DescOut                              |countVec                         |\n",
      "+-------------------------------+-------------------------------------+---------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |(500,[150,185,212],[1.0,1.0,1.0])|\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |(500,[462,463,491],[1.0,1.0,1.0])|\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|(500,[35,41,166],[1.0,1.0,1.0])  |\n",
      "+-------------------------------+-------------------------------------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Row(Description='RABBIT NIGHT LIGHT', DescOut=['rabbit', 'night', 'light'], countVec=SparseVector(500, {150: 1.0, 185: 1.0, 212: 1.0}))\n",
      "Row(Description='DOUGHNUT LIP GLOSS ', DescOut=['doughnut', 'lip', 'gloss'], countVec=SparseVector(500, {462: 1.0, 463: 1.0, 491: 1.0}))\n",
      "Row(Description='12 MESSAGE CARDS WITH ENVELOPES', DescOut=['12', 'message', 'cards', 'with', 'envelopes'], countVec=SparseVector(500, {35: 1.0, 41: 1.0, 166: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\\\n",
    ".setInputCol(\"DescOut\")\\\n",
    ".setOutputCol(\"countVec\")\\\n",
    ".setVocabSize(500)\\\n",
    ".setMinTF(1)\\\n",
    ".setMinDF(2)\n",
    "fittedCV = cv.fit(tokenized)\n",
    "fittedCV.transform(tokenized).show(3, False)\n",
    "\n",
    "# type Row format\n",
    "for i in fittedCV.transform(tokenized).take(3): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF [Term Frequency-Inverse Document Frequency]:\n",
    "-  measures how often a word occurs in each document; weighted according to how many documents that word appears\n",
    "-  words that occur in a few documents have higher weights than words that occur in many documents\n",
    "-  ex: term like \"the\" appearing in every document within corpus will be weighted extremely low\n",
    "-  helps find words that share similar topics\n",
    "    -  How To Read The \"IDFOut\" Sparse Vector:\n",
    "        - (\"total vocabulary size\", [\"hash of every word appearing in document\"], [\"weight of each term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|DescOut                                |\n",
      "+---------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[red, floral, feltcraft, shoulder, bag]|\n",
      "|[alarm, clock, bakelike, red]          |\n",
      "|[pin, cushion, babushka, red]          |\n",
      "|[red, retrospot, mini, cases]          |\n",
      "|[red, kitchen, scales]                 |\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "+---------------------------------------+\n",
      "\n",
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|DescOut                                |TFOut                                                   |IDFOut                                                                                                              |\n",
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[3372,4291,4370,6594,9160],[1.0,1.0,1.0,1.0,1.0])|(10000,[3372,4291,4370,6594,9160],[0.9808292530117262,0.0,0.9808292530117262,0.9808292530117262,0.9808292530117262])|\n",
      "|[red, floral, feltcraft, shoulder, bag]|(10000,[155,1152,4291,5981,6756],[1.0,1.0,1.0,1.0,1.0]) |(10000,[155,1152,4291,5981,6756],[0.0,0.0,0.0,0.0,0.0])                                                             |\n",
      "|[alarm, clock, bakelike, red]          |(10000,[4291,4852,4995,9668],[1.0,1.0,1.0,1.0])         |(10000,[4291,4852,4995,9668],[0.0,0.0,0.0,0.0])                                                                     |\n",
      "|[pin, cushion, babushka, red]          |(10000,[4291,5111,5673,7153],[1.0,1.0,1.0,1.0])         |(10000,[4291,5111,5673,7153],[0.0,0.0,0.0,0.0])                                                                     |\n",
      "|[red, retrospot, mini, cases]          |(10000,[547,1576,2591,4291],[1.0,1.0,1.0,1.0])          |(10000,[547,1576,2591,4291],[0.0,0.0,0.0,0.0])                                                                      |\n",
      "|[red, kitchen, scales]                 |(10000,[3461,4291,6214],[1.0,1.0,1.0])                  |(10000,[3461,4291,6214],[0.0,0.0,0.0])                                                                              |\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[3372,4291,4370,6594,9160],[1.0,1.0,1.0,1.0,1.0])|(10000,[3372,4291,4370,6594,9160],[0.9808292530117262,0.0,0.9808292530117262,0.9808292530117262,0.9808292530117262])|\n",
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Row(DescOut=['gingham', 'heart', '', 'doorstop', 'red'], TFOut=SparseVector(10000, {3372: 1.0, 4291: 1.0, 4370: 1.0, 6594: 1.0, 9160: 1.0}), IDFOut=SparseVector(10000, {3372: 0.9808, 4291: 0.0, 4370: 0.9808, 6594: 0.9808, 9160: 0.9808}))\n",
      "Row(DescOut=['red', 'floral', 'feltcraft', 'shoulder', 'bag'], TFOut=SparseVector(10000, {155: 1.0, 1152: 1.0, 4291: 1.0, 5981: 1.0, 6756: 1.0}), IDFOut=SparseVector(10000, {155: 0.0, 1152: 0.0, 4291: 0.0, 5981: 0.0, 6756: 0.0}))\n",
      "Row(DescOut=['alarm', 'clock', 'bakelike', 'red'], TFOut=SparseVector(10000, {4291: 1.0, 4852: 1.0, 4995: 1.0, 9668: 1.0}), IDFOut=SparseVector(10000, {4291: 0.0, 4852: 0.0, 4995: 0.0, 9668: 0.0}))\n",
      "Row(DescOut=['pin', 'cushion', 'babushka', 'red'], TFOut=SparseVector(10000, {4291: 1.0, 5111: 1.0, 5673: 1.0, 7153: 1.0}), IDFOut=SparseVector(10000, {4291: 0.0, 5111: 0.0, 5673: 0.0, 7153: 0.0}))\n",
      "Row(DescOut=['red', 'retrospot', 'mini', 'cases'], TFOut=SparseVector(10000, {547: 1.0, 1576: 1.0, 2591: 1.0, 4291: 1.0}), IDFOut=SparseVector(10000, {547: 0.0, 1576: 0.0, 2591: 0.0, 4291: 0.0}))\n",
      "Row(DescOut=['red', 'kitchen', 'scales'], TFOut=SparseVector(10000, {3461: 1.0, 4291: 1.0, 6214: 1.0}), IDFOut=SparseVector(10000, {3461: 0.0, 4291: 0.0, 6214: 0.0}))\n",
      "Row(DescOut=['gingham', 'heart', '', 'doorstop', 'red'], TFOut=SparseVector(10000, {3372: 1.0, 4291: 1.0, 4370: 1.0, 6594: 1.0, 9160: 1.0}), IDFOut=SparseVector(10000, {3372: 0.9808, 4291: 0.0, 4370: 0.9808, 6594: 0.9808, 9160: 0.9808}))\n"
     ]
    }
   ],
   "source": [
    "tfIdfIn = tokenized\\\n",
    ".where(\"array_contains(DescOut, 'red')\")\\\n",
    ".select(\"DescOut\")\\\n",
    ".limit(7)\n",
    "tfIdfIn.show(7, False)\n",
    "\n",
    "# hash each word and convert to numerical representation\n",
    "# (similar to CountVectorizer; maps input words to index [words can be mapped to same index])\n",
    "tf = HashingTF()\\\n",
    ".setInputCol(\"DescOut\")\\\n",
    ".setOutputCol(\"TFOut\")\\\n",
    ".setNumFeatures(10000)\n",
    "\n",
    "# weight each word in vocabulary via IDF\n",
    "idf = IDF()\\\n",
    ".setInputCol(\"TFOut\")\\\n",
    ".setOutputCol(\"IDFOut\")\\\n",
    ".setMinDocFreq(2)\n",
    "\n",
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(7, False)\n",
    "\n",
    "# type Row format\n",
    "for i in idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).take(7): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec:\n",
    "-  computes a vector representation of a set of words\n",
    "-  aims to have similar words in the vector space to make generalizations\n",
    "-  captures relationships between words based on their semantics\n",
    "    -  Process:\n",
    "        -  uses technique called \"skip-grams\" to convert sentence of words into vector representation (vectorSize) ...\n",
    "        -  builds vocabulary\n",
    "        -  maps each word to a unique fixed-size vector\n",
    "        -  transforms each document into a vector using the average of all words in the document\n",
    "        -  for each sentence (row) it removes a token / trains model to predict missing token in the \"n-gram\" representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.08242294350638986,-0.005830296874046326,-0.05622698366641998]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.03722663476530994,-0.0021124311855861117,-0.013124272493379456]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.08365558385848999,0.00822269544005394,-0.040985722094774246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input data: each row is a bag of words from a sentence or document\n",
    "documentDF = spark.createDataFrame([\\\n",
    "(\"Hi I heard about Spark\".split(\" \"), ),\n",
    "(\"I wish Java could use case classes\".split(\" \"), ),\n",
    "(\"Logistic regression models are neat\".split(\" \"), )], [\"text\"])\n",
    "\n",
    "# learn a mapping from words to vectors\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Feature Manipulation Examples_:\n",
    "-  automated mathematical means used to:\n",
    "    -  expand the input feature vectors\n",
    "    -  reduce input feature vectors to a lower number of dimensions\n",
    "        -  Estimators:\n",
    "            -  _PCA_\n",
    "        -  Transformers:\n",
    "            -  _Interaction_\n",
    "            -  _PolynomialExpansion_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA [Principal Component Analysis]:\n",
    "-  helps find the most import aspects of the data (principal components)\n",
    "-  helps to create a smaller set of more relevant features as input to model\n",
    "    -  Process:\n",
    "        -  changes feature representation of data by creating a new set of features called \"aspects\":\n",
    "        -  only include combination of original features as input to model\n",
    "    -  Use Case:\n",
    "        -  helpful for large input datasets with too many features\n",
    "    -  Parameter:\n",
    "        -  k:\n",
    "            -  specifies \"number of output features to create\" (should be much smaller than input vectors' dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_44dda20e4cb44486b5e6__output          |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060149758]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca = PCA().setInputCol(\"features\").setK(2)\n",
    "pca.fit(scaleDF).transform(scaleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Interaction:\n",
    "-  creates an interaction between 2 variables having an important correlation\n",
    "-  only available via Scala API however can be developed via RFormula\n",
    "    -  Process:\n",
    "        -  multiplies the 2 features together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Expansion:\n",
    "-  used to see interactions between particular features when unsure which interactions to consider\n",
    "    -  Process:\n",
    "        -  takes each value in feature vector and multiplies it by every other value in feature vector\n",
    "        -  Ex:\n",
    "            -  3 input features with degree-2 polynomial will product 9 output features (3X3)\n",
    "            -  3 input features with degree-3 polynomial will product 27 output features (3X3X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------------------------+\n",
      "|id |features      |PolynomialExpansion_469b93cc7a5a801887ce__output         |\n",
      "+---+--------------+---------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0]|\n",
      "|1  |[2.0,1.1,1.0] |[2.0,4.0,1.1,2.2,1.2100000000000002,1.0,2.0,1.1,1.0]     |\n",
      "|0  |[1.0,0.1,-1.0]|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0]|\n",
      "+---+--------------+---------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pe = PolynomialExpansion().setInputCol(\"features\").setDegree(2)\n",
    "pe.transform(scaleDF).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Feature Selection Examples_:\n",
    "-  method to select smaller subset of features for training from large range of available features\n",
    "-  many features might be correlated, but using to many features might lead to overfitting\n",
    "    -  Estimators:\n",
    "        -  _ChiSqSelector_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiSqSelector:\n",
    "-  helps identify features that are relevant to the dependent variable\n",
    "-  hence drops the uncorrelated features\n",
    "-  best suited for categorical data to reduce number of features as input to model\n",
    "    -  Parameters:\n",
    "        -  numTopFeatures:\n",
    "            -  selects fixed number of features ordered by p-value\n",
    "        -  percentile:\n",
    "            -  takes proportion of the input features\n",
    "        -  fpr:\n",
    "            -  sets p-value cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------------------------------------+\n",
      "|countVec                         |ChiSqSelector_41db9d7f0a1256d2158e__output|\n",
      "+---------------------------------+------------------------------------------+\n",
      "|(500,[150,185,212],[1.0,1.0,1.0])|(2,[],[])                                 |\n",
      "|(500,[462,463,491],[1.0,1.0,1.0])|(2,[],[])                                 |\n",
      "|(500,[35,41,166],[1.0,1.0,1.0])  |(2,[],[])                                 |\n",
      "+---------------------------------+------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Row(countVec=SparseVector(500, {150: 1.0, 185: 1.0, 212: 1.0}), ChiSqSelector_41db9d7f0a1256d2158e__output=SparseVector(2, {}))\n",
      "Row(countVec=SparseVector(500, {462: 1.0, 463: 1.0, 491: 1.0}), ChiSqSelector_41db9d7f0a1256d2158e__output=SparseVector(2, {}))\n",
      "Row(countVec=SparseVector(500, {35: 1.0, 41: 1.0, 166: 1.0}), ChiSqSelector_41db9d7f0a1256d2158e__output=SparseVector(2, {}))\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "\n",
    "tokenized = tkn\\\n",
    ".transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
    ".where(\"CustomerId IS NOT NULL\")\n",
    "\n",
    "prechi = fittedCV.transform(tokenized)\\\n",
    ".where(\"CustomerId IS NOT NULL\")\n",
    "\n",
    "chisq = ChiSqSelector()\\\n",
    ".setFeaturesCol(\"countVec\")\\\n",
    ".setLabelCol(\"CustomerId\")\\\n",
    ".setNumTopFeatures(2)\n",
    "\n",
    "chisq.fit(prechi).transform(prechi).drop(\"customerId\", \"Description\", \"DescOut\").show(3, False)\n",
    "\n",
    "# type Row format\n",
    "for i in chisq.fit(prechi).transform(prechi).drop(\"customerId\", \"Description\", \"DescOut\").take(3): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Feature Selection Examples_:\n",
    "-  persisting transformers/estimators\n",
    "-  custom transformers/estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_44dda20e4cb44486b5e6__output          |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "+---+--------------+------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fittedPCA = pca.fit(scaleDF)\n",
    "fittedPCA.write().overwrite().save(\"/Users/grp/sparkTheDefinitiveGuide/tmp/fittedPCA\")\n",
    "\n",
    "loadedPCA = PCAModel.load(\"/Users/grp/sparkTheDefinitiveGuide/tmp/fittedPCA\")\n",
    "loadedPCA.transform(scaleDF).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport org.apache.spark.ml.UnaryTransformer\\nimport org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable,Identifiable}\\nimport org.apache.spark.sql.types.{ArrayType, StringType, DataType}\\nimport org.apache.spark.ml.param.{IntParam, ParamValidators}\\n\\nclass MyTokenizer(override val uid: String)\\n  extends UnaryTransformer[String, Seq[String],\\n    MyTokenizer] with DefaultParamsWritable {\\n\\n  def this() = this(Identifiable.randomUID(\"myTokenizer\"))\\n\\n  val maxWords: IntParam = new IntParam(this, \"maxWords\",\\n    \"The max number of words to return.\",\\n  ParamValidators.gtEq(0))\\n\\n  def setMaxWords(value: Int): this.type = set(maxWords, value)\\n\\n  def getMaxWords: Integer = $(maxWords)\\n\\n  override protected def createTransformFunc: String => Seq[String] = (\\n    inputString: String) => {\\n      inputString.split(\"\\\\s\").take($(maxWords))\\n  }\\n\\n  override protected def validateInputType(inputType: DataType): Unit = {\\n    require(\\n      inputType == StringType, s\"Bad input type: $inputType. Requires String.\")\\n  }\\n\\n  override protected def outputDataType: DataType = new ArrayType(StringType,\\n    true)\\n}\\n\\n// this will allow you to read it back in by using this object.\\nobject MyTokenizer extends DefaultParamsReadable[MyTokenizer]\\n\\nval myT = new MyTokenizer().setInputCol(\"someCol\").setMaxWords(2)\\nmyT.transform(Seq(\"hello world. This text won\\'t show.\").toDF(\"someCol\")).show()\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import org.apache.spark.ml.UnaryTransformer\n",
    "import org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable,Identifiable}\n",
    "import org.apache.spark.sql.types.{ArrayType, StringType, DataType}\n",
    "import org.apache.spark.ml.param.{IntParam, ParamValidators}\n",
    "\n",
    "class MyTokenizer(override val uid: String)\n",
    "  extends UnaryTransformer[String, Seq[String],\n",
    "    MyTokenizer] with DefaultParamsWritable {\n",
    "\n",
    "  def this() = this(Identifiable.randomUID(\"myTokenizer\"))\n",
    "\n",
    "  val maxWords: IntParam = new IntParam(this, \"maxWords\",\n",
    "    \"The max number of words to return.\",\n",
    "  ParamValidators.gtEq(0))\n",
    "\n",
    "  def setMaxWords(value: Int): this.type = set(maxWords, value)\n",
    "\n",
    "  def getMaxWords: Integer = $(maxWords)\n",
    "\n",
    "  override protected def createTransformFunc: String => Seq[String] = (\n",
    "    inputString: String) => {\n",
    "      inputString.split(\"\\\\s\").take($(maxWords))\n",
    "  }\n",
    "\n",
    "  override protected def validateInputType(inputType: DataType): Unit = {\n",
    "    require(\n",
    "      inputType == StringType, s\"Bad input type: $inputType. Requires String.\")\n",
    "  }\n",
    "\n",
    "  override protected def outputDataType: DataType = new ArrayType(StringType,\n",
    "    true)\n",
    "}\n",
    "\n",
    "// this will allow you to read it back in by using this object.\n",
    "object MyTokenizer extends DefaultParamsReadable[MyTokenizer]\n",
    "\n",
    "val myT = new MyTokenizer().setInputCol(\"someCol\").setMaxWords(2)\n",
    "myT.transform(Seq(\"hello world. This text won't show.\").toDF(\"someCol\")).show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
