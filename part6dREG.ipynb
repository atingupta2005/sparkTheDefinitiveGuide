{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: The Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: Advanced Analytics and Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = '/Users/grp/sparkTheDefinitiveGuide/data/regression/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #27 - Regression_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  predicts a real number (continuous variable) fom a set of features (numerical form)\n",
    "-  infinite # of possible output values\n",
    "-  use metric of error for evaluation compared to accuracy metric (classification)\n",
    "\n",
    "### Use Cases:\n",
    "-  entertainment views [predict # of views for a particular service]\n",
    "-  company revenue [predict how much revenue a company will make in the future]\n",
    "-  resource yield [predict total resource yield for a particular region]\n",
    "\n",
    "### MLlib Regression Models:\n",
    "-  Linear Regression\n",
    "-  Generalized Linear Regression\n",
    "-  Isotonic Regression\n",
    "-  Decision Trees\n",
    "-  Random Forest\n",
    "-  Gradient-Boosted Trees\n",
    "-  Survival Regression\n",
    "-  Isotonic Regression\n",
    "\n",
    "### Evaluators:\n",
    "-  regression evaluator (RegressionEvaluator) expects a \"predicted value\" and a \"true value\"\n",
    "-  metrics (RegressionMetrics) supported:\n",
    "    -  RMSE (root mean squared error)\n",
    "    -  MSE (mean squared error)\n",
    "    -  R2 (r squared)\n",
    "    -  MAE (mean absolute error)\n",
    "    -  EXPLAINED VARIANCE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration:\n",
    "-  Model Hyperparameters (structure of how model can be initialized)\n",
    "-  Training Parameters (structure of how model can be trained)\n",
    "-  Prediction Parameters (structured of how model determines making predictions)\n",
    "-  Model Summary (provides information about final trained model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Summary:\n",
    "- these metrics help represent how well the model is actually fitting the line\n",
    "    -  Residuals:\n",
    "        -  weights of each of the input features plugged into model\n",
    "        -  \"**coefficients of determination; measure of fit**\"\n",
    "    -  Objective History:\n",
    "        -  shows training at each iteration\n",
    "    -  Root Mean Squared Error:\n",
    "        -  measures how well line is fitting the data (by looking at the distance between each predicted value and actual value in dataset)\n",
    "    -  R-Squared:\n",
    "        -  measures the proportion of variance of the predicted variable captured by the model\n",
    "        -  \"**difference between label and the predicted value**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #27 Exercises (Reg)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n",
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[3.0,10.1,3.0]|  2.0|\n",
      "| [2.0,1.1,1.0]|  1.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(reg)\n",
    "df.printSchema()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression:\n",
    "-  assume a linear combination of input features (sum of each feature multiplied by a weight) plus Gaussian error in output\n",
    "-  implements ElasticNet regularization with mix of L1 and L2 regularization\n",
    "    -  Model Hyperparameters:\n",
    "        -  family:\n",
    "            -  uses Gaussian distribution\n",
    "        -  elasticNetParam:\n",
    "            -  float value from 0 to 1\n",
    "            -  mix of L1 and L2 regularization\n",
    "                -  L1 (value of 1):\n",
    "                    -  creates sparsity in model because certain feature weights will be zero (no significance to output)\n",
    "                -  L2 (value of 0):\n",
    "                    -  does not create sparsity because feature weights will never completely be zero\n",
    "        -  fitIntercept:\n",
    "            -  either TRUE or FALSE\n",
    "            -  determines whether to fit intercept to linear combination of inputs and weights of model\n",
    "            -  recommended to fit intercept if the training data has not been normalized\n",
    "        -  regParam:\n",
    "            -  determines how much weight to give to regularization\n",
    "            -  recommended to use wide range of values (0, 0.01, 0.1, 1)\n",
    "        -  standardization:\n",
    "            -  either TRUE or FALSE\n",
    "            -  decides whether to standardize the inputs before passing into model\n",
    "    -  Training Parameters:\n",
    "        -  maxIter:\n",
    "            -  total number of iterations over the data before stopping (default value is usually best)\n",
    "        -  tol:\n",
    "            -  threshold that stops before maxIter when weights are considered optimized (default value is usually best)\n",
    "        -  weightCol:\n",
    "            -  weighs certain rows more than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "print(lr.explainParams())\n",
    "lrModel = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "| 0.12805046585610147|\n",
      "| -0.1446826926157201|\n",
      "|-0.41903832622420606|\n",
      "|-0.41903832622420606|\n",
      "|  0.8547088792080306|\n",
      "+--------------------+\n",
      "\n",
      "6\n",
      "[0.5000000000000001, 0.43152958103627864, 0.313233593388102, 0.312256926665541, 0.30915060819830303, 0.30915058933480266]\n",
      "0.47308424392175985\n",
      "0.720239122691221\n"
     ]
    }
   ],
   "source": [
    "summary = lrModel.summary\n",
    "summary.residuals.show()\n",
    "print(summary.totalIterations)\n",
    "print(summary.objectiveHistory)\n",
    "print(summary.rootMeanSquaredError)\n",
    "print(summary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Regression:\n",
    "-  does not scale well to large numbers of features\n",
    "-  provides more customization to different noise distribution families\n",
    "    -  Model Hyperparameters:\n",
    "        -  family:\n",
    "            -  error distribution to be used in the model\n",
    "            -  Gaussian, Binomial, Poisson, Gamma, Tweedie\n",
    "        -  link:\n",
    "            -  function that provides relationship between the linear predictor and the mean of the distribution function\n",
    "            -  cloglog, probit, logit, inverse, sqrt, identity, log\n",
    "        -  solver:\n",
    "            -  used for optimization\n",
    "        -  variancePower:\n",
    "            -  Tweedie distribution aka relationship between the variance and mean of the distribution\n",
    "        -  linkPower:\n",
    "            -  index in the power link function for the Tweedie family\n",
    "    -  Training Parameters:\n",
    "        -  maxIter:\n",
    "            -  total number of iterations over the data before stopping (default value is usually best)\n",
    "        -  tol:\n",
    "            -  threshold that stops before maxIter when weights are considered optimized (default value is usually best)\n",
    "        -  weightCol:\n",
    "            -  weighs certain rows more than others\n",
    "    -  Prediction Parameters:\n",
    "        -  linkPredictionCol:\n",
    "            -  output of link function for each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family: The name of family which is a description of the error distribution to be used in the model. Supported options: gaussian (default), binomial, poisson, gamma and tweedie. (default: gaussian, current: gaussian)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "link: The name of link function which provides the relationship between the linear predictor and the mean of the distribution function. Supported options: identity, log, inverse, logit, probit, cloglog and sqrt. (current: identity)\n",
      "linkPower: The index in the power link function. Only applicable to the Tweedie family. (undefined)\n",
      "linkPredictionCol: link prediction (linear predictor) column name (current: linkOut)\n",
      "maxIter: max number of iterations (>= 0). (default: 25, current: 10)\n",
      "offsetCol: The offset column name. If this is not set or empty, we treat all instance offsets as 0.0 (undefined)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "solver: The solver algorithm for optimization. Supported options: irls. (default: irls)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "variancePower: The power in the variance function of the Tweedie distribution which characterizes the relationship between the variance and mean of the distribution. Only applicable for the Tweedie family. Supported values: 0 and [1, Inf). (default: 0.0)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "glr = GeneralizedLinearRegression()\\\n",
    ".setFamily(\"gaussian\")\\\n",
    ".setLink(\"identity\")\\\n",
    ".setMaxIter(10)\\\n",
    ".setRegParam(0.3)\\\n",
    ".setLinkPredictionCol(\"linkOut\")\n",
    "print(glr.explainParams())\n",
    "glrModel = glr.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees:\n",
    "-  produces a single number per leaf node instead of label (classification decision tree)\n",
    "-  create a tree to predict numerical outputs instead of training coefficients (classification decision tree) to model a function\n",
    "    -  Model Hyperparameters:\n",
    "        -  impurity:\n",
    "            -  represents metric whether or not the model should split a particular leaf node with a particular value or keep the same\n",
    "        -  maxDepth:\n",
    "            -  helps avoid overfitting dataset\n",
    "        -  maxBins:\n",
    "            -  determines the # of bins created from continuous features (converted from categorical features)\n",
    "            -  more bins give higher level of granularity\n",
    "        -  impurity:\n",
    "            -  used to build the tree\n",
    "            -  metric to determine where model should split at specific leaf node\n",
    "            -  only supports \"variance\"\n",
    "        -  minInfoGain:\n",
    "            -  provides information used for splitting\n",
    "            -  higher value can help prevent overfitting\n",
    "            -  this parameter can require a lot of testing variations\n",
    "        -  minInstancePerNode:\n",
    "            -  determines minimum # of training instances that need to occur in each leaf node\n",
    "            -  tree will be \"pruned\" (removes sections of the tree that provide little power to classify instances) until requirements are met\n",
    "    -  Training Parameters:\n",
    "        -  checkpointInterval:\n",
    "            -  set to save model's work over training every N iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name. (default: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: -1407754390808368278)\n",
      "varianceCol: column name for the biased sample variance of prediction. (undefined)\n"
     ]
    }
   ],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "print(dtr.explainParams())\n",
    "dtrModel = dtr.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests & Gradient-Boosted Trees:\n",
    "-  many trees are trained to perform a regression\n",
    "-  RF [many de-correlated trees are trained and averaged]\n",
    "-  GBT [each tree makes a weighted prediction]\n",
    "    -  Model Hyperparameters:\n",
    "        -  impurity:\n",
    "            -  represents metric whether or not the model should split a particular leaf node with a particular value or keep the same\n",
    "        -  maxDepth:\n",
    "            -  helps avoid overfitting dataset\n",
    "        -  maxBins:\n",
    "            -  determines the # of bins created from continuous features (converted from categorical features)\n",
    "            -  more bins give higher level of granularity\n",
    "        -  impurity:\n",
    "            -  used to build the tree\n",
    "            -  metric to determine where model should split at specific leaf node\n",
    "            -  only supports \"variance\"\n",
    "        -  minInfoGain:\n",
    "            -  provides information used for splitting\n",
    "            -  higher value can help prevent overfitting\n",
    "            -  this parameter can require a lot of testing variations\n",
    "        -  minInstancePerNode:\n",
    "            -  determines minimum # of training instances that need to occur in each leaf node\n",
    "            -  tree will be \"pruned\" (removes sections of the tree that provide little power to classify instances) until requirements are met\n",
    "    -  Training Parameters:\n",
    "        -  checkpointInterval:\n",
    "            -  set to save model's work over training every N iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name. (default: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "numTrees: Number of trees to train (>= 1). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 2502083311556356884)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "\n",
      "\n",
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name. (default: label)\n",
      "lossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute (default: squared)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: -6682481135904123338)\n",
      "stepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n"
     ]
    }
   ],
   "source": [
    "rf =  RandomForestRegressor()\n",
    "print(rf.explainParams())\n",
    "rfModel = rf.fit(df)\n",
    "print(\"\\n\")\n",
    "gbt = GBTRegressor()\n",
    "print(gbt.explainParams())\n",
    "gbtModel = gbt.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival Regression (Accelerated Failure Time):\n",
    "-  used to understand the survival rate of individuals\n",
    "-  models the log of the survival time via accelerated failure time model\n",
    "-  tune coefficients according to feature values\n",
    "\n",
    "## Isotonic Regression:\n",
    "-  linear function that is always monotonically increasing (cannot decrease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Evaluation Metrics & CV Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\\nimport org.apache.spark.ml.regression.GeneralizedLinearRegression\\nimport org.apache.spark.ml.Pipeline\\nimport org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\\nval glr = new GeneralizedLinearRegression()\\n  .setFamily(\"gaussian\")\\n  .setLink(\"identity\")\\nval pipeline = new Pipeline().setStages(Array(glr))\\nval params = new ParamGridBuilder().addGrid(glr.regParam, Array(0, 0.5, 1))\\n  .build()\\nval evaluator = new RegressionEvaluator()\\n  .setMetricName(\"rmse\")\\n  .setPredictionCol(\"prediction\")\\n  .setLabelCol(\"label\")\\nval cv = new CrossValidator()\\n  .setEstimator(pipeline)\\n  .setEvaluator(evaluator)\\n  .setEstimatorParamMaps(params)\\n  .setNumFolds(2) // should always be 3 or more but this dataset is small\\nval model = cv.fit(df)\\n\\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\\nval out = model.transform(df)\\n  .select(\"prediction\", \"label\")\\n  .rdd.map(x => (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))\\nval metrics = new RegressionMetrics(out)\\nprintln(s\"MSE = ${metrics.meanSquaredError}\")\\nprintln(s\"RMSE = ${metrics.rootMeanSquaredError}\")\\nprintln(s\"R-squared = ${metrics.r2}\")\\nprintln(s\"MAE = ${metrics.meanAbsoluteError}\")\\nprintln(s\"Explained variance = ${metrics.explainedVariance}\")\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.GeneralizedLinearRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "val glr = new GeneralizedLinearRegression()\n",
    "  .setFamily(\"gaussian\")\n",
    "  .setLink(\"identity\")\n",
    "val pipeline = new Pipeline().setStages(Array(glr))\n",
    "val params = new ParamGridBuilder().addGrid(glr.regParam, Array(0, 0.5, 1))\n",
    "  .build()\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setMetricName(\"rmse\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setLabelCol(\"label\")\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(params)\n",
    "  .setNumFolds(2) // should always be 3 or more but this dataset is small\n",
    "val model = cv.fit(df)\n",
    "\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "val out = model.transform(df)\n",
    "  .select(\"prediction\", \"label\")\n",
    "  .rdd.map(x => (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))\n",
    "val metrics = new RegressionMetrics(out)\n",
    "println(s\"MSE = ${metrics.meanSquaredError}\")\n",
    "println(s\"RMSE = ${metrics.rootMeanSquaredError}\")\n",
    "println(s\"R-squared = ${metrics.r2}\")\n",
    "println(s\"MAE = ${metrics.meanAbsoluteError}\")\n",
    "println(s\"Explained variance = ${metrics.explainedVariance}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
