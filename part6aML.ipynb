{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: The Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: Advanced Analytics and Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simpleML = '/Users/grp/sparkTheDefinitiveGuide/data/simple-ml/'\n",
    "libsvm = '/Users/grp/sparkTheDefinitiveGuide/data/sample_libsvm_data.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #24 - Advanced Analytics and Machine Learning Overview_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  Data Processing:\n",
    "    -  cleaning data\n",
    "    -  ETL\n",
    "    -  Feature Engineering (**converting required inputs of type Double (labels) and Vector[Double] (features)**)   \n",
    "    <br>\n",
    "-  Supervised Learning:\n",
    "    -  Process:\n",
    "        1. gather historical data with labels (dependent variable)\n",
    "        2. train a model to predict values of labels based on various features (independent variables) of the data points\n",
    "        3. test model of data that wasn't trained\n",
    "        4. make predictions on new unlabeled data\n",
    "    -  Classification:\n",
    "        -  train an algorithm to predict a dependent variable that is **CATEGORICAL**\n",
    "        -  Binary Classification [2 categories]\n",
    "        -  Multiclass Classification [more than 2 categories]\n",
    "    -  Regression:\n",
    "        -  train an algorithm to predict a dependent variable that is **CONTINUOUS**\n",
    "        -  predicts a value on a number line\n",
    "    - Use Cases:\n",
    "        -  predicting customer churn\n",
    "        -  predicting disease\n",
    "        -  predicting sales\n",
    "        -  predicting height   \n",
    "        <br>\n",
    "-  Recommendation Learning:\n",
    "    -  Process:\n",
    "        -  suggest products to users based on their behavior\n",
    "        -  train an algorithm to make recommendations on user preferences via similarities between the users or items\n",
    "    -  Use Cases:\n",
    "        -  movie recommendations\n",
    "        -  product recommendations   \n",
    "        <br>\n",
    "-  Unsupervised Learning:\n",
    "    -  Process:\n",
    "        -  identify patterns to discover underlying structure in dataset\n",
    "        -  no dependent variable (label) to predict\n",
    "        -  can be difficult to determine if model is accurate or not\n",
    "    -  Use Cases:\n",
    "        -  anomaly detection\n",
    "        -  topic modeling\n",
    "        -  user segmentation   \n",
    "        <br>\n",
    "-  Graph Analytics:\n",
    "    -  Process:\n",
    "        -  study of structures to identify relationships within data\n",
    "        -  vertices (objects) and edges (relationships between objects)\n",
    "    -  Use Cases:\n",
    "        -  fraud networks\n",
    "        -  social networks\n",
    "        -  pagerank   \n",
    "        <br>\n",
    "-  Deep Learning:\n",
    "    -  neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Process:\n",
    "1.  gather and collect relevant data for task (**DATA COLLECTION**)   \n",
    "<br>\n",
    "2.  clearn and inspect data to better understand the data (**DATA CLEANSING**):\n",
    "    -  EDA:\n",
    "        -  interactive queries\n",
    "        -  visualization methods\n",
    "        -  statistical inference like distributions, correlations, summaries (mean, standard deviation, median, mode, quartiles) \n",
    "    -  Handling NULLs / missing values   \n",
    "    <br>\n",
    "3.  perform feature engineering to allow algorithm to compute the data in a required form [vectors] (**FEATURE ENGINEERING**):\n",
    "    -  converting features to numeric representation (vectors or doubles)\n",
    "    -  normalizing data\n",
    "    -  adding variables\n",
    "    -  converting categorical variables to proper format for ML Model input   \n",
    "    <br>\n",
    "4.  split data as training set to learn from algorithm (**TRAINING MODELS**):\n",
    "    -  _the output of the training process is called a **MODEL**_\n",
    "    -  provide model inputs to produce outputs (predictions) via mathematical manipulation of inputs   \n",
    "    <br>\n",
    "5.  split data as testing set to understand model performance (**MODEL TUNING AND EVALUATION**):\n",
    "    -  tests model to generalize the data it has not seen before\n",
    "    -  Sets:\n",
    "        -  train (dataset used to train model)\n",
    "        -  validation (dataset used to test different variations of hyperparameters) **fit hyperparameters on validation set and NOT test set to prevent overfitting model**  \n",
    "        -  test \"holdout\" (dataset used for final evaluation to find best performing model)\n",
    "    -  _be on the lookup for OVERFITTING (training a model that does not generalize well to new data instead only notices the output from the training set)_   \n",
    "    <br>\n",
    "6.  use steps 4 and 5 to optimize a model to run on unseen data for predictions (**APPLY MODEL FOR INSIGHTS**):\n",
    "    -  export best performing model and send to production to make predictions on new incoming unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLLib:\n",
    "-  Spark provides an interface for building ML pipelines\n",
    "-  provides distributed ETL FE and ML model training\n",
    "-  \"High-Level\" Structured Types:\n",
    "    -  Transformers:\n",
    "        -  functions that convert raw data in some form\n",
    "        -  primarily used for preprocessing and feature engineering data\n",
    "        -  ex: create new column, convert data type, convert categorical variables into numerical values\n",
    "    -  Estimators:\n",
    "        -  typically \"fits\" a transformer\n",
    "        -  ex: learning algorithm that trains on DF and produces a Model\n",
    "    -  Evaluators:\n",
    "        -  shows model performance\n",
    "        -  ex: ROC curve\n",
    "    -  Pipelines:\n",
    "        -  wrapped up sequence of stages containing [transformers, estimators, and evaluators] to make an ML Workflow\n",
    "-  \"Low-Level\" Data Types:\n",
    "    -  Vectors:\n",
    "        -  Sparse:\n",
    "            -  many elements are zero for better compressed representation\n",
    "        -  Dense:\n",
    "            -  many unique values\n",
    "-  Hyperparameters:\n",
    "    -  configuration parameters via learning algorithms set prior to model training\n",
    "    -  used to compare different variations of models to one another to find best performance combination\n",
    "    -  ex: regularization (parameter that pushes models against overfitting data)\n",
    "-  TrainValidationSplit:\n",
    "    -  performs random split on data into 2 different groups\n",
    "-  CrossValidator:\n",
    "    -  performs Kfold cross validation that splits dataset into \"k\" non-overlapping randomly partitioned folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment Options:\n",
    "-  train model offline and then supply it with offline OLAP data (solid method )\n",
    "-  train model offline and then put results into a database [Hive, HBase, Cassandra] (solid method)\n",
    "-  train model offline, persist to disk, and serve to REST API (custom method)\n",
    "-  train model offline and manually convert distributed model to single machine model (complex method)\n",
    "-  train model online and use it online via streaming framework (complex method)\n",
    "-  **productionizing ML can be very difficult and under heavy development / future innovations are currently being worked on**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #24 Exercises (ML)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _Vector (dense & sparse) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denseVec = Vectors.dense(1.0, 2.0, 3.0)\n",
    "size = 3\n",
    "idx = [1, 2] # locations of non-zero elements in vector\n",
    "values = [2.0, 3.0]\n",
    "\n",
    "sparseVec = Vectors.sparse(size, idx, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.0,3.0]\n",
      "(3,[1,2],[2.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "print(denseVec)\n",
    "print(sparseVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Read LIBSVM Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "libsvmDF = spark.read.format(\"libsvm\").load(libsvm)\n",
    "libsvmDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Categorical & Continuous Variable Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|    12|14.386294994851129|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(simpleML)\n",
    "df.orderBy(\"value2\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- lab: string (nullable = true)\n",
      " |-- value1: long (nullable = true)\n",
      " |-- value2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RFormula Example_:\n",
    "-  formula method used to transform data into features for ML model input\n",
    "-  \"~\" separate target and terms\n",
    "-  \"+\" concat terms\n",
    "-  \"-\" remove term\n",
    "-  \":\" interaction (multiplies numeric values; binarizes categorical values)\n",
    "-  \".\" include all columns except target (dependent varaible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supervised = RFormula(formula = \"lab ~ . + color:value1 + color:value2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "|color|lab |value1|value2            |features                                                              |label|\n",
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "|green|good|1     |14.386294994851129|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])  |1.0  |\n",
      "|blue |bad |8     |14.386294994851129|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])        |0.0  |\n",
      "|blue |bad |12    |14.386294994851129|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])      |0.0  |\n",
      "|green|good|15    |38.97187133755819 |(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])  |1.0  |\n",
      "|green|good|12    |14.386294994851129|(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])|1.0  |\n",
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fittedRF = supervised.fit(df) # outputs trained transformer object [RFormulaModel] to transform data via custom Rformula\n",
    "preparedDF = fittedRF.transform(df)\n",
    "preparedDF.show(5, False)\n",
    "\n",
    "# assigns numerical value to each possible color category\n",
    "# creates additional features for the interaction varaibles between colors and value1/value2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Test Set Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Fit Model (Estimator) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit to return a LogisticRegressionModel\n",
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Make Predictions (Transform) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fittedLR.transform(train).select(\"label\", \"prediction\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Pipeline Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666667\n",
      "[0.6918966592050804, 0.5993039195122997, 0.5352368612654572, 0.4597944834802591, 0.4496977652371098, 0.4411504923996298, 0.4368051124453638, 0.43178494030257675, 0.4281782327571738, 0.426192838196295, 0.4257479921914462, 0.4257312079788327, 0.425729301169335, 0.4257286665852039, 0.42572859744959407, 0.42572854981719105, 0.42572853357806606, 0.42572853213299605, 0.4257285320834505, 0.42572853208197436]\n",
      "PipelineModel_4620b985159bfe5c9cdb\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splits\n",
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# estimators\n",
    "rForm = RFormula()\n",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "# pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "\n",
    "# prepare parameter grid to train multiple models with different parameter combinations [2X3X2 = 12 versions being trained]\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder()\\\n",
    "  .addGrid(rForm.formula, [\n",
    "    \"lab ~ . + color:value1\",\n",
    "    \"lab ~ . + color:value1 + color:value2\"])\\\n",
    "  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "  .addGrid(lr.regParam, [0.1, 2.0])\\\n",
    "  .build()\n",
    "\n",
    "# evaluator measuring model performance via areaUnderROC (total area under the receiver)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "  .setMetricName(\"areaUnderROC\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setLabelCol(\"label\")\n",
    "\n",
    "# setup validation set avoid performing hyperparameter fitting on test set to prevent overfitting\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit()\\\n",
    "  .setTrainRatio(0.75)\\\n",
    "  .setEstimatorParamMaps(params)\\\n",
    "  .setEstimator(pipeline)\\\n",
    "  .setEvaluator(evaluator)\n",
    "\n",
    "# will output a model type TrainValidationSplitModel\n",
    "tvsFitted = tvs.fit(train)\n",
    "\n",
    "# evaluate on test set\n",
    "print(evaluator.evaluate(tvsFitted.transform(test)))\n",
    "\n",
    "# how algorithm performed over each training iteration\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "trainedPipeline = tvsFitted.bestModel\n",
    "trainedLR = trainedPipeline.stages[1]\n",
    "summaryLR = trainedLR.summary\n",
    "print(summaryLR.objectiveHistory)\n",
    "\n",
    "# persist to disk to use for predictions on new data\n",
    "model = tvsFitted.bestModel\n",
    "model.write().overwrite().save(\"/Users/grp/sparkTheDefinitiveGuide/tmp/model\")\n",
    "\n",
    "# load model to make predictions on new data\n",
    "# must import specific package based on persisted model type\n",
    "# from pyspark.ml.tuning import TrainValidationSplitModel\n",
    "from pyspark.ml import PipelineModel\n",
    "print(model)\n",
    "applyModel = PipelineModel.load(\"/Users/grp/sparkTheDefinitiveGuide/tmp/model\")\n",
    "testModel = applyModel.transform(test)\n",
    "testModel.select(\"label\", \"prediction\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
