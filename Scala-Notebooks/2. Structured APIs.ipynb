{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [number: bigint]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.range(500).toDF(\"number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.DataFrame = [(number + 10): bigint]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df(\"number\") + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.DataFrame = [number: bigint]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[Long] = Array(0, 1)\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "24: error: not found: value ByteType",
     "output_type": "error",
     "traceback": [
      "<console>:24: error: not found: value ByteType",
      "       var b = ByteType()",
      "               ^",
      ""
     ]
    }
   ],
   "source": [
    "var b = ByteType()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Structured Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val df = spark.read.format(\"json\")\n",
    "  .load(\"../data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "spark.read.format(\"json\").load(\"../data/flight-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
       "import org.apache.spark.sql.types.Metadata\n",
       "myManualSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "import org.apache.spark.sql.types.Metadata\n",
    "\n",
    "val myManualSchema = StructType(Array(\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "  StructField(\"count\", LongType, false,\n",
    "    Metadata.fromJson(\"{\\\"hello\\\":\\\"world\\\"}\"))\n",
    "))\n",
    "\n",
    "val df = spark.read.format(\"json\").schema(myManualSchema)\n",
    "  .load(\"../data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, column}\n",
       "res10: org.apache.spark.sql.Column = someColumnName\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{col, column}\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: org.apache.spark.sql.Column = count\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.col(\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: org.apache.spark.sql.Column = ((((someCol + 5) * 200) - 6) < otherCol)\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\n",
       "res13: org.apache.spark.sql.Column = ((((someCol + 5) * 200) - 6) < otherCol)\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.expr\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count)\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"../data/flight-data/json/2015-summary.json\")\n",
    "  .columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: org.apache.spark.sql.Row = [United States,Romania,15]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "myRow: org.apache.spark.sql.Row = [Hello,null,1,false]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.Row\n",
    "val myRow = Row(\"Hello\", null, 1, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Int = 1\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "myRow(0) // type Any\n",
    "myRow(0).asInstanceOf[String] // String\n",
    "myRow.getString(0) // String\n",
    "myRow.getInt(2) // Int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val df = spark.read.format(\"json\")\n",
    "  .load(\"../data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"dfTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
       "myManualSchema: org.apache.spark.sql.types.StructType = StructType(StructField(some,StringType,true), StructField(col,StringType,true), StructField(names,LongType,false))\n",
       "myRows: Seq[org.apache.spark.sql.Row] = List([Hello,null,1])\n",
       "myRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[28] at parallelize at <console>:40\n",
       "myDf: org.apache.spark.sql.DataFrame = [some: string, col: string ... 1 more field]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "\n",
    "val myManualSchema = new StructType(Array(\n",
    "  new StructField(\"some\", StringType, true),\n",
    "  new StructField(\"col\", StringType, true),\n",
    "  new StructField(\"names\", LongType, false)))\n",
    "val myRows = Seq(Row(\"Hello\", null, 1L))\n",
    "val myRDD = spark.sparkContext.parallelize(myRows)\n",
    "val myDf = spark.createDataFrame(myRDD, myManualSchema)\n",
    "myDf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myDF: org.apache.spark.sql.DataFrame = [col1: string, col2: int ... 1 more field]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val myDF = Seq((\"Hello\", 2, 1L)).toDF(\"col1\", \"col2\", \"col3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{expr, col, column}\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{expr, col, column}\n",
    "df.select(\n",
    "    df.col(\"DEST_COUNTRY_NAME\"),\n",
    "    col(\"DEST_COUNTRY_NAME\"),\n",
    "    column(\"DEST_COUNTRY_NAME\"),\n",
    "    'DEST_COUNTRY_NAME,\n",
    "    $\"DEST_COUNTRY_NAME\",\n",
    "    expr(\"DEST_COUNTRY_NAME\"))\n",
    "  .show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\n",
    "  .show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.selectExpr(\n",
    "    \"*\", // include all original columns\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\n",
    "  .show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.lit\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.lit\n",
    "df.select(expr(\"*\"), lit(1).as(\"One\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\n",
    "  .show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count, Destination)\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\")).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res31: Array[String] = Array(dest, ORIGIN_COUNTRY_NAME, count)\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\n",
       "dfWithLongColName: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "val dfWithLongColName = df.withColumn(\n",
    "  \"This Long Column-Name\",\n",
    "  expr(\"ORIGIN_COUNTRY_NAME\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "dfWithLongColName.selectExpr(\n",
    "    \"`This Long Column-Name`\",\n",
    "    \"`This Long Column-Name` as `new col`\")\n",
    "  .show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithLongColName.createOrReplaceTempView(\"dfTableLong\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: Array[String] = Array(This Long Column-Name)\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "dfWithLongColName.select(col(\"This Long Column-Name\")).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res35: Array[String] = Array(DEST_COUNTRY_NAME, count)\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res36: org.apache.spark.sql.DataFrame = [count: bigint, This Long Column-Name: string]\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res37: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"count2\", col(\"count\").cast(\"long\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"count\") < 2).show(2)\n",
    "df.where(\"count < 2\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") =!= \"Croatia\")\n",
    "  .show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res40: Long = 256\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: Long = 125\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seed: Int = 5\n",
       "withReplacement: Boolean = false\n",
       "fraction: Double = 0.5\n",
       "res42: Long = 138\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 5\n",
    "val withReplacement = false\n",
    "val fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataFrames: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field], [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field])\n",
       "res43: Boolean = false\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val dataFrames = df.randomSplit(Array(0.25, 0.75), seed)\n",
    "dataFrames(0).count() > dataFrames(1).count() // False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\n",
       "newRows: Seq[org.apache.spark.sql.Row] = List([New Country,Other Country,5], [New Country 2,Other Country 3,1])\n",
       "parallelizedRows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[112] at parallelize at <console>:45\n",
       "newDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.Row\n",
    "val schema = df.schema\n",
    "val newRows = Seq(\n",
    "  Row(\"New Country\", \"Other Country\", 5L),\n",
    "  Row(\"New Country 2\", \"Other Country 3\", 1L)\n",
    ")\n",
    "val parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "val newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "df.union(newDF)\n",
    "  .where(\"count = 1\")\n",
    "  .where($\"ORIGIN_COUNTRY_NAME\" =!= \"United States\")\n",
    "  .show() // get all of them and we'll see our new rows at the end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{desc, asc}\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{desc, asc}\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "df.orderBy(desc(\"count\"), asc(\"DEST_COUNTRY_NAME\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "spark.read.format(\"json\").load(\"../data/flight-data/json/*-summary.json\")\n",
    "  .sortWithinPartitions(\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res50: Int = 1\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "df.rdd.getNumPartitions // 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res52: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res53: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res54: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "collectDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n",
       "res55: Array[org.apache.spark.sql.Row] = Array([United States,Romania,15], [United States,Croatia,1], [United States,Ireland,344], [Egypt,United States,15], [United States,India,62], [United States,Singapore,1], [United States,Grenada,62], [Costa Rica,United States,588], [Senegal,United States,40], [Moldova,United States,1])\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val collectDF = df.limit(10)\n",
    "collectDF.take(5) // take works with an Integer count\n",
    "collectDF.show() // this prints it out nicely\n",
    "collectDF.show(5, false)\n",
    "collectDF.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res56: java.util.Iterator[org.apache.spark.sql.Row] = IteratorWrapper(<iterator>)\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Different Types of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"../data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.lit\n",
       "res59: org.apache.spark.sql.DataFrame = [5: int, five: string ... 1 more field]\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------+\n",
      "|InvoiceNo|Description                        |\n",
      "+---------+-----------------------------------+\n",
      "|536365   |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|536365   |WHITE METAL LANTERN                |\n",
      "|536365   |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|536365   |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|536365   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+---------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.col\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.col\n",
    "df.where(col(\"InvoiceNo\").equalTo(536365))\n",
    "  .select(\"InvoiceNo\", \"Description\")\n",
    "  .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------+\n",
      "|InvoiceNo|Description                        |\n",
      "+---------+-----------------------------------+\n",
      "|536365   |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|536365   |WHITE METAL LANTERN                |\n",
      "|536365   |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|536365   |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|536365   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+---------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.col\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.col\n",
    "df.where(col(\"InvoiceNo\") === 536365)\n",
    "  .select(\"InvoiceNo\", \"Description\")\n",
    "  .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"InvoiceNo = 536365\")\n",
    "  .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|\n",
      "|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "|536367   |22748    |POPPY'S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"InvoiceNo <> 536365\")\n",
    "  .show(5, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "priceFilter: org.apache.spark.sql.Column = (UnitPrice > 600)\n",
       "descripFilter: org.apache.spark.sql.Column = contains(Description, POSTAGE)\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n",
    "df.where(col(\"StockCode\").isin(\"DOT\")).where(priceFilter.or(descripFilter))\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|unitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DOTCodeFilter: org.apache.spark.sql.Column = (StockCode = DOT)\n",
       "priceFilter: org.apache.spark.sql.Column = (UnitPrice > 600)\n",
       "descripFilter: org.apache.spark.sql.Column = contains(Description, POSTAGE)\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val DOTCodeFilter = col(\"StockCode\") === \"DOT\"\n",
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter.and(priceFilter.or(descripFilter)))\n",
    "  .where(\"isExpensive\")\n",
    "  .select(\"unitPrice\", \"isExpensive\").show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|   Description|UnitPrice|\n",
      "+--------------+---------+\n",
      "|DOTCOM POSTAGE|   569.77|\n",
      "|DOTCOM POSTAGE|   607.49|\n",
      "+--------------+---------+\n",
      "\n",
      "+--------------+---------+\n",
      "|   Description|UnitPrice|\n",
      "+--------------+---------+\n",
      "|DOTCOM POSTAGE|   569.77|\n",
      "|DOTCOM POSTAGE|   607.49|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{expr, not, col}\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{expr, not, col}\n",
    "df.withColumn(\"isExpensive\", not(col(\"UnitPrice\").leq(250)))\n",
    "  .filter(\"isExpensive\")\n",
    "  .select(\"Description\", \"UnitPrice\").show(5)\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\n",
    "  .filter(\"isExpensive\")\n",
    "  .select(\"Description\", \"UnitPrice\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"Description\").eqNullSafe(\"hello\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{expr, pow}\n",
       "fabricatedQuantity: org.apache.spark.sql.Column = (POWER((Quantity * UnitPrice), 2.0) + 5)\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{expr, pow}\n",
    "val fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.selectExpr(\n",
    "  \"CustomerId\",\n",
    "  \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|rounded|UnitPrice|\n",
      "+-------+---------+\n",
      "|    2.6|     2.55|\n",
      "|    3.4|     3.39|\n",
      "|    2.8|     2.75|\n",
      "|    3.4|     3.39|\n",
      "|    3.4|     3.39|\n",
      "+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{round, bround}\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{round, bround}\n",
    "df.select(round(col(\"UnitPrice\"), 1).alias(\"rounded\"), col(\"UnitPrice\")).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.lit\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.lit\n",
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.corr\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{corr}\n",
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max}\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colName: String = UnitPrice\n",
       "quantileProbs: Array[Double] = Array(0.5)\n",
       "relError: Double = 0.05\n",
       "res74: Array[Double] = Array(2.51)\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val colName = \"UnitPrice\"\n",
    "val quantileProbs = Array(0.5)\n",
    "val relError = 0.05\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError) // 2.51\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "|             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.stat.crosstab(\"StockCode\", \"Quantity\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "| StockCode_freqItems|  Quantity_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[90214E, 20728, 2...|[200, 128, 23, 32...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.stat.freqItems(Seq(\"StockCode\", \"Quantity\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.monotonically_increasing_id\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id()).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|initcap(Description)              |\n",
      "+----------------------------------+\n",
      "|White Hanging Heart T-light Holder|\n",
      "|White Metal Lantern               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.initcap\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{initcap}\n",
    "df.select(initcap(col(\"Description\"))).show(2, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------+\n",
      "|         Description|  lower(Description)|upper(lower(Description))|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{lower, upper}\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{lower, upper}\n",
    "df.select(col(\"Description\"),\n",
    "  lower(col(\"Description\")),\n",
    "  upper(lower(col(\"Description\")))).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+---+----------+\n",
      "|    ltrim|    rtrim| trim| lp|        rp|\n",
      "+---------+---------+-----+---+----------+\n",
      "|HELLO    |    HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO    |    HELLO|HELLO|HEL|HELLO     |\n",
      "+---------+---------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}\n",
    "df.select(\n",
    "    ltrim(lit(\"    HELLO    \")).as(\"ltrim\"),\n",
    "    rtrim(lit(\"    HELLO    \")).as(\"rtrim\"),\n",
    "    trim(lit(\"    HELLO    \")).as(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").as(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").as(\"rp\")).show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.regexp_replace\n",
       "simpleColors: Seq[String] = List(black, white, red, green, blue)\n",
       "regexString: String = BLACK|WHITE|RED|GREEN|BLUE\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.regexp_replace\n",
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"|\")\n",
    "// the | signifies `OR` in regular expression syntax\n",
    "df.select(\n",
    "  regexp_replace(col(\"Description\"), regexString, \"COLOR\").alias(\"color_clean\"),\n",
    "  col(\"Description\")).show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|translate(Description, LEET, 1337)|         Description|\n",
      "+----------------------------------+--------------------+\n",
      "|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n",
      "|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n",
      "+----------------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.translate\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"), col(\"Description\"))\n",
    "  .show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|color_clean|         Description|\n",
      "+-----------+--------------------+\n",
      "|      WHITE|WHITE HANGING HEA...|\n",
      "|      WHITE| WHITE METAL LANTERN|\n",
      "+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.regexp_extract\n",
       "regexString: String = (BLACK|WHITE|RED|GREEN|BLUE)\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.regexp_extract\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"(\", \"|\", \")\")\n",
    "// the | signifies OR in regular expression syntax\n",
    "df.select(\n",
    "     regexp_extract(col(\"Description\"), regexString, 1).alias(\"color_clean\"),\n",
    "     col(\"Description\")).show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "containsBlack: org.apache.spark.sql.Column = contains(Description, BLACK)\n",
       "containsWhite: org.apache.spark.sql.Column = contains(DESCRIPTION, WHITE)\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val containsBlack = col(\"Description\").contains(\"BLACK\")\n",
    "val containsWhite = col(\"DESCRIPTION\").contains(\"WHITE\")\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack.or(containsWhite))\n",
    "  .where(\"hasSimpleColor\")\n",
    "  .select(\"Description\").show(3, false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "simpleColors: Seq[String] = List(black, white, red, green, blue)\n",
       "selectedColumns: Seq[org.apache.spark.sql.Column] = List(contains(Description, BLACK) AS `is_black`, contains(Description, WHITE) AS `is_white`, contains(Description, RED) AS `is_red`, contains(Description, GREEN) AS `is_green`, contains(Description, BLUE) AS `is_blue`, unresolvedstar())\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val selectedColumns = simpleColors.map(color => {\n",
    "   col(\"Description\").contains(color.toUpperCase).alias(s\"is_$color\")\n",
    "}):+expr(\"*\") // could also append this value\n",
    "df.select(selectedColumns:_*).where(col(\"is_white\").or(col(\"is_red\")))\n",
    "  .select(\"Description\").show(3, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{current_date, current_timestamp}\n",
       "dateDF: org.apache.spark.sql.DataFrame = [id: bigint, today: date ... 1 more field]\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{current_date, current_timestamp}\n",
    "val dateDF = spark.range(10)\n",
    "  .withColumn(\"today\", current_date())\n",
    "  .withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2021-05-03|        2021-05-13|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{date_add, date_sub}\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{date_add, date_sub}\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{datediff, months_between, to_date}\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{datediff, months_between, to_date}\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n",
    "  .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "dateDF.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\"))\n",
    "  .select(months_between(col(\"start\"), col(\"end\"))).show(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{to_date, lit}\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{to_date, lit}\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\n",
    "  .select(to_date(col(\"date\"))).show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.to_date\n",
       "dateFormat: String = yyyy-dd-MM\n",
       "cleanDateDF: org.apache.spark.sql.DataFrame = [date: date, date2: date]\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.to_date\n",
    "val dateFormat = \"yyyy-dd-MM\"\n",
    "val cleanDateDF = spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|to_timestamp(`date`, 'yyyy-dd-MM')|\n",
      "+----------------------------------+\n",
      "|               2017-11-12 00:00:00|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.to_timestamp\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.to_timestamp\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|date|date2|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|coalesce(Description, CustomerId)|\n",
      "+---------------------------------+\n",
      "|             WHITE HANGING HEA...|\n",
      "|              WHITE METAL LANTERN|\n",
      "|             CREAM CUPID HEART...|\n",
      "|             KNITTED UNION FLA...|\n",
      "|             RED WOOLLY HOTTIE...|\n",
      "|             SET 7 BABUSHKA NE...|\n",
      "|             GLASS STAR FROSTE...|\n",
      "|             HAND WARMER UNION...|\n",
      "|             HAND WARMER RED P...|\n",
      "|             ASSORTED COLOUR B...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             FELTCRAFT PRINCES...|\n",
      "|             IVORY KNITTED MUG...|\n",
      "|             BOX OF 6 ASSORTED...|\n",
      "|             BOX OF VINTAGE JI...|\n",
      "|             BOX OF VINTAGE AL...|\n",
      "|             HOME BUILDING BLO...|\n",
      "|             LOVE BUILDING BLO...|\n",
      "|             RECIPE BOX WITH M...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.coalesce\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.coalesce\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res98: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop()\n",
    "df.na.drop(\"any\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res99: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res100: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "df.na.drop(\"all\", Seq(\"StockCode\", \"InvoiceNo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res101: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(\"All Null values become this string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res102: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "df.na.fill(5, Seq(\"StockCode\", \"InvoiceNo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fillColValues: scala.collection.immutable.Map[String,Any] = Map(StockCode -> 5, Description -> No Value)\n",
       "res103: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val fillColValues = Map(\"StockCode\" -> 5, \"Description\" -> \"No Value\")\n",
    "df.na.fill(fillColValues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res104: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "df.na.replace(\"Description\", Map(\"\" -> \"UNKNOWN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res105: org.apache.spark.sql.DataFrame = [complex: struct<Description: string, InvoiceNo: string>, InvoiceNo: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res106: org.apache.spark.sql.DataFrame = [complex: struct<Description: string, InvoiceNo: string>, InvoiceNo: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.struct\n",
       "complexDF: org.apache.spark.sql.DataFrame = [complex: struct<Description: string, InvoiceNo: string>]\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.struct\n",
    "val complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res110: org.apache.spark.sql.DataFrame = [complex.Description: string]\n"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\")\n",
    "complexDF.select(col(\"complex\").getField(\"Description\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res111: org.apache.spark.sql.DataFrame = [Description: string, InvoiceNo: string]\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexDF.select(\"complex.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.split\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.split\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\n",
    "  .selectExpr(\"array_col[0]\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.size\n"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.size\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2) // shows 5 and 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.array_contains\n"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{split, explode}\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{split, explode}\n",
    "\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\n",
    "  .withColumn(\"exploded\", explode(col(\"splitted\")))\n",
    "  .select(\"Description\", \"InvoiceNo\", \"exploded\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         complex_map|\n",
      "+--------------------+\n",
      "|[WHITE HANGING HE...|\n",
      "|[WHITE METAL LANT...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.map\n"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.map\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            null|\n",
      "|                          536365|\n",
      "+--------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n",
    "  .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                 key| value|\n",
      "+--------------------+------+\n",
      "|WHITE HANGING HEA...|536365|\n",
      "| WHITE METAL LANTERN|536365|\n",
      "+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n",
    "  .selectExpr(\"explode(complex_map)\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsonDF: org.apache.spark.sql.DataFrame = [jsonString: string]\n"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "  '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|                  c0|\n",
      "+------+--------------------+\n",
      "|     2|{\"myJSONValue\":[1...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{get_json_object, json_tuple}\n"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{get_json_object, json_tuple}\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\") as \"column\",\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|column|\n",
      "+------+\n",
      "|  null|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.selectExpr(\n",
    "  \"json_tuple(jsonString, '$.myJSONKey.myJSONValue[1]') as column\").show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.to_json\n",
       "res123: org.apache.spark.sql.DataFrame = [to_json(myStruct): string]\n"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n",
    "  .select(to_json(col(\"myStruct\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  from_json(newJSON)|             newJSON|\n",
      "+--------------------+--------------------+\n",
      "|[536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n",
      "|[536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.from_json\n",
       "import org.apache.spark.sql.types._\n",
       "parseSchema: org.apache.spark.sql.types.StructType = StructType(StructField(InvoiceNo,StringType,true), StructField(Description,StringType,true))\n"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.from_json\n",
    "import org.apache.spark.sql.types._\n",
    "val parseSchema = new StructType(Array(\n",
    "  new StructField(\"InvoiceNo\",StringType,true),\n",
    "  new StructField(\"Description\",StringType,true)))\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n",
    "  .select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\n",
    "  .select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "udfExampleDF: org.apache.spark.sql.DataFrame = [num: bigint]\n",
       "power3: (number: Double)Double\n",
       "res126: Double = 8.0\n"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(number:Double):Double = number * number * number\n",
    "power3(2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.udf\n",
       "power3udf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4700/1870837060@606aea6e,DoubleType,List(Some(Schema(DoubleType,false))),None,false,true)\n"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.udf\n",
    "val power3udf = udf(power3(_:Double):Double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|UDF(num)|\n",
      "+--------+\n",
      "|     0.0|\n",
      "|     1.0|\n",
      "|     8.0|\n",
      "|    27.0|\n",
      "|    64.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|        0.0|\n",
      "|        1.0|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "spark.udf.register(\"power3\", power3(_:Double):Double)\n",
    "udfExampleDF.selectExpr(\"power3(num)\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"../data/retail-data/all/*.csv\")\n",
    "  .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res131: Boolean = true\n"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() == 541909\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.count\n"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.count\n",
    "df.select(count(\"StockCode\")).show() // 541909\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.countDistinct\n"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show() // 4070\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.approx_count_distinct\n"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show() // 3364\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{first, last}\n"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{first, last}\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{min, max}\n"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{min, max}\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.sum\n"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.sum\n",
    "df.select(sum(\"Quantity\")).show() // 5176450\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.sumDistinct\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show() // 29310\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{sum, count, avg, expr}\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{sum, count, avg, expr}\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\n",
    "  .selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|47559.30364660923| 47559.39140929892|  218.08095663447835|   218.08115785023455|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{var_pop, stddev_pop}\n",
       "import org.apache.spark.sql.functions.{var_samp, stddev_samp}\n"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{var_pop, stddev_pop}\n",
    "import org.apache.spark.sql.functions.{var_samp, stddev_samp}\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|  skewness(Quantity)|kurtosis(Quantity)|\n",
      "+--------------------+------------------+\n",
      "|-0.26407557610528376|119768.05495530753|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{skewness, kurtosis}\n"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{skewness, kurtosis}\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085640497E-4|             1052.7280543915997|            1052.7260778754955|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}\n"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "    covar_pop(\"InvoiceNo\", \"Quantity\")).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{collect_set, collect_list}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{collect_set, collect_list}\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "|   544578|     12365|    1|\n",
      "|   545165|     16339|   20|\n",
      "|   545289|     14732|   30|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "|   538184|  26|             26|\n",
      "|   538517|  53|             53|\n",
      "|   538879|  19|             19|\n",
      "|   539275|   6|              6|\n",
      "|   539630|  12|             12|\n",
      "|   540499|  24|             24|\n",
      "|   540540|  22|             22|\n",
      "|  C540850|   1|              1|\n",
      "|   540976|  48|             48|\n",
      "|   541432|   4|              4|\n",
      "|   541518| 101|            101|\n",
      "|   541783|  35|             35|\n",
      "|   542026|   9|              9|\n",
      "|   542375|   6|              6|\n",
      "|  C542604|   8|              8|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.count\n"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "  count(\"Quantity\").alias(\"quan\"),\n",
    "  expr(\"count(Quantity)\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|  C542604|              -8.0|  15.173990905493518|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "df.groupBy(\"InvoiceNo\").agg(\"Quantity\"->\"avg\", \"Quantity\"->\"stddev_pop\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, to_date}\n",
       "dfWithDate: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{col, to_date}\n",
    "val dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"),\n",
    "  \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions.col\n",
       "windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@4b0e0cac\n"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.col\n",
    "val windowSpec = Window\n",
    "  .partitionBy(\"CustomerId\", \"date\")\n",
    "  .orderBy(col(\"Quantity\").desc)\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.max\n",
       "maxPurchaseQuantity: org.apache.spark.sql.Column = max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.max\n",
    "val maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{dense_rank, rank}\n",
       "purchaseDenseRank: org.apache.spark.sql.Column = DENSE_RANK() OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n",
       "purchaseRank: org.apache.spark.sql.Column = RANK() OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{dense_rank, rank}\n",
    "val purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "val purchaseRank = rank().over(windowSpec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----+--------+------------+-----------------+-------------------+\n",
      "|     12346|null|   74215|           1|                1|              74215|\n",
      "|     12346|null|  -74215|           2|                2|              74215|\n",
      "|     12347|null|     240|           1|                1|                240|\n",
      "|     12347|null|      36|           2|                2|                240|\n",
      "|     12347|null|      36|           2|                2|                240|\n",
      "|     12347|null|      36|           2|                2|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "|     12347|null|      24|           5|                3|                240|\n",
      "+----------+----+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.col\n"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfNoNull: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------+\n",
      "|Date|             Country|total_quantity|\n",
      "+----+--------------------+--------------+\n",
      "|null|              Poland|          2580|\n",
      "|null|               Malta|           776|\n",
      "|null|              Sweden|         23810|\n",
      "|null|                EIRE|        100984|\n",
      "|null|              Norway|          8246|\n",
      "|null|              France|         71048|\n",
      "|null|             Iceland|          1271|\n",
      "|null|             Lebanon|           386|\n",
      "|null|         Unspecified|          2659|\n",
      "|null|               Spain|         18918|\n",
      "|null|                null|       3297071|\n",
      "|null|                null|       5176450|\n",
      "|null|               Italy|          4060|\n",
      "|null|             Germany|         76234|\n",
      "|null|           Australia|         71593|\n",
      "|null|              Israel|          4310|\n",
      "|null|  European Community|           497|\n",
      "|null|      Czech Republic|           307|\n",
      "|null|United Arab Emirates|           982|\n",
      "|null|      United Kingdom|       2667928|\n",
      "+----+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rolledUpDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Date: date, Country: string ... 1 more field]\n"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\n",
    "  .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\n",
    "  .orderBy(\"Date\")\n",
    "rolledUpDF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      null|   null|       3297071|\n",
      "|      null|   null|       5176450|\n",
      "|2010-12-01|   null|         26814|\n",
      "|2010-12-02|   null|         21023|\n",
      "|2010-12-03|   null|         14830|\n",
      "|2010-12-05|   null|         16395|\n",
      "|2010-12-06|   null|         21419|\n",
      "|2010-12-07|   null|         24995|\n",
      "|2010-12-08|   null|         22741|\n",
      "|2010-12-09|   null|         18431|\n",
      "|2010-12-10|   null|         20297|\n",
      "|2010-12-12|   null|         10565|\n",
      "|2010-12-13|   null|         17623|\n",
      "|2010-12-14|   null|         20098|\n",
      "|2010-12-15|   null|         18229|\n",
      "|2010-12-16|   null|         29632|\n",
      "|2010-12-17|   null|         16069|\n",
      "|2010-12-19|   null|          3795|\n",
      "|2010-12-20|   null|         14965|\n",
      "|2010-12-21|   null|         15467|\n",
      "+----------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------+\n",
      "|Date|             Country|total_quantity|\n",
      "+----+--------------------+--------------+\n",
      "|null|      United Kingdom|       2667928|\n",
      "|null|             Germany|         76234|\n",
      "|null|      Czech Republic|           307|\n",
      "|null|                null|       3297071|\n",
      "|null|           Australia|         71593|\n",
      "|null|  European Community|           497|\n",
      "|null|               Italy|          4060|\n",
      "|null|                EIRE|        100984|\n",
      "|null|                null|       5176450|\n",
      "|null|             Lebanon|           386|\n",
      "|null|United Arab Emirates|           982|\n",
      "|null|              Israel|          4310|\n",
      "|null|              France|         71048|\n",
      "|null|              Norway|          8246|\n",
      "|null|               Malta|           776|\n",
      "|null|               Spain|         18918|\n",
      "|null|              Poland|          2580|\n",
      "|null|         Unspecified|          2659|\n",
      "|null|              Greece|          1320|\n",
      "|null|             Iceland|          1271|\n",
      "+----+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Date IS NULL\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n",
      "|Date|             Country|sum(Quantity)|\n",
      "+----+--------------------+-------------+\n",
      "|null|United Arab Emirates|          982|\n",
      "|null|             Lebanon|          386|\n",
      "|null|              Cyprus|         6317|\n",
      "|null|           Hong Kong|         4769|\n",
      "|null|                null|      3297071|\n",
      "|null|United Arab Emirates|          982|\n",
      "|null|     Channel Islands|         9479|\n",
      "|null|             Denmark|         8188|\n",
      "|null|               Spain|        26824|\n",
      "|null|             Finland|        10666|\n",
      "|null|                 USA|         1034|\n",
      "|null|      Czech Republic|          307|\n",
      "|null|           Singapore|         5234|\n",
      "|null|      Czech Republic|          592|\n",
      "|null|              Norway|        19247|\n",
      "|null|  European Community|          497|\n",
      "|null|           Australia|        71593|\n",
      "|null|             Germany|        76234|\n",
      "|null|      United Kingdom|      2667928|\n",
      "|null|  European Community|          497|\n",
      "+----+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\")))\n",
    "  .select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+-------------+\n",
      "|customerId|stockCode|grouping_id()|sum(Quantity)|\n",
      "+----------+---------+-------------+-------------+\n",
      "|      null|     null|            3|      5176450|\n",
      "|      null|    84226|            2|           17|\n",
      "|      null|    22856|            2|          518|\n",
      "|      null|    22352|            2|         3077|\n",
      "|      null|    22413|            2|         3759|\n",
      "|      null|    22940|            2|         1896|\n",
      "|      null|    16014|            2|        13328|\n",
      "|      null|   85114B|            2|          476|\n",
      "|      null|    22155|            2|         2336|\n",
      "|      null|    85016|            2|          522|\n",
      "|      null|    37509|            2|            5|\n",
      "|      null|    22400|            2|         1399|\n",
      "|      null|    22260|            2|          548|\n",
      "|      null|    20832|            2|         1200|\n",
      "|      null|    21987|            2|         2100|\n",
      "|      null|    22316|            2|          660|\n",
      "|      null|    22732|            2|         1645|\n",
      "|      null|   10124A|            2|           16|\n",
      "|      null|    35646|            2|          619|\n",
      "|      null|   85132a|            2|            8|\n",
      "+----------+---------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{grouping_id, sum, expr}\n"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{grouping_id, sum, expr}\n",
    "\n",
    "dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\n",
    ".orderBy(expr(\"grouping_id()\").desc)\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pivoted: org.apache.spark.sql.DataFrame = [date: date, Australia_sum(Quantity): bigint ... 113 more fields]\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|      date|USA_sum(Quantity)|\n",
      "+----------+-----------------+\n",
      "|2011-12-06|             null|\n",
      "|2011-12-09|             null|\n",
      "|2011-12-08|             -196|\n",
      "|2011-12-07|             null|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\" ,\"`USA_sum(Quantity)`\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
       "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "defined class BoolAnd\n"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "class BoolAnd extends UserDefinedAggregateFunction {\n",
    "  def inputSchema: org.apache.spark.sql.types.StructType =\n",
    "    StructType(StructField(\"value\", BooleanType) :: Nil)\n",
    "  def bufferSchema: StructType = StructType(\n",
    "    StructField(\"result\", BooleanType) :: Nil\n",
    "  )\n",
    "  def dataType: DataType = BooleanType\n",
    "  def deterministic: Boolean = true\n",
    "  def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "    buffer(0) = true\n",
    "  }\n",
    "  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "    buffer(0) = buffer.getAs[Boolean](0) && input.getAs[Boolean](0)\n",
    "  }\n",
    "  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "    buffer1(0) = buffer1.getAs[Boolean](0) && buffer2.getAs[Boolean](0)\n",
    "  }\n",
    "  def evaluate(buffer: Row): Any = {\n",
    "    buffer(0)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|booland(t)|booland(f)|\n",
      "+----------+----------+\n",
      "|      true|     false|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ba: BoolAnd = BoolAnd@6d28c148\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val ba = new BoolAnd\n",
    "spark.udf.register(\"booland\", ba)\n",
    "import org.apache.spark.sql.functions._\n",
    "spark.range(1)\n",
    "  .selectExpr(\"explode(array(TRUE, TRUE, TRUE)) as t\")\n",
    "  .selectExpr(\"explode(array(TRUE, FALSE, TRUE)) as f\", \"t\")\n",
    "  .select(ba(col(\"t\")), expr(\"booland(f)\"))\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n",
       "graduateProgram: org.apache.spark.sql.DataFrame = [id: int, degree: string ... 2 more fields]\n",
       "sparkStatus: org.apache.spark.sql.DataFrame = [id: int, status: string]\n"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val person = Seq(\n",
    "    (0, \"Bill Chambers\", 0, Seq(100)),\n",
    "    (1, \"Matei Zaharia\", 1, Seq(500, 250, 100)),\n",
    "    (2, \"Michael Armbrust\", 1, Seq(250, 100)))\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "val graduateProgram = Seq(\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\"))\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "val sparkStatus = Seq(\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\"))\n",
    "  .toDF(\"id\", \"status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinExpression: org.apache.spark.sql.Column = (graduate_program = id)\n"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val joinExpression = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wrongJoinExpression: org.apache.spark.sql.Column = (name = school)\n"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val wrongJoinExpression = person.col(\"name\") === graduateProgram.col(\"school\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinType: String = inner\n"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "var joinType = \"inner\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinType: String = outer\n"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"outer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinType: String = left_outer\n"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.join(person, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinType: String = right_outer\n"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"right_outer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinType: String = left_semi\n"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"left_semi\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.join(person, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradProgram2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, degree: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val gradProgram2 = graduateProgram.union(Seq(\n",
    "    (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")).toDF())\n",
    "\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+---+----------------+----------------+---------------+\n",
      "| id| degree|          department|           school| id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------------+---+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|  0|   Bill Chambers|               0|          [100]|\n",
      "+---+-------+--------------------+-----------------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gradProgram2.join(person, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = left_anti\n"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"left_anti\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = cross\n"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.crossJoin(graduateProgram).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\n"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "person.withColumnRenamed(\"id\", \"personId\")\n",
    "  .join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradProgramDupe: org.apache.spark.sql.DataFrame = [graduate_program: int, degree: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = graduate_program)\n"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExpr = gradProgramDupe.col(\"graduate_program\") === person.col(\n",
    "  \"graduate_program\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|graduate_program| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|               0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|graduate_program| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|               0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|               0|\n",
      "|               1|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe,\"graduate_program\").select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|               0|\n",
      "|               1|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).drop(person.col(\"graduate_program\"))\n",
    "  .select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = id)\n"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "person.join(graduateProgram, joinExpr).drop(graduateProgram.col(\"id\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|grad_id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|      0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|      1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|      1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gradProgram3: org.apache.spark.sql.DataFrame = [grad_id: int, degree: string ... 2 more fields]\n",
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = grad_id)\n"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\n",
    "val joinExpr = person.col(\"graduate_program\") === gradProgram3.col(\"grad_id\")\n",
    "person.join(gradProgram3, joinExpr).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) BroadcastHashJoin [graduate_program#14349], [id#14368], Inner, BuildLeft\n",
      ":- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[2, int, false] as bigint))), [id=#2755]\n",
      ":  +- LocalTableScan [id#14347, name#14348, graduate_program#14349, spark_status#14350]\n",
      "+- *(1) LocalTableScan [id#14368, degree#14369, department#14370, school#14371]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = id)\n"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "\n",
    "person.join(graduateProgram, joinExpr).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) BroadcastHashJoin [graduate_program#14349], [id#14368], Inner, BuildRight\n",
      ":- *(1) LocalTableScan [id#14347, name#14348, graduate_program#14349, spark_status#14350]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint))), [id=#3083]\n",
      "   +- LocalTableScan [id#14368, degree#14369, department#14370, school#14371]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.broadcast\n",
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = id)\n"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.broadcast\n",
    "val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
       "myManualSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))\n"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "val myManualSchema = new StructType(Array(\n",
    "  new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "  new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "  new StructField(\"count\", LongType, false)\n",
    "))\n",
    "spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"mode\", \"FAILFAST\")\n",
    "  .schema(myManualSchema)\n",
    "  .load(\"../data/flight-data/csv/2010-summary.csv\")\n",
    "  .show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "spark.read.format(\"json\").option(\"mode\", \"FAILFAST\").schema(myManualSchema)\n",
    "  .load(\"../data/flight-data/json/2010-summary.json\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\")\n",
    "  .load(\"../data/flight-data/parquet/2010-summary.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// in Scala\n",
    "spark.read.format(\"orc\").load(\"../data/flight-data/orc/2010-summary.orc\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                rows|\n",
      "+--------------------+\n",
      "|[DEST_COUNTRY_NAM...|\n",
      "|[United States, R...|\n",
      "|[United States, I...|\n",
      "|[United States, I...|\n",
      "|[Egypt, United St...|\n",
      "|[Equatorial Guine...|\n",
      "|[United States, S...|\n",
      "|[United States, G...|\n",
      "|[Costa Rica, Unit...|\n",
      "|[Senegal, United ...|\n",
      "|[United States, M...|\n",
      "|[Guyana, United S...|\n",
      "|[United States, S...|\n",
      "|[Malta, United St...|\n",
      "|[Bolivia, United ...|\n",
      "|[Anguilla, United...|\n",
      "|[Turks and Caicos...|\n",
      "|[United States, A...|\n",
      "|[Saint Vincent an...|\n",
      "|[Italy, United St...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.textFile(\"../data/flight-data/csv/2010-summary.csv\")\n",
    "  .selectExpr(\"split(value, ',') as rows\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvFile: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val csvFile = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(myManualSchema)\n",
    "  .load(\"../data/flight-data/csv/2010-summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\n",
    "  .save(\"/tmp/my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"/tmp/simple-text-file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numberBuckets: Int = 10\n",
       "columnToBucketBy: String = count\n"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "csvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\")\n",
    "  .write.partitionBy(\"count\").text(\"/tmp/five-csv-files2.csv\")\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\n",
    "  .save(\"/tmp/partitioned-files.parquet\")\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "val numberBuckets = 10\n",
    "val columnToBucketBy = \"count\"\n",
    "\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\n",
    "  .bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFiles\")\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|(1 + 1)|\n",
      "+-------+\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT 1 + 1\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res221: Long = 12\n"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "spark.read.json(\"../data/flight-data/json/2015-summary.json\")\n",
    "  .createOrReplaceTempView(\"some_sql_view\") // DF => SQL\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "  .where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\n",
    "  .count() // SQL => DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [DEST_COUNTRY_NAME#16436, ORIGIN_COUNTRY_NAME#16437, count#16438L]\n",
      "+- *(1) Filter (isnotnull(DEST_COUNTRY_NAME#16436) AND (DEST_COUNTRY_NAME#16436 = United States))\n",
      "   +- BatchScan[DEST_COUNTRY_NAME#16436, ORIGIN_COUNTRY_NAME#16437, count#16438L] JsonScan Location: InMemoryFileIndex[file:/home/u1/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flights: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n",
       "just_usa_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights = spark.read.format(\"json\")\n",
    "  .load(\"../data/flight-data/json/2015-summary.json\")\n",
    "val just_usa_df = flights.where(\"dest_country_name = 'United States'\")\n",
    "just_usa_df.selectExpr(\"*\").explain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "power3: (number: Double)Double\n",
       "res224: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5709/1344795075@7060eb57,DoubleType,List(Some(Schema(DoubleType,false))),Some(power3),false,true)\n"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def power3(number:Double):Double = number * number * number\n",
    "spark.udf.register(\"power3\", power3(_:Double):Double)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Flight\n"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Flight(DEST_COUNTRY_NAME: String,\n",
    "                  ORIGIN_COUNTRY_NAME: String, count: BigInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightsDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n",
       "flights: org.apache.spark.sql.Dataset[Flight] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightsDF = spark.read\n",
    "  .parquet(\"../data/flight-data/parquet/2010-summary.parquet/\")\n",
    "val flights = flightsDF.as[Flight]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res240: String = United States\n"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.first.DEST_COUNTRY_NAME // United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "originIsDestination: (flight_row: Flight)Boolean\n"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def originIsDestination(flight_row: Flight): Boolean = {\n",
    "  return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res229: Array[Flight] = Array(Flight(United States,United States,348113))\n"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.collect().filter(flight_row => originIsDestination(flight_row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "destinations: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val destinations = flights.map(f => f.DEST_COUNTRY_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class FlightMetadata\n"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class FlightMetadata(count: BigInt, randomData: BigInt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightsMeta: org.apache.spark.sql.Dataset[FlightMetadata] = [count: bigint, randomData: bigint]\n"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightsMeta = spark.range(500).map(x => (x, scala.util.Random.nextLong))\n",
    "  .withColumnRenamed(\"_1\", \"count\").withColumnRenamed(\"_2\", \"randomData\")\n",
    "  .as[FlightMetadata]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flights2: org.apache.spark.sql.Dataset[(Flight, FlightMetadata)] = [_1: struct<DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field>, _2: struct<count: bigint, randomData: bigint>]\n"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights2 = flights\n",
    "  .joinWith(flightsMeta, flights.col(\"count\") === flightsMeta.col(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res230: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string]\n"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights2.selectExpr(\"_1.DEST_COUNTRY_NAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res231: Array[(Flight, FlightMetadata)] = Array((Flight(United States,Uganda,1),FlightMetadata(1,-6396282296836297926)), (Flight(United States,French Guiana,1),FlightMetadata(1,-6396282296836297926)))\n"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights2.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flights2: org.apache.spark.sql.DataFrame = [count: bigint, DEST_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights2 = flights.join(flightsMeta, Seq(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flights2: org.apache.spark.sql.DataFrame = [count: bigint, DEST_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights2 = flights.join(flightsMeta.toDF(), Seq(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res232: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count: bigint]\n"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.groupBy(\"DEST_COUNTRY_NAME\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res233: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[value#16618], functions=[count(1)])\n",
      "+- *(2) HashAggregate(keys=[value#16618], functions=[partial_count(1)])\n",
      "   +- *(2) Project [value#16618]\n",
      "      +- AppendColumns $line323.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$5929/883505811@634759b3, newInstance(class $line294.$read$$iw$$iw$Flight), [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#16618]\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- BatchScan[DEST_COUNTRY_NAME#16448, ORIGIN_COUNTRY_NAME#16449, count#16450L] ParquetScan Location: InMemoryFileIndex[file:/home/u1/Spark-The-Definitive-Guide/data/flight-data/parquet/2010-summary...., ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>, PushedFilters: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).count().explain\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
