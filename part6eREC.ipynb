{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: The Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: Advanced Analytics and Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recs = '/Users/grp/sparkTheDefinitiveGuide/data/sample_movielens_ratings.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #28 - Recommendation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  study of people's preferences (ratings and observed behaviors)\n",
    "-  explicit feedback:\n",
    "    -  numerical rating aimed to predict (ex: 4 out of 5 stars)\n",
    "-  implicit feedback:\n",
    "    -  ratings represent strength of interactions observed between user and item (measures level of confidence in user's preference on that particular item (ex: # of visits/clicks on a particular webpage)\n",
    "\n",
    "### Use Cases:\n",
    "-  recommendations (ex: movies, courses, products)\n",
    "\n",
    "### MLlib Recommendation Models:\n",
    "-  ALS (Alternating Least Squares)\n",
    "-  FPM (Frequent Pattern Mining)\n",
    "\n",
    "### Evaluators:\n",
    "-  aim to optimize at reducing total differences between user's ratings and true values\n",
    "-  good to set coldStartStrategy to \"drop\" instead of NaN then switch back to NaN when making predictions in production\n",
    "-  regression evaluator (RegressionEvaluator) expect a \"predicted value\" and a \"true value\"\n",
    "-  regression metrics (RegressionMetrics) supported:\n",
    "    -  RMSE (root mean squared error)\n",
    "    -  MSE (mean squared error)\n",
    "    -  R2 (r squared)\n",
    "    -  MAE (mean absolute error)\n",
    "    -  EXPLAINED VARIANCE\n",
    "-  ranking metrics (RankingMetric):\n",
    "    -  compares recommendations with actual set of ratings from user\n",
    "    -  does not focus on rank value rather if algorithm has recommended an already ranked item to user\n",
    "    -  MEAN AVERAGE PRECISION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration:\n",
    "-  Model Hyperparameters (structure of how model can be initialized)\n",
    "-  Training Parameters (structure of how model can be trained)\n",
    "-  Prediction Parameters (structured of how model determines making predictions)\n",
    "-  Model Summary (provides information about final trained model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #28 Exercises (Rec)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-------------------+\n",
      "|              value|\n",
      "+-------------------+\n",
      "|0::2::3::1424380312|\n",
      "|0::3::1::1424380312|\n",
      "|0::5::2::1424380312|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.text(recs)\n",
    "data.printSchema()\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ALS:\n",
    "-  finds K-dimensional feature vector via for each user/item pair (gets dot product relationship) to determine user's rating for that item\n",
    "    -  input dataset schema:\n",
    "        -  user ID column\n",
    "        -  item ID column\n",
    "        -  rating column\n",
    "    -  performs collaborative filtering:\n",
    "        -  makes recommendations based only on which items users interacted with in the past\n",
    "    -  \"cold start problem\":\n",
    "        -  when new ratings appear that are not in the training set thus algorithm will not know what to recommend\n",
    "-  Model Hyperparameters:\n",
    "    -  rank:\n",
    "        -  determines dimension of feature vectors learned for users and items\n",
    "        -  must be tuned accordinly\n",
    "        -  too high a rank may cause overfitting\n",
    "        -  too low a rank may make bad training predictions\n",
    "    -  alpha:\n",
    "        -  sets baseline confidence for preference from implicit feedback\n",
    "    -  regParam:\n",
    "        -  controls regularization to prevent overfitting\n",
    "    -  implicitPrefs:\n",
    "        -  TRUE (sets implicit behavior) or FALSE (sets explicit behavior; set as default)\n",
    "    -  nonnegative:\n",
    "        -  TRUE (places non-negative contraints on the least-squares and only returns non-negative feature vectors) or FALSE (set as default)\n",
    "-  Training Parameters:\n",
    "    -  groups data into blocks that are distributed across cluster\n",
    "    -  amount of data in each block can have significant impact on training time and performance\n",
    "    -  rule of thumb is to aim for approximately 1 to 5 million ratings per block\n",
    "        -  numUserBlocks:\n",
    "            -  determines how many blocks to split the users into ... default is 10\n",
    "        -  numItemBlocks:\n",
    "            -  determines how many blocks to split the items into  ... default is 10\n",
    "        -  maxIter:\n",
    "            -  total # of iterations over data before stopping\n",
    "            -  should be adjusted if objective history shows lack of flatline and volatile differences between iterations\n",
    "        -  checkpointInterval:\n",
    "            -  allows to save model state during training to quickly recover from any node failures\n",
    "        -  seed:\n",
    "            -  random seed to re-generate same results\n",
    "-  Prediction Parameters:\n",
    "    -  coldStartStrategy:\n",
    "        -  determines what the model should predict for users or items that did not appear in training set\n",
    "        -  defaultly assigns NaN prediction values when finding user/item not present in model\n",
    "        -  can set parameter to \"drop\" to drop any rows in DF of predictions that contain NaN values\n",
    "        -  good to set coldStartStrategy to \"drop\" instead of NaN then switch back to NaN when making predictions in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: alpha for implicit preference (default: 1.0)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "coldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: 'nan', 'drop'. (default: nan)\n",
      "finalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\n",
      "implicitPrefs: whether to use implicit preference (default: False)\n",
      "intermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\n",
      "itemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: movieId)\n",
      "maxIter: max number of iterations (>= 0). (default: 10, current: 5)\n",
      "nonnegative: whether to use nonnegative constraint for least squares (default: False)\n",
      "numItemBlocks: number of item blocks (default: 10)\n",
      "numUserBlocks: number of user blocks (default: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "rank: rank of the factorization (default: 10)\n",
      "ratingCol: column name for ratings (default: rating, current: rating)\n",
      "regParam: regularization parameter (>= 0). (default: 0.1, current: 0.01)\n",
      "seed: random seed. (default: -1517157561977538513)\n",
      "userCol: column name for user ids. Ids must be within the integer value range. (default: user, current: userId)\n"
     ]
    }
   ],
   "source": [
    "ratings = spark.read.text(recs)\\\n",
    ".rdd.toDF()\\\n",
    ".selectExpr(\"split(value , '::') as col\")\\\n",
    ".selectExpr(\n",
    "    \"cast(col[0] as int) as userId\",\n",
    "    \"cast(col[1] as int) as movieId\",\n",
    "    \"cast(col[2] as float) as rating\",\n",
    "    \"cast(col[3] as long) as timestamp\")\n",
    "\n",
    "training, test = ratings.randomSplit([0.8, 0.2])\n",
    "als = ALS()\\\n",
    ".setMaxIter(5)\\\n",
    ".setRegParam(0.01)\\\n",
    ".setUserCol(\"userId\")\\\n",
    ".setItemCol(\"movieId\")\\\n",
    ".setRatingCol(\"rating\")\n",
    "\n",
    "print(als.explainParams())\n",
    "\n",
    "alsModel = als.fit(training)\n",
    "predictions = alsModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n",
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     0|      2|   3.0|1424380312|\n",
      "|     0|      3|   1.0|1424380312|\n",
      "|     0|      5|   2.0|1424380312|\n",
      "+------+-------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()\n",
    "ratings.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|userId|            col|\n",
      "+------+---------------+\n",
      "|    28|[92, 4.9692082]|\n",
      "|    28| [12, 4.601353]|\n",
      "|    28|[89, 4.2311087]|\n",
      "+------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+---------------+\n",
      "|movieId|            col|\n",
      "+-------+---------------+\n",
      "|     31|[12, 3.8597145]|\n",
      "|     31| [6, 3.5119514]|\n",
      "|     31|[10, 3.3996582]|\n",
      "+-------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shows top K recommendations for each user or movie\n",
    "alsModel.recommendForAllUsers(10).selectExpr(\"userId\", \"explode(recommendations)\").show(3)\n",
    "alsModel.recommendForAllItems(10).selectExpr(\"movieId\", \"explode(recommendations)\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Evaluation Metrics (Regression & Ranking) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.682634\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator()\\\n",
    ".setMetricName(\"rmse\")\\\n",
    ".setLabelCol(\"rating\")\\\n",
    ".setPredictionCol(\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = %f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regComparison = predictions.select(\"rating\", \"prediction\")\\\n",
    ".rdd.map(lambda x: (x(0), x(1)))\n",
    "metrics = RegressionMetrics(regComparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics, RegressionMetrics\n",
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2548396048396048\n",
      "0.49999999999999994\n"
     ]
    }
   ],
   "source": [
    "perUserActual = predictions\\\n",
    ".where(\"rating > 2.5\")\\\n",
    ".groupBy(\"userId\")\\\n",
    ".agg(expr(\"collect_set(movieId) as movies\"))\n",
    "\n",
    "perUserPredictions = predictions\\\n",
    ".orderBy(col(\"userId\"), expr(\"prediction DESC\"))\\\n",
    ".groupBy(\"userId\")\\\n",
    ".agg(expr(\"collect_list(movieId) as movies\"))\n",
    "\n",
    "perUserActualvPred = perUserActual.join(perUserPredictions, [\"userId\"]).rdd.map(lambda row: (row[1], row[2][:15]))\n",
    "ranks = RankingMetrics(perUserActualvPred)\n",
    "\n",
    "print(ranks.meanAveragePrecision)\n",
    "print(ranks.precisionAt(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Pattern Mining:\n",
    "-  aka Market Basket Analysis\n",
    "-  finds association rules from raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
