{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: The Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Low-Level APIs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retailAll = '/Users/grp/sparkTheDefinitiveGuide/data/retail-data/all/'\n",
    "flightData2010 = '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/parquet/2010-summary.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## _Chapter #12 - Resilient Distributed Datasets (RDDs)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **Spak operates on a per-partition basis when executing code**\n",
    "-  Basically all DF Spark code compiles down to an RDD\n",
    "-  When calling a DF transformation the underlying logic becomes a set of RDD transformations\n",
    "-  SparkContext is the entry point for low-level API functionality accessed through SparkSession\n",
    "-  Main reason to use RDDs is for fine grained control over physical distribution of data (**custom partitioning of data**)\n",
    "-  Spark's Structured APIs automatically store data in an optimized compressioned binary format, unlike RDDs, which will require manual object implementation to acheive this same functionality and performance optimization\n",
    "-  _RDD performance is best via Scala/Java ... Python RDDs just like PySpark UDFs require serializing the data to the Python process then perform the Python operation code then serialize it back to the JVM_\n",
    "-  DF vs RDD maipulation: **RDDs manipulate raw objects whereas DFs/DSs manipulate Spark Types**   \n",
    "<br>\n",
    "-  RDDs:\n",
    "    -  **immutable, partitioned collection of records operated on in parallel as (Python, Scala, Java) objects**\n",
    "    -  \"Rows\" do not exist in RDDs ... data records are just raw (Python, Scala, Java) objects   \n",
    "    <br>\n",
    "    -  Types:\n",
    "        -  \"generic\" RDD\n",
    "        -  key-value RDD\n",
    "    -  Properties:\n",
    "        -  list of partitions\n",
    "        -  function for computing each split\n",
    "        -  list of dependencies on other RDDs\n",
    "        -  optional partitioner for key-value RDDs\n",
    "        -  optional preferred locations on which to compute each split (ex: block locations for HDFS)\n",
    "    -  Saving Files:\n",
    "        -  RDDs can be written out to plain-text files\n",
    "        -  RDDs can be written out to sequence files (flat file with binary key-value pairs) if in key-value format \n",
    "        -  Spark takes each partition and writes out to target destination\n",
    "    -  Caching:\n",
    "        -  ability to cache or persists an RDD\n",
    "        -  ability to specify a storage level [org.apache.spark.storage.StorageLevel] (combinations of memory only, disk only, and off heap)\n",
    "    -  Checkpointing:\n",
    "        -  saves RDD to risk so future computations on that RDD point to its partitions on disk rather than recomputing the RDD from the original source\n",
    "        -  similar to caching except checkpointing is stored only on disk and not in memory (like cache)\n",
    "        -  when checkpointed RDD is referenced it derives from checkpoint instead of source data, which helps improve performance and optimization\n",
    "    -  Finding Partitions:\n",
    "        -  pipe helps with finding the # of lines per partition in RDD\n",
    "        -  mapPartitions helps with finding the # of partitions in RDD\n",
    "        -  mapPartitionsWithIndex helps with finding where each record in the RDD is mapped to what RDD partition\n",
    "        -  foreachPartition helps with iterating over all the partitions of the data\n",
    "        -  glom takes every partition in RDD and converts them to arrays **be CAREFUL because collecting large partitions can crash driver**   \n",
    "        <br>\n",
    "-  Shared Variables:\n",
    "    -  broadcast variables\n",
    "    -  accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #12 Exercises (RDDs)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD to DF Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=0)\n",
      "Row(id=1)\n",
      "Row(id=2)\n"
     ]
    }
   ],
   "source": [
    "for i in spark.range(3).rdd.collect(): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in spark.range(3).toDF(\"id\").rdd.map(lambda row: row[0]).collect(): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(3).rdd.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Local Collection to RDD Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2) # sets number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "The\n",
      "Definitive\n",
      "Guide\n",
      ":\n",
      "Big\n",
      "Data\n",
      "Processing\n",
      "Made\n",
      "Simple\n"
     ]
    }
   ],
   "source": [
    "words.setName(\"myWords\") # names app for Spark UI\n",
    "words.name()\n",
    "for i in words.collect(): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD Data Source Read Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspark.sparkContext.textFile(...) # reads text file line by line\\nspark.sparkContext.wholeTextFiles(...) # reads key-value (fileName, textFileValue)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "spark.sparkContext.textFile(...) # reads text file line by line\n",
    "spark.sparkContext.wholeTextFiles(...) # reads key-value (fileName, textFileValue)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD Transformation Examples_:\n",
    "-  distinct [removes duplicates from RDD]\n",
    "-  filter [where clause]\n",
    "-  map [returns value based on input]\n",
    "-  flatMap [returns mulitple values (splits) based on input]\n",
    "-  sort [sort by variable]\n",
    "-  randomSplit [randomly splits an RDD into an Array of RDDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['Spark', 'Simple']\n",
      "[('Spark', 'S', True), ('Simple', 'S', True)]\n",
      "['S', 'p', 'a', 'r', 'k', 'T', 'h', 'e', 'D', 'e', 'f', 'i', 'n', 'i', 't', 'i', 'v', 'e', 'G', 'u', 'i', 'd', 'e', ':', 'B', 'i', 'g', 'D', 'a', 't', 'a', 'P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', 'M', 'a', 'd', 'e', 'S', 'i', 'm', 'p', 'l', 'e']\n",
      "['Definitive', 'Processing', 'Simple', 'Spark', 'Guide', 'Data', 'Made', 'The', 'Big', ':']\n"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "print(words.distinct().count())\n",
    "\n",
    "# filter\n",
    "def startsWithS(individual):\n",
    "  return individual.startswith(\"S\")\n",
    "\n",
    "print(words.filter(lambda word: startsWithS(word)).collect())\n",
    "\n",
    "# map\n",
    "words2 = words.map(lambda word: (word, word[0], word.startswith(\"S\")))\n",
    "print(words2.filter(lambda record: record[2]).collect())\n",
    "\n",
    "# flatMap\n",
    "print(words.flatMap(lambda word: list(word)).collect())\n",
    "\n",
    "# sort\n",
    "print(words.sortBy(lambda word: len(word) * -1).collect())\n",
    "\n",
    "# randomSplit\n",
    "fiftyFiftySplit = words.randomSplit([0.5, 0.5]) # [weight, random seed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD Actions Examples_:\n",
    "-  reduce [reduce \"aggregate\" values]\n",
    "-  count [count records]\n",
    "-  countApprox [approximation of count based off confidence threshold]\n",
    "-  countApproxDistinct [approximation of count based off relative accuracy threshold]\n",
    "-  countByValue [counts # of values in RDD; returns results to memory of driver so be CAREFUL executing]\n",
    "-  countByValueApprox [same as countByValue except as an approximation]\n",
    "-  first [returns first value in RDD]\n",
    "-  min [min value]\n",
    "-  max [max value]\n",
    "-  take [takes number of values from RDD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "Processing\n",
      "10\n",
      "10\n",
      "10\n",
      "defaultdict(<class 'int'>, {'Spark': 1, 'The': 1, 'Definitive': 1, 'Guide': 1, ':': 1, 'Big': 1, 'Data': 1, 'Processing': 1, 'Made': 1, 'Simple': 1})\n",
      "10\n",
      "Spark\n",
      "1\n",
      "19\n",
      "['Spark', 'The', 'Definitive']\n",
      "[':', 'Big', 'Data']\n",
      "['The', 'Spark', 'Simple']\n",
      "['Data', 'Definitive', 'Data', 'The', 'Definitive', 'Spark']\n"
     ]
    }
   ],
   "source": [
    "# reduce\n",
    "print(spark.sparkContext.parallelize(range(1, 21)).reduce(lambda x, y: x + y))\n",
    "\n",
    "def wordLengthReducer(leftWord, rightWord):\n",
    "  if len(leftWord) > len(rightWord):\n",
    "    return leftWord\n",
    "  else:\n",
    "    return rightWord\n",
    "\n",
    "print(words.reduce(wordLengthReducer))\n",
    "\n",
    "# count\n",
    "print(words.count())\n",
    "\n",
    "# countApprox\n",
    "confidence = 0.95\n",
    "timeoutMilliseconds = 400\n",
    "print(words.countApprox(timeoutMilliseconds, confidence))\n",
    "\n",
    "# countApproxDistinct\n",
    "print(words.countApproxDistinct(0.05))\n",
    "#print(words.countApproxDistinct(4, 10)) # inputs: precision, sparse precision\n",
    "\n",
    "# countByValue\n",
    "print(words.countByValue())\n",
    "\n",
    "# countByValueApprox\n",
    "print(words.countApproxDistinct(0.05))\n",
    "\n",
    "# first\n",
    "print(words.first())\n",
    "\n",
    "# min\n",
    "print(spark.sparkContext.parallelize(range(1, 20)).min())\n",
    "\n",
    "# max\n",
    "print(spark.sparkContext.parallelize(range(1, 20)).max())\n",
    "\n",
    "# take\n",
    "print(words.take(3)) # returns values\n",
    "print(words.takeOrdered(3)) # asc order\n",
    "print(words.top(3)) # top values based on implicit order\n",
    "withReplacement = True\n",
    "numberToTake = 6\n",
    "randomSeed = 100\n",
    "print(words.takeSample(withReplacement, numberToTake, randomSeed)) # random sample from RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD TXT Save (Uncompressed & Compressed) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"/Users/grp/sparkTheDefinitiveGuide/tmp/words\")\n",
    "\n",
    "words.saveAsTextFile(\"/Users/grp/sparkTheDefinitiveGuide/tmp/words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "._SUCCESS.crc\n",
      ".part-00000.crc\n",
      ".part-00001.crc\n",
      "_SUCCESS\n",
      "part-00001\n",
      "part-00000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in os.listdir(\"/Users/grp/sparkTheDefinitiveGuide/tmp/words\"): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "The\n",
      "Definitive\n",
      "Guide\n",
      ":\n",
      "\n",
      "\n",
      "Big\n",
      "Data\n",
      "Processing\n",
      "Made\n",
      "Simple\n"
     ]
    }
   ],
   "source": [
    "!head /Users/grp/sparkTheDefinitiveGuide/tmp/words/part-00000\n",
    "print(\"\\n\")\n",
    "!head /Users/grp/sparkTheDefinitiveGuide/tmp/words/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"/Users/grp/sparkTheDefinitiveGuide/tmp/wordsCompressed\")\n",
    "\n",
    "words.saveAsTextFile(\"/Users/grp/sparkTheDefinitiveGuide/tmp/wordsCompressed\", \\\n",
    "                     compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "._SUCCESS.crc\n",
      ".part-00001.gz.crc\n",
      "_SUCCESS\n",
      "part-00000.gz\n",
      "part-00001.gz\n",
      ".part-00000.gz.crc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in os.listdir(\"/Users/grp/sparkTheDefinitiveGuide/tmp/wordsCompressed/\"): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD Caching Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myWords ParallelCollectionRDD[26] at parallelize at PythonRDD.scala:194"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD Checkpointing Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"/Users/grp/sparkTheDefinitiveGuide/tmp/checkpointRDD\")\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD Partitions Examples_:\n",
    "-  pipe\n",
    "-  mapPartitions\n",
    "-  mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['       5', '       5']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.pipe(\"wc -l\").collect() # five lines per partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.mapPartitions(lambda part: [1]).sum() # value '1' for every partition in RDD then sum to get total # of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partition: 0 => Spark',\n",
       " 'partition: 0 => The',\n",
       " 'partition: 0 => Definitive',\n",
       " 'partition: 0 => Guide',\n",
       " 'partition: 0 => :',\n",
       " 'partition: 1 => Big',\n",
       " 'partition: 1 => Data',\n",
       " 'partition: 1 => Processing',\n",
       " 'partition: 1 => Made',\n",
       " 'partition: 1 => Simple']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def indexedFunc(partitionIndex, withinPartIterator):\n",
    "  return [\"partition: {} => {}\".format(partitionIndex, x) for x in withinPartIterator]\n",
    "\n",
    "words.mapPartitionsWithIndex(indexedFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello'], ['World']]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.parallelize([\"Hello\", \"World\"], 2).glom().collect())\n",
    "print(type(spark.sparkContext.parallelize([\"Hello\", \"World\"], 2).glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #13 - Advanced RDDs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  RDD Key-Value Pairs:\n",
    "    -  can only perform on **PairRDD** type (ex: \"some-operation\" ByKey)\n",
    "    -  holds 2 values in each record of RDD (ex: tuple -> (key, value))   \n",
    "    <br>\n",
    "-  RDD Aggregation   \n",
    "<br>\n",
    "-  RDD Joins   \n",
    "<br>\n",
    "-  RDD Partitioning:\n",
    "    -  Controlling Partitions:\n",
    "        -  with RDDs one has control over how data is exactly physically distributed across the cluster\n",
    "        -  coalesce:\n",
    "            -  collapses partitions on the same worker node in order to avoid a shuffle of the data when repartitioning\n",
    "        -  repartition:\n",
    "            -  repartition data up or down but performs a shuffle across nodes\n",
    "            -  increasing # of partitions helps with increasing level of parallelism\n",
    "        -  repartitionAndSortWithinPartitions:\n",
    "            -  repartition as well as specify ordering of each output partition\n",
    "    -  Custom Partitioning:\n",
    "        -  controls layout of the data on the cluster to avoid shuffles\n",
    "        -  main goal is to even out data skews / the distribution of your data across the cluster\n",
    "        -  used to help avoid **key skews** which means some keys have way more values than other keys\n",
    "        -  HashPartitioner and RangePartitioner   \n",
    "        <br>\n",
    "-  RDD Serialization:\n",
    "    -  objects looking to be parallelized must be serializable\n",
    "    -  Kryo [configure \"spark.serializer\" to \"org.apache.spark.serializer.KryoSerializer\"] is faster compared to default Java serializer\n",
    "    -  \"spark.serializer\" parameter is used for shuffling data between worker nodes and serializing RDDs to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #13 Exercises (RDDs)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Key-Value RDD Examples_:\n",
    "-  keyBy [creates a key from value]\n",
    "-  Mapping over Values [maps over the values by key]\n",
    "-  Extract Keys and Values [collects individual keys and individual values]\n",
    "-  lookup [lookup value(s) for particular key]\n",
    "-  sampleByKey [pulls sample of data by key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark', 1), ('the', 1), ('definitive', 1), ('guide', 1), (':', 1), ('big', 1), ('data', 1), ('processing', 1), ('made', 1), ('simple', 1)]\n",
      "[('s', 'Spark'), ('t', 'The'), ('d', 'Definitive'), ('g', 'Guide'), (':', ':'), ('b', 'Big'), ('d', 'Data'), ('p', 'Processing'), ('m', 'Made'), ('s', 'Simple')]\n",
      "\n",
      "\n",
      "[('s', 'SPARK'), ('t', 'THE'), ('d', 'DEFINITIVE'), ('g', 'GUIDE'), (':', ':'), ('b', 'BIG'), ('d', 'DATA'), ('p', 'PROCESSING'), ('m', 'MADE'), ('s', 'SIMPLE')]\n",
      "[('s', 'S'), ('s', 'P'), ('s', 'A'), ('s', 'R'), ('s', 'K'), ('t', 'T'), ('t', 'H'), ('t', 'E'), ('d', 'D'), ('d', 'E'), ('d', 'F'), ('d', 'I'), ('d', 'N'), ('d', 'I'), ('d', 'T'), ('d', 'I'), ('d', 'V'), ('d', 'E'), ('g', 'G'), ('g', 'U'), ('g', 'I'), ('g', 'D'), ('g', 'E'), (':', ':'), ('b', 'B'), ('b', 'I'), ('b', 'G'), ('d', 'D'), ('d', 'A'), ('d', 'T'), ('d', 'A'), ('p', 'P'), ('p', 'R'), ('p', 'O'), ('p', 'C'), ('p', 'E'), ('p', 'S'), ('p', 'S'), ('p', 'I'), ('p', 'N'), ('p', 'G'), ('m', 'M'), ('m', 'A'), ('m', 'D'), ('m', 'E'), ('s', 'S'), ('s', 'I'), ('s', 'M'), ('s', 'P'), ('s', 'L'), ('s', 'E')]\n",
      "\n",
      "\n",
      "['s', 't', 'd', 'g', ':', 'b', 'd', 'p', 'm', 's']\n",
      "['Spark', 'The', 'Definitive', 'Guide', ':', 'Big', 'Data', 'Processing', 'Made', 'Simple']\n",
      "\n",
      "\n",
      "['Spark', 'Simple']\n",
      "\n",
      "\n",
      "[('t', 'The'), (':', ':'), ('m', 'Made'), ('s', 'Simple')]\n"
     ]
    }
   ],
   "source": [
    "# keyBy\n",
    "print(words.map(lambda word: (word.lower(), 1)).collect())\n",
    "keyword = words.keyBy(lambda word: word.lower()[0])\n",
    "print(keyword.collect())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Mapping over Values\n",
    "print(keyword.mapValues(lambda word: word.upper()).collect())\n",
    "print(keyword.flatMapValues(lambda word: word.upper()).collect())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Extract Keys and Values\n",
    "print(keyword.keys().collect())\n",
    "print(keyword.values().collect())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# lookup\n",
    "print(keyword.lookup(\"s\"))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# sampleByKey\n",
    "import random\n",
    "distinctChars = words.flatMap(lambda word: list(word.lower())).distinct().collect()\n",
    "sampleMap = dict(map(lambda c: (c, random.random()), distinctChars))\n",
    "print(words.map(lambda word: (word.lower()[0], word)).sampleByKey(True, sampleMap, 6).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Aggregation RDD Examples_:\n",
    "-  countByKey:\n",
    "    -  counts items (values) per each key\n",
    "-  groupByKey:\n",
    "    -  when called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs\n",
    "    -  each executor must hold all values for key in memory before applying function\n",
    "    -  this can be a problem if partitions hold tons of values and could result in executor(s) OOM\n",
    "-  reduceByKey:\n",
    "    -  when called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V\n",
    "    -  happens within each partition and doesn't need to put anything in memory\n",
    "    -  each worker performs individual tasks before performing final reduce\n",
    "-  aggregate:\n",
    "    - requires start value, and 2 functions (aggregates within partitions, aggregates across partitions)\n",
    "    -  performs final aggregation on driver thus if executors are too large the driver could face OOM\n",
    "-  treeAggregate:\n",
    "    -  helps to improve performance by performing subaggregations within executor to take away strain on driver\n",
    "-  aggregateByKey:\n",
    "    -  aggregates by key instead of partition by partition like \"aggregate\" function\n",
    "-  combineByKey:\n",
    "    -  operates on given key and merges values based on a custom function\n",
    "-  foldByKey:\n",
    "    -  merges values for each key via custom function as well as binary value [0=addition, 1=multiplication]\n",
    "-  cogroup:\n",
    "    -  joins values by keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = words.flatMap(lambda word: word.lower())\n",
    "KVcharacters = chars.map(lambda letter: (letter, 1))\n",
    "def maxFunc(left, right):\n",
    "  return max(left, right)\n",
    "def addFunc(left, right):\n",
    "  return left + right\n",
    "nums = sc.parallelize(range(1,31), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'s': 4, 'p': 3, 'a': 4, 'r': 2, 'k': 1, 't': 3, 'h': 1, 'e': 7, 'd': 4, 'f': 1, 'i': 7, 'n': 2, 'v': 1, 'g': 3, 'u': 1, ':': 1, 'b': 1, 'o': 1, 'c': 1, 'm': 2, 'l': 1})\n",
      "\n",
      "\n",
      "[('s', 4), ('p', 3), ('r', 2), ('h', 1), ('d', 4), ('i', 7), ('g', 3), ('b', 1), ('c', 1), ('l', 1), ('a', 4), ('k', 1), ('t', 3), ('e', 7), ('f', 1), ('n', 2), ('v', 1), ('u', 1), (':', 1), ('o', 1), ('m', 2)]\n",
      "[('s', 4), ('p', 3), ('r', 2), ('h', 1), ('d', 4), ('i', 7), ('g', 3), ('b', 1), ('c', 1), ('l', 1), ('a', 4), ('k', 1), ('t', 3), ('e', 7), ('f', 1), ('n', 2), ('v', 1), ('u', 1), (':', 1), ('o', 1), ('m', 2)]\n",
      "\n",
      "\n",
      "90\n",
      "90\n",
      "\n",
      "\n",
      "[('s', 3), ('p', 2), ('r', 1), ('h', 1), ('d', 2), ('i', 4), ('g', 2), ('b', 1), ('c', 1), ('l', 1), ('a', 3), ('k', 1), ('t', 2), ('e', 4), ('f', 1), ('n', 1), ('v', 1), ('u', 1), (':', 1), ('o', 1), ('m', 2)]\n",
      "\n",
      "\n",
      "[('s', [1, 1, 1, 1]), ('d', [1, 1, 1, 1]), ('l', [1]), ('v', [1]), (':', [1]), ('p', [1, 1, 1]), ('r', [1, 1]), ('c', [1]), ('k', [1]), ('t', [1, 1, 1]), ('n', [1, 1]), ('u', [1]), ('o', [1]), ('h', [1]), ('i', [1, 1, 1, 1, 1, 1, 1]), ('g', [1, 1, 1]), ('b', [1]), ('a', [1, 1, 1, 1]), ('e', [1, 1, 1, 1, 1, 1, 1]), ('f', [1]), ('m', [1, 1])]\n",
      "\n",
      "\n",
      "[('s', 4), ('p', 3), ('r', 2), ('h', 1), ('d', 4), ('i', 7), ('g', 3), ('b', 1), ('c', 1), ('l', 1), ('a', 4), ('k', 1), ('t', 3), ('e', 7), ('f', 1), ('n', 2), ('v', 1), ('u', 1), (':', 1), ('o', 1), ('m', 2)]\n",
      "\n",
      "\n",
      "('s', (<pyspark.resultiterable.ResultIterable object at 0x10ecab8d0>, <pyspark.resultiterable.ResultIterable object at 0x10ecab400>))\n",
      "('p', (<pyspark.resultiterable.ResultIterable object at 0x10ecab7b8>, <pyspark.resultiterable.ResultIterable object at 0x10ecaba20>))\n",
      "('r', (<pyspark.resultiterable.ResultIterable object at 0x10ecab160>, <pyspark.resultiterable.ResultIterable object at 0x10ecabb38>))\n"
     ]
    }
   ],
   "source": [
    "# countByKey\n",
    "print(KVcharacters.countByKey())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# groupByKey\n",
    "print(KVcharacters.groupByKey().map(lambda row: (row[0], reduce(addFunc, row[1]))).collect())\n",
    "\n",
    "# reduceByKey\n",
    "print(KVcharacters.reduceByKey(addFunc).collect())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# aggregate\n",
    "print(nums.aggregate(0, maxFunc, addFunc))\n",
    "\n",
    "# treeAggregate\n",
    "depth = 3\n",
    "print(nums.treeAggregate(0, maxFunc, addFunc, depth))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# aggregateByKey\n",
    "print(KVcharacters. aggregateByKey(0, addFunc, maxFunc).collect())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# combineByKey\n",
    "def valToCombiner(value):\n",
    "  return [value]\n",
    "def mergeValuesFunc(vals, valToAppend):\n",
    "  vals.append(valToAppend)\n",
    "  return vals\n",
    "def mergeCombinerFunc(vals1, vals2):\n",
    "  return vals1 + vals2\n",
    "outputPartitions = 6\n",
    "print(KVcharacters.combineByKey(valToCombiner, mergeValuesFunc, mergeCombinerFunc, outputPartitions).collect())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# foldByKey\n",
    "print(KVcharacters.foldByKey(0, addFunc).collect())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# cogroup\n",
    "import random\n",
    "distinctChars = words.flatMap(lambda word: word.lower()).distinct()\n",
    "charRDD = distinctChars.map(lambda c: (c, random.random()))\n",
    "charRDD2 = distinctChars.map(lambda c: (c, random.random()))\n",
    "for i in charRDD.cogroup(charRDD2).take(3): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Join RDD Example_:\n",
    "-  join\n",
    "-  fullOuterJoin\n",
    "-  leftOuterJoin\n",
    "-  rightOuterJoin\n",
    "-  cartesian\n",
    "-  zip [zips together 2 RDDs with same length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyedChars = distinctChars.map(lambda c: (c, random.random()))\n",
    "outputPartitions = 10\n",
    "KVcharacters.join(keyedChars).count()\n",
    "KVcharacters.join(keyedChars, outputPartitions).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 0),\n",
       " ('The', 1),\n",
       " ('Definitive', 2),\n",
       " ('Guide', 3),\n",
       " (':', 4),\n",
       " ('Big', 5),\n",
       " ('Data', 6),\n",
       " ('Processing', 7),\n",
       " ('Made', 8),\n",
       " ('Simple', 9)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRange = sc.parallelize(range(10), 2)\n",
    "words.zip(numRange).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Controlling Partitions Examples_:\n",
    "-  coalesce\n",
    "-  repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# collapses 2 partition RDD to 1 partition RDD to avoid shuffle operation\n",
    "print(words.coalesce(1).getNumPartitions())\n",
    "# repartitions 2 partition RDD to 10 partition RDD for increased parallelism but performs shuffle\n",
    "print(words.repartition(10).getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Custom Partition Example_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(retailAll)\n",
    "rdd = df.coalesce(10).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4302, 4296]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def partitionFunc(key):\n",
    "  import random\n",
    "  if key == 17850 or key == 12583:\n",
    "    return 0\n",
    "  else:\n",
    "    return random.randint(1,2)\n",
    "\n",
    "keyedRDD = rdd.keyBy(lambda row: row[6]) # 6th element [customerID]\n",
    "keyedRDD\\\n",
    "  .partitionBy(3, partitionFunc)\\\n",
    "  .map(lambda x: x[0])\\\n",
    "  .glom()\\\n",
    "  .map(lambda x: len(set(x)))\\\n",
    "  .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #14 - Distributed Shared Variables_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Broadcast Variables:\n",
    "    -  saves large value on all worker nodes without re-sending to cluster every time (ex: lookup table as function that fits in memory on each executor)\n",
    "    -  avoids deserialization per task on the worker nodes every time variable is used\n",
    "    -  shared immutable variables that are cached on every machine in cluster instead of serialized with every single task\n",
    "    -  the cost of serializing data for every task can be quite expensive thus broadcast variables are a good alternative   \n",
    "<br>\n",
    "-  Accumulators:\n",
    "    -  adds data together from all tasks into a shared result (ex: error logging counter and debugging)\n",
    "    -  mutable variable that updates value via transformations and sends value to driver node in an efficient manner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #14 Exercises (Distributed Shared Variables)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_collection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(my_collection, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Broadcast Example_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supplementalData = {\"Spark\":1000, \"Definitive\":200,\n",
    "                    \"Big\":-300, \"Simple\":100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suppBroadcast = spark.sparkContext.broadcast(supplementalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Big': -300, 'Definitive': 200, 'Simple': 100, 'Spark': 1000}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Big', -300),\n",
       " ('The', 0),\n",
       " ('Guide', 0),\n",
       " (':', 0),\n",
       " ('Data', 0),\n",
       " ('Processing', 0),\n",
       " ('Made', 0),\n",
       " ('Simple', 100),\n",
       " ('Definitive', 200),\n",
       " ('Spark', 1000)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: (word, suppBroadcast.value.get(word, 0))).sortBy(lambda wordPair: wordPair[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Accumulator Example_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flights = spark.read.parquet(flightData2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count number of flights to or from China\n",
    "accChina = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def accChinaFunc(flight_row):\n",
    "  destination = flight_row[\"DEST_COUNTRY_NAME\"]\n",
    "  origin = flight_row[\"ORIGIN_COUNTRY_NAME\"]\n",
    "  if destination == \"China\":\n",
    "    accChina.add(flight_row[\"count\"])\n",
    "  if origin == \"China\":\n",
    "    accChina.add(flight_row[\"count\"])\n",
    "\n",
    "# runs foreach row in the input DF (flights) and runs function against each row\n",
    "flights.foreach(lambda flight_row: accChinaFunc(flight_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accChina.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
