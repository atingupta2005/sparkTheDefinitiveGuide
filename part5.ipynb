{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: The Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5: Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activityDataSample = '/Users/grp/sparkTheDefinitiveGuide/data/activity-data-sample/'\n",
    "checkpointPath = '/Users/grp/sparkTheDefinitiveGuide/checkpoint/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #20 - Stream Processing Fundamentals_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  **Streaming Processing**:\n",
    "    -  continuously incorporating new data to compute results\n",
    "    -  input data in unbounded (no beginning or end)\n",
    "    -  Use Cases (alerts, real-time transaction reporting, incremental ETL, online ML)\n",
    "    -  Challenges (input records might arrive out of order [timestamps], handling machine failures, joining with other data) sources   \n",
    "    <br>\n",
    "    -  **Event-Time vs Processing Time**:\n",
    "        -  ET (process data based on timestamp records within source data)\n",
    "        -  PT (process data based on when streaming application receives records)   \n",
    "        <br>\n",
    "    -  **Continuous Processing vs Micro-Batch Execution**:\n",
    "        -  CP (read records one by one from input source for low latency benefits)\n",
    "        -  MBE (read records in small batches from input source for higher throughput benefits but higher latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #21 - Structured Streaming Basics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  SS engine computes a switch for running queries incrementally and continuously as new data arrives into the system\n",
    "-  SS ensures data processed once as well as fault-tolerance through checkpointing and write-ahead logs\n",
    "-  SS treats a stream as a \"table\" that is continuously appended and periodically checks for new data flowing through active streams to update new results\n",
    "-  SS supports Spark Transformations and Actions (starting a stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **Structured Streaming Features**:   \n",
    "<br>\n",
    "    -  **Input Sources (streams source entry point)**:\n",
    "         -  Kafka\n",
    "         -  distributed file system (HDFS, S3)\n",
    "         -  socket for testing purposes  \n",
    "         <br>\n",
    "    -  **Sinks (streams target result destination)**:\n",
    "         -  Kafka\n",
    "         -  many file formats\n",
    "         -  foreach sink for running arbitary computation on the output records\n",
    "         -  console sink for testing\n",
    "         -  memory sink for debugging   \n",
    "         <br>\n",
    "    -  **Output Modes (how to write data to sink)**:\n",
    "         -  Append (only add new records to output sink based on trigger; does not support aggregations because of logic)\n",
    "         -  Update (update changed records in place [rows different from previous write are written out to sink])\n",
    "         -  Complete (rewrite full output to result table [useful for data where all rows are expected to change])   \n",
    "         <br>\n",
    "    -  **Triggers**:\n",
    "        - frequency when data is outputed to sink\n",
    "        -  ex: trigger duration of 1 min will fire at 12:00, 12:01, 12:02, etc.\n",
    "        -  Spark will wait until next trigger if trigger time is missed\n",
    "        -  \"Once Trigger\" can be used to run streaming job manually to import new data occasionally   \n",
    "        <br>\n",
    "    -  **Event-Time Processing**:\n",
    "        -  processes data based on timestamp column in source DF   \n",
    "        <br>\n",
    "    -  **Watermarks**:\n",
    "        -  feature specifying how late (delays) stream expects to see data in event time\n",
    "        -  limit how long stream needs to remember old data\n",
    "        -  used frequently with event time windows (waiting until the watermark for window has passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources and Sinks:\n",
    "-  _File Source & Sink_   \n",
    "<br>\n",
    "__Disclaimer: if new files are manually added into a streaming job's input directory Spark will process partially written files before the files have finished writing hence the new files should be written to an external directory then moved to the stream's source directory when the stream in inactive__  \n",
    "<br>\n",
    "-  _Kafka Source & Sink_  \n",
    "    -  acts as a distributed buffer\n",
    "    -  stores streams of records in categories called _topics_\n",
    "    -  each record consists of a key, a value, and a timestamp\n",
    "    -  reading data is called subscribing to a topic and writing data is called publishing to a topic   \n",
    "    <br>\n",
    "-  _Socket Source_\n",
    "    -  ability to send data to streams via TCP sockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n// reading from Kafka\\n\\n# subscribe to 1 topic\\ndf1 = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\").option(\"subscribe\", \"topic1\").load()\\n\\n# subscribe to multiple topics\\ndf1 = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\").option(\"subscribe\", \"topic1,topic2\").load()\\n\\n# subscribe to a pattern\\ndf1 = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\").option(\"subscribePattern\", \"topic.*\").load()\\n\\n// writing to Kafka\\n\\ndf1.selectExpr(\"topic\", \"CAST(key as STRING)\", \"CAST(value as STRING)\").writeStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\").option(\"checkpointLocation\", \"...\").start()\\n\\ndf1.selectExpr(\"CAST(key as STRING)\", \"CAST(value as STRING)\").writeStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\").option(\"checkpointLocation\", \"...\").option(\"topic\", \"topic1\").start()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "// reading from Kafka\n",
    "\n",
    "# subscribe to 1 topic\n",
    "df1 = spark.readStream.format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    ".option(\"subscribe\", \"topic1\")\\\n",
    ".load()\n",
    "\n",
    "# subscribe to multiple topics\n",
    "df1 = spark.readStream.format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    ".option(\"subscribe\", \"topic1,topic2\")\\\n",
    ".load()\n",
    "\n",
    "# subscribe to a pattern\n",
    "df1 = spark.readStream.format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    ".option(\"subscribePattern\", \"topic.*\")\\\n",
    ".load()\n",
    "\n",
    "// writing to Kafka\n",
    "\n",
    "df1.selectExpr(\"topic\", \"CAST(key as STRING)\", \"CAST(value as STRING)\")\\\n",
    ".writeStream\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    ".option(\"checkpointLocation\", \"...\")\\\n",
    ".start()\n",
    "\n",
    "df1.selectExpr(\"CAST(key as STRING)\", \"CAST(value as STRING)\")\\\n",
    ".writeStream\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    ".option(\"checkpointLocation\", \"...\")\\\n",
    ".option(\"topic\", \"topic1\")\\\n",
    ".start()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n// reading from socket\\n\\nsocketDF = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\\n\\n// connect to socket\\n\\nnc -lk 9999\\n\\n// console sink testing\\n\\ndf.format(\"console\").write()\\n\\n// memory sink testing\\n\\ndf.writeStream.format(\"memory\").queryName(\"test_table\")\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "// reading from socket\n",
    "\n",
    "socketDF = spark.readStream.format(\"socket\")\\\n",
    ".option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "// connect to socket\n",
    "\n",
    "nc -lk 9999\n",
    "\n",
    "// console sink testing\n",
    "\n",
    "df.format(\"console\").write()\n",
    "\n",
    "// memory sink testing\n",
    "\n",
    "df.writeStream.format(\"memory\").queryName(\"test_table\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #21 Exercises (Structured Streaming)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "static = spark.read.json(activityDataSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+-------------+-------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|            y|            z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+-------------+-------------+\n",
      "|1424686734968|1424688581023530396|nexus4_2|    2|nexus4|   g|stand| 6.866455E-4|  0.033355713|  0.030136108|\n",
      "|1424686735183|1424686733186371836|nexus4_1|   37|nexus4|   g|stand|0.0014038086|-0.0027008057|-0.0124053955|\n",
      "|1424686735388|1424688581441712769|nexus4_2|   85|nexus4|   g|stand| 6.866455E-4|  0.011993408|-0.0029754639|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+-------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    ".json(activityDataSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    ".format(\"memory\").outputMode(\"complete\")\\\n",
    ".start()\n",
    "#activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#activityQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x10dfb8550>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|49238|\n",
      "|     stand|45539|\n",
      "|stairsdown|37459|\n",
      "+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|49238|\n",
      "|     stand|45539|\n",
      "|stairsdown|37459|\n",
      "+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(3):\n",
    "    spark.sql(\"select * from activity_counts\").show(3)\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Select & Filter Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    ".where(\"stairs\")\\\n",
    ".where(\"gt is not null\")\\\n",
    ".select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    ".writeStream\\\n",
    ".queryName(\"simple_transform\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"append\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simpleTransform.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   79268|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   79268|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(3):\n",
    "    spark.sql(\"select count(*) from simple_transform\").show()\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Aggregation Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    ".drop(\"avg(Arrival_time)\")\\\n",
    ".drop(\"avg(Creation_time)\")\\\n",
    ".drop(\"avg(Index)\")\\\n",
    ".writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deviceModelStats.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+--------------------+--------------------+\n",
      "|   gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+-----+------+--------------------+--------------------+--------------------+\n",
      "|  sit|  null|-5.27535491537025...|3.678924478593777...|-1.48893910179131...|\n",
      "|stand|  null|-4.10214448224599...|3.879979054370976E-4|1.904237520345197...|\n",
      "|  sit|nexus4|-5.27535491537025...|3.678924478593777...|-1.48893910179131...|\n",
      "|stand|nexus4|-4.10214448224599...|3.879979054370976E-4|1.904237520345197...|\n",
      "| null|  null|-0.00684720545658...|3.304535732167207...|0.006290737756605493|\n",
      "| null|  null|7.216251708678295E-4|-0.00607285249718343|-0.00815160685941575|\n",
      "| walk|  null|-0.00357654462748...|0.004378921096641145|0.002121673306851616|\n",
      "| null|nexus4|-0.00684720545658...|3.304535732167207...|0.006290737756605493|\n",
      "| null|nexus4|7.216251708678295E-4|-0.00607285249718343|-0.00815160685941575|\n",
      "| bike|  null|0.022771987227411025| -0.0093323141311089|-0.08289858777656002|\n",
      "+-----+------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from device_counts\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Joins Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "historicalAgg = static.groupBy(\"gt\", \"model\").avg()\n",
    "deviceModelStats = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\\\n",
    ".cube(\"gt\", \"model\").avg()\\\n",
    ".join(historicalAgg, [\"gt\", \"model\"])\\\n",
    ".writeStream.queryName(\"device_join\").format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deviceModelStats.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(gt='sit', model=None, avg(x)=-0.0005275354915370251, avg(y)=0.00036789244785937776, avg(z)=-0.00014889391017913116)\n",
      "Row(gt='stand', model=None, avg(x)=-0.0004102144482245993, avg(y)=0.0003879979054370976, avg(z)=0.00019042375203451976)\n",
      "Row(gt='sit', model='nexus4', avg(x)=-0.0005275354915370251, avg(y)=0.00036789244785937776, avg(z)=-0.00014889391017913116)\n",
      "Row(gt='stand', model='nexus4', avg(x)=-0.0004102144482245993, avg(y)=0.0003879979054370976, avg(z)=0.00019042375203451976)\n",
      "Row(gt='null', model=None, avg(x)=-0.006847205456583953, avg(y)=0.00033045357321672076, avg(z)=0.006290737756605493)\n",
      "Row(gt=None, model=None, avg(x)=0.0007216251708678295, avg(y)=-0.00607285249718343, avg(z)=-0.00815160685941575)\n",
      "Row(gt='walk', model=None, avg(x)=-0.003576544627480005, avg(y)=0.004378921096641145, avg(z)=0.002121673306851616)\n",
      "Row(gt='null', model='nexus4', avg(x)=-0.006847205456583953, avg(y)=0.00033045357321672076, avg(z)=0.006290737756605493)\n",
      "Row(gt=None, model='nexus4', avg(x)=0.0007216251708678295, avg(y)=-0.00607285249718343, avg(z)=-0.00815160685941575)\n",
      "Row(gt='bike', model=None, avg(x)=0.022771987227411025, avg(y)=-0.0093323141311089, avg(z)=-0.08289858777656002)\n"
     ]
    }
   ],
   "source": [
    "for i in spark.sql(\"select * from device_counts\").take(10): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Trigger Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigger = activityCounts.writeStream.trigger(processingTime = '5 seconds')\\\n",
    ".queryName(\"t1\").format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triggerOnce = activityCounts.writeStream.trigger(once=True)\\\n",
    ".queryName(\"t2\").format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigger.stop()\n",
    "triggerOnce.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #22 - Event-Time and Stateful Processing_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  **Event-Time** (analyze information with respect to the time that it was created, NOT the time it was processed):   \n",
    "<br>\n",
    "__Summary: the order of the series of events in the processing system does not guarantee an ordering in event time hence it is imperative in certain use cases to operate on event time (actual real timestamp on data) rather than the timestamp when the data arrives and is processed in the system__  \n",
    "<br>    \n",
    "-  **Stateful Processing**:   \n",
    "    - used to capture intermediate information in a \"state store\"\n",
    "    - gets implemented in a fault-tolerant in-memory state store for intermediate state to the checkpoint directory   \n",
    "    <br>\n",
    "-  **Event-Time Watermarks**:\n",
    "    - amount of time following a given event or set of events after which we do not expect to see any more data\n",
    "    - used to age-out data in a stream to avoid overwhelming system over a long period of time\n",
    "    - ex: 10 min watermark means that any event that occurs more than 10 \"event-time\" min past previous event should be ignored\n",
    "    - if watermark is not specified Spark will maintain that data in memory forever\n",
    "    - allows Spark to free pending objects from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #22 Exercises (Structured Streaming)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _Event-Time Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static = spark.read.json(activityDataSample)\n",
    "timestampLatency = static\\\n",
    ".selectExpr(\"*\", \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")\\\n",
    ".orderBy(\"event_time\")\n",
    "streaming = spark\\\n",
    ".readStream\\\n",
    ".schema(static.schema)\\\n",
    ".option(\"maxFilesPerTrigger\", 10)\\\n",
    ".json(activityDataSample)\n",
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|event_time                |\n",
      "+--------------------------+\n",
      "|2015-02-21 18:41:40.120513|\n",
      "|2015-02-23 04:18:53.176179|\n",
      "|2015-02-23 04:18:53.181336|\n",
      "|2015-02-23 04:18:53.186371|\n",
      "|2015-02-23 04:18:53.382813|\n",
      "|2015-02-23 04:18:53.579072|\n",
      "|2015-02-23 04:18:53.584107|\n",
      "|2015-02-23 04:18:53.589417|\n",
      "|2015-02-23 04:18:53.785493|\n",
      "|2015-02-23 04:18:53.986909|\n",
      "+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestampLatency.select(\"event_time\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Window Event-Time Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "withEventTime = streaming\\\n",
    ".selectExpr(\"*\", \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")\n",
    "withEventTime = withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\")).count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevents_per_window\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#withEventTime.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from pyevents_per_window\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-----+\n",
      "|window                                       |count|\n",
      "+---------------------------------------------+-----+\n",
      "|[2015-02-23 04:40:00.0,2015-02-23 04:50:00.0]|4409 |\n",
      "|[2015-02-24 05:50:00.0,2015-02-24 06:00:00.0]|7519 |\n",
      "|[2015-02-24 07:00:00.0,2015-02-24 07:10:00.0]|6650 |\n",
      "|[2015-02-23 07:20:00.0,2015-02-23 07:30:00.0]|5343 |\n",
      "|[2015-02-21 18:40:00.0,2015-02-21 18:50:00.0]|1    |\n",
      "|[2015-02-23 06:30:00.0,2015-02-23 06:40:00.0]|5059 |\n",
      "|[2015-02-24 05:20:00.0,2015-02-24 05:30:00.0]|5697 |\n",
      "|[2015-02-23 04:20:00.0,2015-02-23 04:30:00.0]|4966 |\n",
      "|[2015-02-24 06:20:00.0,2015-02-24 06:30:00.0]|6688 |\n",
      "|[2015-02-24 08:00:00.0,2015-02-24 08:10:00.0]|7469 |\n",
      "+---------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from pyevents_per_window\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "withEventTime = streaming\\\n",
    ".selectExpr(\"*\", \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")\n",
    "withEventTime2 = withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\"), \"User\").count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevents_per_window2\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#withEventTime2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+----+-----+\n",
      "|window                                       |User|count|\n",
      "+---------------------------------------------+----+-----+\n",
      "|[2015-02-23 04:20:00.0,2015-02-23 04:30:00.0]|g   |4966 |\n",
      "|[2015-02-24 07:30:00.0,2015-02-24 07:40:00.0]|b   |4087 |\n",
      "|[2015-02-24 06:20:00.0,2015-02-24 06:30:00.0]|f   |6688 |\n",
      "|[2015-02-24 09:00:00.0,2015-02-24 09:10:00.0]|e   |4998 |\n",
      "|[2015-02-24 07:00:00.0,2015-02-24 07:10:00.0]|f   |1663 |\n",
      "|[2015-02-23 07:40:00.0,2015-02-23 07:50:00.0]|a   |5537 |\n",
      "|[2015-02-24 08:50:00.0,2015-02-24 09:00:00.0]|e   |6384 |\n",
      "|[2015-02-23 08:30:00.0,2015-02-23 08:40:00.0]|h   |4723 |\n",
      "|[2015-02-24 08:10:00.0,2015-02-24 08:20:00.0]|e   |3384 |\n",
      "|[2015-02-24 07:00:00.0,2015-02-24 07:10:00.0]|d   |4987 |\n",
      "+---------------------------------------------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from pyevents_per_window2\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Sliding Window Event-Time Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10 min window starting every 5 min\n",
    "withEventTime = streaming\\\n",
    ".selectExpr(\"*\", \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")\n",
    "withEventTime3 = withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\")).count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevents_per_window3\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#withEventTime3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-----+\n",
      "|window                                       |count|\n",
      "+---------------------------------------------+-----+\n",
      "|[2015-02-23 08:15:00.0,2015-02-23 08:25:00.0]|5403 |\n",
      "|[2015-02-24 05:50:00.0,2015-02-24 06:00:00.0]|7519 |\n",
      "|[2015-02-24 07:00:00.0,2015-02-24 07:10:00.0]|6650 |\n",
      "|[2015-02-21 18:35:00.0,2015-02-21 18:45:00.0]|1    |\n",
      "|[2015-02-23 06:30:00.0,2015-02-23 06:40:00.0]|5059 |\n",
      "|[2015-02-23 04:20:00.0,2015-02-23 04:30:00.0]|4966 |\n",
      "|[2015-02-23 07:25:00.0,2015-02-23 07:35:00.0]|4603 |\n",
      "|[2015-02-24 06:30:00.0,2015-02-24 06:40:00.0]|6289 |\n",
      "|[2015-02-24 07:10:00.0,2015-02-24 07:20:00.0]|5234 |\n",
      "|[2015-02-24 08:25:00.0,2015-02-24 08:35:00.0]|10126|\n",
      "+---------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from pyevents_per_window3\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Event-Time Watermark Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 30 min watermark (SS will wait 30 min after final timestamp before finalizing) on 10 min rolling window every 5 min\n",
    "withEventTime = streaming\\\n",
    ".selectExpr(\"*\", \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")\n",
    "watermark = withEventTime\\\n",
    ".withWatermark(\"event_time\", \"30 minutes\")\\\n",
    ".groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\")).count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevents_watermark\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#watermark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-----+\n",
      "|window                                       |count|\n",
      "+---------------------------------------------+-----+\n",
      "|[2015-02-23 08:15:00.0,2015-02-23 08:25:00.0]|5403 |\n",
      "|[2015-02-24 05:50:00.0,2015-02-24 06:00:00.0]|7519 |\n",
      "|[2015-02-24 07:00:00.0,2015-02-24 07:10:00.0]|6650 |\n",
      "|[2015-02-21 18:35:00.0,2015-02-21 18:45:00.0]|1    |\n",
      "|[2015-02-23 06:30:00.0,2015-02-23 06:40:00.0]|5059 |\n",
      "|[2015-02-23 04:20:00.0,2015-02-23 04:30:00.0]|4966 |\n",
      "|[2015-02-23 07:25:00.0,2015-02-23 07:35:00.0]|4603 |\n",
      "|[2015-02-24 06:30:00.0,2015-02-24 06:40:00.0]|6289 |\n",
      "|[2015-02-24 07:10:00.0,2015-02-24 07:20:00.0]|5234 |\n",
      "|[2015-02-24 08:25:00.0,2015-02-24 08:35:00.0]|10126|\n",
      "+---------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from pyevents_watermark\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Dropping Duplicates Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropDuplicates = withEventTime\\\n",
    ".withWatermark(\"event_time\", \"5 minutes\")\\\n",
    ".dropDuplicates([\"User\", \"event_time\"])\\\n",
    ".groupBy(\"User\")\\\n",
    ".count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pydeduplicated\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropDuplicates.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|User|count|\n",
      "+----+-----+\n",
      "|a   |32340|\n",
      "|b   |36492|\n",
      "|c   |30860|\n",
      "|g   |36671|\n",
      "|h   |30932|\n",
      "|e   |38412|\n",
      "|f   |36824|\n",
      "|d   |32496|\n",
      "|i   |37020|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from pydeduplicated\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #23 - Structured Streaming in Production_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **Fault Tolerance & Checkpointing**:\n",
    "    - ability to recover SS applications via checkpointing and write-ahead logs in HDFS / S3 directory\n",
    "    - simply restart SS application (make sure checkpoint dir exists) to recover its state to start processing data where it crashed\n",
    "    - checkpoint dir stores the streams processed information   \n",
    "    <br>\n",
    "-  **Sizing & Rescaling**:\n",
    "    - will need to scale up cluster or application is input rate (data flowing) > processing rate \"aka\" stream is falling behind and cannot handle the load   \n",
    "    <br>\n",
    "-  **Other Production Elements**:\n",
    "    - Alerting\n",
    "    - Advanced Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #23 Exercises (Structured Streaming)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Checkpointing Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "static = spark.read.json(activityDataSample)\n",
    "streaming = spark\\\n",
    ".readStream\\\n",
    ".schema(static.schema)\\\n",
    ".option(\"maxFilesPerTrigger\", 1)\\\n",
    ".json(activityDataSample)\\\n",
    ".groupBy(\"gt\")\\\n",
    ".count()\n",
    "query = streaming\\\n",
    ".writeStream\\\n",
    ".outputMode(\"complete\")\\\n",
    ".option(\"checkpointLocation\", checkpointPath)\\\n",
    ".queryName(\"test_python_stream\")\\\n",
    ".format(\"memory\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'isDataAvailable': False,\n",
       " 'isTriggerActive': False,\n",
       " 'message': 'Waiting for data to arrive'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'durationMs': {'addBatch': 175,\n",
       "   'getBatch': 4,\n",
       "   'getOffset': 34,\n",
       "   'queryPlanning': 3,\n",
       "   'triggerExecution': 248,\n",
       "   'walCommit': 32},\n",
       "  'id': '5427238c-b8b2-41c9-b563-f6b99908543e',\n",
       "  'name': 'test_python_stream',\n",
       "  'numInputRows': 78012,\n",
       "  'processedRowsPerSecond': 314564.51612903224,\n",
       "  'runId': '26faf9f6-350d-4de9-ab4d-784ecb57300d',\n",
       "  'sink': {'description': 'MemorySink'},\n",
       "  'sources': [{'description': 'FileStreamSource[file:/Users/grp/sparkTheDefinitiveGuide/data/activity-data-sample]',\n",
       "    'endOffset': {'logOffset': 0},\n",
       "    'numInputRows': 78012,\n",
       "    'processedRowsPerSecond': 314564.51612903224,\n",
       "    'startOffset': None}],\n",
       "  'stateOperators': [{'numRowsTotal': 7, 'numRowsUpdated': 7}],\n",
       "  'timestamp': '2018-09-27T22:49:03.869Z'},\n",
       " {'durationMs': {'addBatch': 178,\n",
       "   'getBatch': 4,\n",
       "   'getOffset': 34,\n",
       "   'queryPlanning': 2,\n",
       "   'triggerExecution': 253,\n",
       "   'walCommit': 34},\n",
       "  'id': '5427238c-b8b2-41c9-b563-f6b99908543e',\n",
       "  'inputRowsPerSecond': 277619.2170818505,\n",
       "  'name': 'test_python_stream',\n",
       "  'numInputRows': 78011,\n",
       "  'processedRowsPerSecond': 308343.87351778656,\n",
       "  'runId': '26faf9f6-350d-4de9-ab4d-784ecb57300d',\n",
       "  'sink': {'description': 'MemorySink'},\n",
       "  'sources': [{'description': 'FileStreamSource[file:/Users/grp/sparkTheDefinitiveGuide/data/activity-data-sample]',\n",
       "    'endOffset': {'logOffset': 1},\n",
       "    'inputRowsPerSecond': 277619.2170818505,\n",
       "    'numInputRows': 78011,\n",
       "    'processedRowsPerSecond': 308343.87351778656,\n",
       "    'startOffset': {'logOffset': 0}}],\n",
       "  'stateOperators': [{'numRowsTotal': 7, 'numRowsUpdated': 7}],\n",
       "  'timestamp': '2018-09-27T22:49:04.150Z'},\n",
       " {'durationMs': {'addBatch': 179,\n",
       "   'getBatch': 4,\n",
       "   'getOffset': 34,\n",
       "   'queryPlanning': 3,\n",
       "   'triggerExecution': 254,\n",
       "   'walCommit': 34},\n",
       "  'id': '5427238c-b8b2-41c9-b563-f6b99908543e',\n",
       "  'inputRowsPerSecond': 273726.3157894737,\n",
       "  'name': 'test_python_stream',\n",
       "  'numInputRows': 78012,\n",
       "  'processedRowsPerSecond': 307133.85826771654,\n",
       "  'runId': '26faf9f6-350d-4de9-ab4d-784ecb57300d',\n",
       "  'sink': {'description': 'MemorySink'},\n",
       "  'sources': [{'description': 'FileStreamSource[file:/Users/grp/sparkTheDefinitiveGuide/data/activity-data-sample]',\n",
       "    'endOffset': {'logOffset': 2},\n",
       "    'inputRowsPerSecond': 273726.3157894737,\n",
       "    'numInputRows': 78012,\n",
       "    'processedRowsPerSecond': 307133.85826771654,\n",
       "    'startOffset': {'logOffset': 1}}],\n",
       "  'stateOperators': [{'numRowsTotal': 7, 'numRowsUpdated': 7}],\n",
       "  'timestamp': '2018-09-27T22:49:04.435Z'},\n",
       " {'durationMs': {'addBatch': 171,\n",
       "   'getBatch': 4,\n",
       "   'getOffset': 35,\n",
       "   'queryPlanning': 2,\n",
       "   'triggerExecution': 246,\n",
       "   'walCommit': 33},\n",
       "  'id': '5427238c-b8b2-41c9-b563-f6b99908543e',\n",
       "  'inputRowsPerSecond': 271818.8153310105,\n",
       "  'name': 'test_python_stream',\n",
       "  'numInputRows': 78012,\n",
       "  'processedRowsPerSecond': 317121.9512195122,\n",
       "  'runId': '26faf9f6-350d-4de9-ab4d-784ecb57300d',\n",
       "  'sink': {'description': 'MemorySink'},\n",
       "  'sources': [{'description': 'FileStreamSource[file:/Users/grp/sparkTheDefinitiveGuide/data/activity-data-sample]',\n",
       "    'endOffset': {'logOffset': 3},\n",
       "    'inputRowsPerSecond': 271818.8153310105,\n",
       "    'numInputRows': 78012,\n",
       "    'processedRowsPerSecond': 317121.9512195122,\n",
       "    'startOffset': {'logOffset': 2}}],\n",
       "  'stateOperators': [{'numRowsTotal': 7, 'numRowsUpdated': 7}],\n",
       "  'timestamp': '2018-09-27T22:49:04.722Z'},\n",
       " {'durationMs': {'getOffset': 1, 'triggerExecution': 1},\n",
       "  'id': '5427238c-b8b2-41c9-b563-f6b99908543e',\n",
       "  'inputRowsPerSecond': 0.0,\n",
       "  'name': 'test_python_stream',\n",
       "  'numInputRows': 0,\n",
       "  'processedRowsPerSecond': 0.0,\n",
       "  'runId': '26faf9f6-350d-4de9-ab4d-784ecb57300d',\n",
       "  'sink': {'description': 'MemorySink'},\n",
       "  'sources': [{'description': 'FileStreamSource[file:/Users/grp/sparkTheDefinitiveGuide/data/activity-data-sample]',\n",
       "    'endOffset': {'logOffset': 3},\n",
       "    'inputRowsPerSecond': 0.0,\n",
       "    'numInputRows': 0,\n",
       "    'processedRowsPerSecond': 0.0,\n",
       "    'startOffset': {'logOffset': 3}}],\n",
       "  'stateOperators': [{'numRowsTotal': 7, 'numRowsUpdated': 0}],\n",
       "  'timestamp': '2018-09-27T22:49:05.000Z'},\n",
       " {'durationMs': {'getOffset': 2, 'triggerExecution': 2},\n",
       "  'id': '5427238c-b8b2-41c9-b563-f6b99908543e',\n",
       "  'inputRowsPerSecond': 0.0,\n",
       "  'name': 'test_python_stream',\n",
       "  'numInputRows': 0,\n",
       "  'processedRowsPerSecond': 0.0,\n",
       "  'runId': '26faf9f6-350d-4de9-ab4d-784ecb57300d',\n",
       "  'sink': {'description': 'MemorySink'},\n",
       "  'sources': [{'description': 'FileStreamSource[file:/Users/grp/sparkTheDefinitiveGuide/data/activity-data-sample]',\n",
       "    'endOffset': {'logOffset': 3},\n",
       "    'inputRowsPerSecond': 0.0,\n",
       "    'numInputRows': 0,\n",
       "    'processedRowsPerSecond': 0.0,\n",
       "    'startOffset': {'logOffset': 3}}],\n",
       "  'stateOperators': [{'numRowsTotal': 7, 'numRowsUpdated': 0}],\n",
       "  'timestamp': '2018-09-27T22:49:15.011Z'},\n",
       " {'durationMs': {'getOffset': 3, 'triggerExecution': 3},\n",
       "  'id': '5427238c-b8b2-41c9-b563-f6b99908543e',\n",
       "  'inputRowsPerSecond': 0.0,\n",
       "  'name': 'test_python_stream',\n",
       "  'numInputRows': 0,\n",
       "  'processedRowsPerSecond': 0.0,\n",
       "  'runId': '26faf9f6-350d-4de9-ab4d-784ecb57300d',\n",
       "  'sink': {'description': 'MemorySink'},\n",
       "  'sources': [{'description': 'FileStreamSource[file:/Users/grp/sparkTheDefinitiveGuide/data/activity-data-sample]',\n",
       "    'endOffset': {'logOffset': 3},\n",
       "    'inputRowsPerSecond': 0.0,\n",
       "    'numInputRows': 0,\n",
       "    'processedRowsPerSecond': 0.0,\n",
       "    'startOffset': {'logOffset': 3}}],\n",
       "  'stateOperators': [{'numRowsTotal': 7, 'numRowsUpdated': 0}],\n",
       "  'timestamp': '2018-09-27T22:49:25.011Z'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.recentProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
