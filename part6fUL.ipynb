{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: The Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: Advanced Analytics and Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust = '/Users/grp/sparkTheDefinitiveGuide/data/retail-data/by-day/*.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #29 - Unsupervised Learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-  UL tries to discover patterns and underlying structure of dataset typically via clustering:\n",
    "    -  clustering can create odd clusters because of high-dimensional spaces aka \"the curse of dimensionality\":\n",
    "        -  as feature space expands in dimensionality it becomes increasingly sparse and contains more noise in the data:\n",
    "            - thus as the dimensions increase the data needed to compute statistical results increases very fast:\n",
    "                -  thus making it difficult to predict hence reading in on noise instead of the factors causing the cluster groupings\n",
    "-  K-Means / Bisecting K-Means group data by reducing sum of squared distances from the center of the cluster\n",
    "-  Gaussian Mixture Model produces clusters based on Gaussian distribution\n",
    "\n",
    "### Use Cases:\n",
    "-  identifying anomalies in data\n",
    "-  topic modeling to identify topics within unstructured text\n",
    "-  identifying groups in data\n",
    "\n",
    "### MLlib Unsupervised Models:\n",
    "-  K-Means\n",
    "-  Bisecting K-Means\n",
    "-  GMM (Gaussian Mixture Model)\n",
    "-  LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration:\n",
    "-  Model Hyperparameters (structure of how model can be initialized)\n",
    "-  Training Parameters (structure of how model can be trained)\n",
    "-  Prediction Parameters (structured of how model determines making predictions)\n",
    "-  Model Summary (provides information about final trained model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #29 Exercises (UL)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, features: vector]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va = VectorAssembler().setInputCols([\"Quantity\", \"UnitPrice\"]).setOutputCol(\"features\")\n",
    "\n",
    "sales = va.transform(spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(clust)\n",
    ".limit(50)\n",
    ".coalesce(1)\n",
    ".where(\"Description IS NOT NULL\"))\n",
    "\n",
    "sales.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|   features|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|[48.0,1.79]|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|[20.0,1.25]|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|[24.0,1.65]|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.printSchema()\n",
    "sales.show(3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means:\n",
    "-  number of clusters are randomly assigned to different points in the dataset\n",
    "-  unassigned points are assigned to a cluster based on Euclidean distance measure from previously assigned cluster:\n",
    "    -  hence cluster centers (centroids) are computed and all points are assigned to a centroid\n",
    "    -  process continues until # of iterations is reached or when centroid locations stop changing\n",
    "-  must choose K value, which can be a hard task and requires experimentation and understanding dataset\n",
    "-  trade-off with adjusting parameters can lead to increased processing time, but may lead to better clustering results\n",
    "    -  Model Hyperparameters:\n",
    "        -  K:\n",
    "            -  hardcoded number of clusters in model\n",
    "    -  Training Parameters:\n",
    "        -  initMode:\n",
    "            -  determines starting locations of the centroids:\n",
    "                -  random (random position of centroids)\n",
    "                -  k-means (default, well spread out centroids)\n",
    "        -  initSteps:\n",
    "            -  number of steps for k-means\n",
    "        -  maxIter:\n",
    "            -  total # of iterations over data before stopping\n",
    "        -  tol:\n",
    "            -  threshold for changes in centroids to stop model\n",
    "    -  Metrics Summary:\n",
    "        -  summary class to evaluate model\n",
    "        -  information about clusters created and their relative sizes\n",
    "        -  computes \"within set sum of squared errors\" for how close values are from each cluster centroid (**computeCost**)\n",
    "        -  goal is to minimize \"within set sum of squared error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name. (default: features)\n",
      "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2, current: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 7969353092125344463)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n"
     ]
    }
   ],
   "source": [
    "km = KMeans().setK(5)\n",
    "print(km.explainParams())\n",
    "kmModel = km.fit(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Metrics Summary Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 8, 29, 2, 1]\n",
      "Cluster Centers: \n",
      "[23.2    0.956]\n",
      "[ 2.5     11.24375]\n",
      "[7.55172414 2.77172414]\n",
      "[48.    1.32]\n",
      "[36.    0.85]\n"
     ]
    }
   ],
   "source": [
    "summary = kmModel.summary\n",
    "print(summary.clusterSizes) # number of points\n",
    "kmModel.computeCost(sales)\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisecting K-Means:\n",
    "-  \"top-down\" clustering method whereas K-Means performs \"bottom-up\" aka initially assigning different groups\n",
    "-  initially creates a single group and then splits group into smaller groups to finalize into K # of clusters specified\n",
    "-  Model Hyperparameters:\n",
    "    -  K:\n",
    "        -  hardcoded number of clusters in model\n",
    "-  Training Parameters:\n",
    "    -  minDivisibleClusterSize:\n",
    "        -  minimum # of points / minimum proportion of points\n",
    "        -  sets how many minimum points must be in each cluster\n",
    "    -  maxIter:\n",
    "        -  total # of iterations over data before stopping\n",
    "-  Metrics Summary:\n",
    "    -  same as K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name. (default: features)\n",
      "k: The desired number of leaf clusters. Must be > 1. (default: 4, current: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 20, current: 5)\n",
      "minDivisibleClusterSize: The minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster. (default: 1.0)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: -6311319853468918464)\n"
     ]
    }
   ],
   "source": [
    "bkm = BisectingKMeans().setK(5).setMaxIter(5)\n",
    "print(bkm.explainParams())\n",
    "bkmModel = bkm.fit(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Metrics Summary Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 8, 13, 10, 3]\n",
      "Cluster Centers: \n",
      "[23.2    0.956]\n",
      "[ 2.5     11.24375]\n",
      "[7.55172414 2.77172414]\n",
      "[48.    1.32]\n",
      "[36.    0.85]\n"
     ]
    }
   ],
   "source": [
    "summary = bkmModel.summary\n",
    "print(summary.clusterSizes) # number of points\n",
    "kmModel.computeCost(sales)\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM:\n",
    "-  assumes each cluster produces data randomly from Gaussian distribution and probabalities\n",
    "    -  Model Hyperparameters:\n",
    "        -  K:\n",
    "            -  hardcoded number of clusters in model\n",
    "    -  Training Parameters:\n",
    "        -  maxIter:\n",
    "            -  total # of iterations over data before stopping\n",
    "        -  tol:\n",
    "            -  threshold for changes in weights to stop model\n",
    "    -  Metrics Summary:\n",
    "        - produces cluster metrics like:\n",
    "            -  weights\n",
    "            -  means\n",
    "            -  covariance of Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name. (default: features)\n",
      "k: Number of independent Gaussians in the mixture model. Must be > 1. (default: 2, current: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "seed: random seed. (default: -7090211980209472397)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.01)\n"
     ]
    }
   ],
   "source": [
    "gmm = GaussianMixture().setK(5)\n",
    "print(gmm.explainParams())\n",
    "model = gmm.fit(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Metrics Summary Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16503937777770641, 0.35496420094056985, 0.06003637101912308, 0.1999636297743671, 0.21999642048823354]\n",
      "+--------------------+--------------------+\n",
      "|                mean|                 cov|\n",
      "+--------------------+--------------------+\n",
      "|[2.54180583818530...|0.785769315153778...|\n",
      "|[5.07243095740621...|2.059950971034034...|\n",
      "|[43.9877864408847...|32.22707068867282...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         2|\n",
      "|         3|\n",
      "|         3|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+\n",
      "|         probability|\n",
      "+--------------------+\n",
      "|[1.37632400885157...|\n",
      "|[4.89041912245635...|\n",
      "|[1.67299627008735...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = model.summary\n",
    "print(model.weights)\n",
    "model.gaussiansDF.show(3)\n",
    "summary.cluster.show(3)\n",
    "summary.clusterSizes\n",
    "summary.probability.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA:\n",
    "-  hierarchical clustering model\n",
    "-  performs well on text documents\n",
    "-  treats each document as having a variable number of contributing factors from multiple input topics\n",
    "-  implementations:\n",
    "    -  online LDA:\n",
    "        -  works better with larger document examples\n",
    "    -  expectation maximization (EM):\n",
    "        -  works better with larger input vocabulary and more topics associated with corpus\n",
    "-  Model Hyperparameters:\n",
    "    -  K:\n",
    "        -  total number of topics for data\n",
    "    -  docConcentration:\n",
    "        -  computes Dirichlet distribution (document-topic distribution)\n",
    "    -  topicConcentration:\n",
    "        -  symmetric Dirichlet distribution (document-topic distribution)\n",
    "-  Training Parameters:\n",
    "    -  maxIter:\n",
    "        -  total # of iterations over data before stopping\n",
    "    -  optimizer:\n",
    "        -  determines training mechanism [EM or online (default)]\n",
    "    -  learningDecay:\n",
    "        -  learning rate (should be between 0.5 and 1.0)\n",
    "    -  learningOffset:\n",
    "        -  only relevant for online optimizer\n",
    "        -  downweights early iterations\n",
    "    -  optimizerDocConcentration:\n",
    "        -  only relevant for online optimizer\n",
    "        -  dependent on docConcentration value and if optimization will occur during trainnig\n",
    "    -  subsamplingRate:\n",
    "        -  sample fraction of corpus for mini-batch gradient descent iterations\n",
    "    -  seed:\n",
    "        -  re-produces same results\n",
    "    -  checkpointInterval:\n",
    "        -  saves model's work over course of training for recovery purposes\n",
    "-  Prediction Parameters:\n",
    "    -  topicDistributionCol:\n",
    "        -  holds output of topic mixture distribution for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "\n",
    "tokenized = tkn.transform(sales.drop(\"features\"))\n",
    "cv = CountVectorizer()\\\n",
    ".setInputCol(\"DescOut\")\\\n",
    ".setOutputCol(\"features\")\\\n",
    ".setVocabSize(500)\\\n",
    ".setMinTF(0)\\\n",
    ".setMinDF(0)\\\n",
    ".setBinary(True)\n",
    "\n",
    "cvFitted = cv.fit(tokenized)\n",
    "prepped = cvFitted.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "docConcentration: Concentration parameter (commonly named \"alpha\") for the prior placed on documents' distributions over topics (\"theta\"). (undefined)\n",
      "featuresCol: features column name. (default: features)\n",
      "k: The number of topics (clusters) to infer. Must be > 1. (default: 10, current: 10)\n",
      "keepLastCheckpoint: (For EM optimizer) If using checkpointing, this indicates whether to keep the last checkpoint. If false, then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. (default: True)\n",
      "learningDecay: Learning rate, set as anexponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. (default: 0.51)\n",
      "learningOffset: A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less (default: 1024.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20, current: 5)\n",
      "optimizeDocConcentration: Indicates whether the docConcentration (Dirichlet parameter for document-topic distribution) will be optimized during training. (default: True)\n",
      "optimizer: Optimizer or inference algorithm used to estimate the LDA model.  Supported: online, em (default: online)\n",
      "seed: random seed. (default: 7673890338921026109)\n",
      "subsamplingRate: Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. (default: 0.05)\n",
      "topicConcentration: Concentration parameter (commonly named \"beta\" or \"eta\") for the prior placed on topic' distributions over terms. (undefined)\n",
      "topicDistributionCol: Output column with estimates of the topic mixture distribution for each document (often called \"theta\" in the literature). Returns a vector of zeros for an empty document. (default: topicDistribution)\n"
     ]
    }
   ],
   "source": [
    "lda = LDA().setK(10).setMaxIter(5)\n",
    "print(lda.explainParams())\n",
    "model = lda.fit(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+\n",
      "|topic|  termIndices|         termWeights|\n",
      "+-----+-------------+--------------------+\n",
      "|    0|[137, 90, 49]|[0.00891024805336...|\n",
      "|    1| [56, 29, 98]|[0.00916648406836...|\n",
      "|    2|[15, 131, 45]|[0.00897001752758...|\n",
      "|    3|   [2, 7, 16]|[0.01734152140219...|\n",
      "|    4|  [40, 10, 8]|[0.01155559318753...|\n",
      "|    5| [11, 23, 13]|[0.01462723671834...|\n",
      "|    6|    [3, 1, 0]|[0.01443826360785...|\n",
      "+-----+-------------+--------------------+\n",
      "only showing top 7 rows\n",
      "\n",
      "['water', 'hot', 'vintage', 'bottle', 'paperweight', '6', 'home', 'doormat', 'landmark', 'bicycle']\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "# limiting to 3 terms per topic\n",
    "df = model.describeTopics(3)\n",
    "df.show(7)\n",
    "# searching vocabulary\n",
    "print(cvFitted.vocabulary[:10])\n",
    "# number of terms in vocabulary\n",
    "print(len(cvFitted.vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Map Index-Terms Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indicesToTermsMap(vocabulary):\n",
    "    def indicesToTermsMap(x):\n",
    "        return [vocabulary[int(i)] for i in x]\n",
    "    return udf(indicesToTermsMap, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----------------------------+\n",
      "|topic|termIndices  |termWords                    |\n",
      "+-----+-------------+-----------------------------+\n",
      "|0    |[137, 90, 49]|[slate, woodland, design]    |\n",
      "|1    |[56, 29, 98] |[message, night, rack]       |\n",
      "|2    |[15, 131, 45]|[kit, or, amelie]            |\n",
      "|3    |[2, 7, 16]   |[vintage, doormat, leaf]     |\n",
      "|4    |[40, 10, 8]  |[notting, frame, landmark]   |\n",
      "|5    |[11, 23, 13] |[ribbons, christmas, classic]|\n",
      "|6    |[3, 1, 0]    |[bottle, hot, water]         |\n",
      "+-----+-------------+-----------------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maps = df.withColumn(\"termWords\", indicesToTermsMap(cvFitted.vocabulary)(\"termIndices\"))\n",
    "maps.select(\"topic\", \"termIndices\", \"termWords\").show(7, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
